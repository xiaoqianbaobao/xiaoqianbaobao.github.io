[{"content":"这篇文章主要记录我搭建hugo博客的过程，包括搭建过程中遇到的问题及解决方法\n1、新建github仓库 新建respository, Respository的名称为 \u0026ldquo;你的github名称.github.io\u0026rdquo;.\n2、下载并安装对应版本的hugo 下载链接: 点击选择hugo版本下载\n3、创建hugo网站 进入想要存放网站的文件夹，输入以下命令:\n1 hugo new site demo-blog 4、选择主题 输入命令下载主题:\n1 git clone https://github.com/adityatelange/hugo-PaperMod.git 使用该主题的方法就是在站点文件夹下的配置文件里输入主题的名字:\n然后把主题文件夹里面的一些静态文件和配置文件复制到站点目录下，目的是为了可以自定义博客的样式，而不会改动主题文件夹里的样式，这样主题要更新的时候，直接在主题目录下git pull就可以，站点目录的修改会优先覆盖主题里的配置，所以可以实现平滑更新\n","permalink":"https://csqread.top/posts/blog/hugo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","summary":"这篇文章主要记录我搭建hugo博客的过程，包括搭建过程中遇到的问题及解决方法 1、新建github仓库 新建respository, Respos","title":"Hugo博客搭建"},{"content":"第五章：数据复制 复制的目的：\n● 使得数据与用户在地理上接近（从而减少延迟）\n● 即使系统的一部分出现故障，系统也能继续工作（从而提高可用性）\n● 伸缩可以接受读请求的机器数量（从而提高读取吞吐量）\n如果复制的数据不会随时间而改变，那复制就很简单：复制一次即可。 复制的难点在于复制数据的变更。 三种流行的变更复制算法：\n● 单领导者\n● 多领导者\n● 无领导者\n复制时的权衡：使用同步复制还是异步复制？如何处理失败的副本？\n领导者与追随者 ● 存储数据库副本的每个节点称为 副本（replica）。\n● 多副本的问题：如何确保数据都落在了所有的副本上。\n○ 每次对数据库的写入都要传播到所有副本上，否则副本就会有不一样的数据。\n○ 常见的解决方案：基于领导者的复制（主从复制）。 主从复制工作原理：\n副本之一被指定为领导者（leader，也被称作主库）\na. 客户端写数据时，要把请求发送给领导者； b. 领导者把新输入写入本地存储。 2. 其他副本被称为追随者（followers，也被称作只读副本、从库、热备） a. 每当领导者将新数据写入本地存储时，他会把数据变更发送给所有的追随者，称之为复制日志或变更流。 b. 每个追随者从领导者拉取日志，并相应更新其本地数据库副本，方法是按照领导者处理的相同顺序应用所有写入。 3. 当客户想要从数据库中读取数据时，它可以向领导者或追随者查询。 但只有领导者才能接受写操作（从客户端的角度来看从库都是只读的）。 同步复制与异步复制 复制系统的一个重要细节是：复制是 同步（synchronously） 发生还是 异步（asynchronously） 发生。 以用户更新头像为例： ● 从库 1 的复制是同步的 ● 从库 2 的复制是异步的\n同步复制： ● 优点：从库保证和主库一直的最新数据副本 ● 缺点：如果从库没有响应（如已崩溃、网络故障），主库就无法处理写入操作。主库必须阻止所有的写入，等待副本再次可用。\n半同步：通常使用一个从库与主库是同步的，而其他从库是异步的。这保证了至少两个节点拥有最新的数据副本。\n通常情况下，基于领导者的复制都配置为完全异步。注意，主库故障可能导致丢失数据。\n设置新从库 有时会增加一个新的从库。\n过程：\n在某个时刻获取主库的一致性快照（如果可能），而不必锁定整个数据库。大多数数据库都具有这个功能，因为它是备份必需的。对于某些场景，可能需要第三方工具，例如MySQL的innobackupex 。 将快照复制到新的从库节点。 从库连接到主库，并拉取快照之后发生的所有数据变更。这要求快照与主库复制日志中的位置精确关联。该位置有不同的名称：例如，PostgreSQL将其称为 日志序列号（log sequence number, LSN），MySQL将其称为 二进制日志坐标（binlog coordinates）。 当从库处理完快照之后积压的数据变更，我们说它 赶上（caught up） 了主库。现在它可以继续处理主库产生的数据变化了。 处理节点宕机 我们的目标：即使个别节点失效，也要能保持整个系统运行，并尽可能控制节点停机带来的影响。\n从库失效：追赶恢复 ● 从库可以从日志知道，在发生故障前处理的最后一个事务。 ● 所以从库可以连接到主库，并拉取断开连接后的所有数据变更。 ● 应用完成所有变更之后，它就赶上了主库，继续接收数据变更流。\n主库失效：故障切换 ● 故障切换：需要把一个从库提升为新的主库，重新配置客户端，其他从库需要开始拉取来自新主库的变更。 ● 故障切换可以手动或者自动进行。\n自动故障切换：\n确认主库失效。有很多事情可能会出错：崩溃，停电，网络问题等等。没有万无一失的方法来检测出现了什么问题，所以大多数系统只是简单使用 超时（Timeout） ：节点频繁地相互来回传递消息，并且如果一个节点在一段时间内（例如30秒）没有响应，就认为它挂了（因为计划内维护而故意关闭主库不算）。 选择一个新的主库。这可以通过选举过程（主库由剩余副本以多数选举产生）来完成，或者可以由之前选定的控制器节点（controller node） 来指定新的主库。主库的最佳人选通常是拥有旧主库最新数据副本的从库（最小化数据损失）。让所有的节点同意一个新的领导者，是一个共识问题，将在第九章详细讨论。 重新配置系统以启用新的主库。客户端现在需要将它们的写请求发送给新主库（将在“请求路由”中讨论这个问题）。如果老领导回来，可能仍然认为自己是主库，没有意识到其他副本已经让它下台了。系统需要确保老领导认可新领导，成为一个从库。 故障切换会出现很多大麻烦：\n● 如果使用异步复制，则新主库可能没有收到老主库宕机前最后的写入操作。在选出新主库后，如果老主库重新加入集群，新主库在此期间可能会收到冲突的写入，那这些写入该如何处理？最常见的解决方案是简单丢弃老主库未复制的写入，这很可能打破客户对于数据持久性的期望。 ● 如果数据库需要和其他外部存储相协调，那么丢弃写入内容是极其危险的操作。例如在GitHub 【13】的一场事故中，一个过时的MySQL从库被提升为主库。数据库使用自增ID作为主键，因为新主库的计数器落后于老主库的计数器，所以新主库重新分配了一些已经被老主库分配掉的ID作为主键。这些主键也在Redis中使用，主键重用使得MySQL和Redis中数据产生不一致，最后导致一些私有数据泄漏到错误的用户手中。 ● 发生某些故障时（见第八章）可能会出现两个节点都以为自己是主库的情况。这种情况称为 脑裂(split brain)，非常危险：如果两个主库都可以接受写操作，却没有冲突解决机制（请参阅“多主复制”），那么数据就可能丢失或损坏。一些系统采取了安全防范措施：当检测到两个主库节点同时存在时会关闭其中一个节点[1]，但设计粗糙的机制可能最后会导致两个节点都被关闭【14】。 ● 主库被宣告死亡之前的正确超时应该怎么配置？在主库失效的情况下，超时时间越长，意味着恢复时间也越长。但是如果超时设置太短，又可能会出现不必要的故障切换。例如，临时负载峰值可能导致节点的响应时间超时，或网络故障可能导致数据包延迟。如果系统已经处于高负载或网络问题的困扰之中，那么不必要的故障切换可能会让情况变得更糟糕。\n复制日志的实现 基于主库的复制，底层工作有几种不同的复制方式。\n基于语句的复制 在最简单的情况下，主库记录下它执行的每个写入请求（语句（statement））并将该语句日志发送给其从库。 问题： ● 任何调用 非确定性函数（nondeterministic） 的语句，可能会在每个副本上生成不同的值。比如 NOW(), RAND()。 ● 如果语句使用了自增列（auto increment），或者依赖于数据库中的现有数据（例如，UPDATE \u0026hellip; WHERE \u0026lt;某些条件\u0026gt;），则必须在每个副本上按照完全相同的顺序执行它们，否则可能会产生不同的效果。影响并发。 ● 有副作用的语句（例如，触发器，存储过程，用户定义的函数）可能会在每个副本上产生不同的副作用，除非副作用是绝对确定的。\n传输预写式日志（WAL） 第三章告诉我们，写操作通常追加到日志中： ● 对于日志结构存储引擎（SSTables 和 LSM 树），日志是主要存储位置。日志段在后台压缩，并进行垃圾回收。 ● 覆盖单个磁盘块的 B 树，每次修改会先写入预写式日志（Write Ahead Log, WAL），以便崩溃后索引可以恢复到一个一致的状态。 所以，日志都是包含所有数据库写入的仅追加字节序列。可以使用完全相同的日志在另一个节点上构建副本：主库把日志发送给从库。 PostgreSQL和Oracle等使用这种复制方法。 缺点： ● 复制与存储引擎紧密耦合。 ● 不可能使主库和从库上运行不同版本的数据库软件。 ● 运维时如果升级软件版本，有可能会要求停机。\n逻辑日志复制（基于行） 采用逻辑日志，可以把复制与存储逻辑分离。 关系型数据库通常以行作为粒度描述数据库写入的记录序列： ● 对于插入的行，日志包含所有列的新值； ● 对于删除的行，日志包含足够的信息来唯一标识已删除的行。通常是主键，或者所有列的旧值。 ● 对于更新的行，日志包含足够的信息来唯一标识更新的行，以及所有列（至少是更新列）的新值。 优点： ● 逻辑日志与存储引擎分离，方便向后兼容。可以让领导者和跟随者运行不同版本的数据库软件。 ● 对于外部应用，逻辑日志也更容易解析。比如复制到数据仓库，或者自定义索引和缓存。被称为数据变更捕获。\n基于触发器的复制 ● 上述复制都是数据库自己实现的。也可以自定义复制方法：数据库提供了触发器和存储过程。 ● 允许数据库变更时，自动执行应用的程序代码。 ● 开销更大，更容易出错。但更灵活。\n复制延迟问题 ● 主从异步同步会有延迟：导致同时对主库和从库的查询，结果可能不同。 ● 因为从库会赶上主库，所以上述效应被称为「最终一致性」。 ● 复制延迟可能超过几秒或者几分钟，下文是 3 个例子。\n读己之写 如果用户把数据提交到了主库，但是主从有延迟，用户马上看数据的时候请求的从库，会感觉到数据丢失。\n此时需要「读写一致性」，也成为读己之写一致性。 技术： ● 读用户可能已经修改过的内容时，都从主库读；比如读个人资料都从主库读，读别人的资料可以读从库。 ● 如果应用的部分内容都可能被用户编辑，上述方法无效。可以指定更新后的时间窗口，比如上次更新的一分钟内从主库读。 ● 客户端记住最近一次写入的时间戳，从库提供查询时，保证该时间戳前的变更都已经传播到了本从库；否则从另外的从库读，或者等待从库追赶上来。（时间戳可以是逻辑时间戳，如日志序列号；或者要有准确的时间同步） ● 如果副本在多个数据中心，则比较复杂。任何需要从领导者提供服务的请求，都必须路由到包含主库的数据中心。 用户有多个设备时，还要考虑的问题： ● 记录更新时间戳变得更困难； ● 不同设备可能路由到不同的数据中心。如果你的方法需要读主库，就需要把同一用户的请求路由到同一个数据中心。\n单调读 用户可能会遇到时光倒流。 第一次请求到从库看到了评论，第二次请求到另外一个从库发现评论消失。\n单调读保证了这种异常不会发生。 方法： ● 确保每个用户总是从同一副本来读取。比如基于用户 ID 的散列来选择副本，而不是随机选。 ● 但是如果该副本失败，则需要路由到另一个副本。\n一致前缀读 一系列事件可能出现前后顺序不一致问题。比如回答可能在提问之前发生。 这是分区（分片）数据库中的一个特殊问题：不同分区之间独立，不存在全局写入顺序。\n需要「一致前缀读」。 方法： ● 任何因果相关的写入都写入相同的分区。\n复制延迟的解决方案 ● 可以信赖数据库：需要事务。 ● 事务（transaction） 存在的原因：数据库通过事务提供强大的保证，所以应用程序可以更加简单。 ● 单节点事务存在了很长时间，但是分布式数据库中，许多系统放弃了事务。“因为事务的代价太高。” ● 本书的其余部分将继续探讨事务。\n","permalink":"https://csqread.top/posts/tech/ddia%E7%AC%AC%E4%BA%94%E7%AB%A0/","summary":"第五章：数据复制 复制的目的： ● 使得数据与用户在地理上接近（从而减少延迟） ● 即使系统的一部分出现故障，系统也能继续工作（从而提高可用性） ● 伸缩","title":"DDIA第五章"},{"content":"最近有一些原因，导致我想把我的小米11Ultra刷个MIUI GLOBAL， 想起来上一次刷机还是在大学的时候用的redmi k20 pro, 当时只是为了获取到root权限然后使用模拟定位打卡软件避免早起跑操打卡。\n这次折腾完之后，我又用上了我的iphone XR, 11 ultra成为了我的备用机。\n而这次刷机是因为工作上软件强制使用入职时候的证件照作为头像，并且你改成自己的头像后每天晚上12点就会强制刷新回证件照。所以我在想能不能想想办法可以一直使用自己的头像。\n然后我的思路大概就是这样的：其实他们定时改头像就是服务端发到客户端的一个请求，我是不是把这个请求给拦截掉就可以了？或者说我把这个请求的·返回报文里的 改成我自己的头像。总之是个模糊的想法。但是手机肯定是要先root掉。\n后面就是对手机的一系列操作，在这次root的过程中，其实还发现了一个问题，就是有一个自己的NAS服务器还是挺重要的。否则我前面的那么多东西备份起来是真的麻烦。所以后续我还会自己建一个NAS服务器专门用来备份我的照片和视频什么的。\n其实这不是一个刷机的教学博客，只是我的一些碎碎念。工作上遇到的一些糟心的事情罢了，只不过我想的是，我是否还有刚刚毕业时候那种遇到问题就动手解决的热情和动力。包括这种糟心的强制使用证件照做头像。\n如果有一天，我再也不想折腾了，那样的生活会不会很无趣\n","permalink":"https://csqread.top/posts/tech/%E5%8F%88%E4%B8%80%E6%AC%A1%E7%9A%84%E5%88%B7%E6%9C%BA%E4%B9%8B%E6%97%85/","summary":"最近有一些原因，导致我想把我的小米11Ultra刷个MIUI GLOBAL， 想起来上一次刷机还是在大学的时候用的redmi k20 pro, 当时只是为了获取","title":"又一次的刷机之旅"},{"content":"数据编码与演化 应用程序总是增增改改。 修改程序大多数情况下也在修改存储的数据。\n关系数据库通常假定数据库中的所有数据都遵循一个模式：尽管可以更改该模式（通过模式迁移，即ALTER语句），但是在任何时间点都有且仅有一个正确的模式。 读时模式（schema-on-read）（或 无模式（schemaless））数据库不会强制一个模式，因此数据库可以包含在不同时间写入的新老数据格式的混合。 当数据格式发生改变时，需要代码更改：\n服务端应用程序，会灰度发布； 客户端应用程序，看用户心情。 新旧版本的代码和数据，可能同时共处。 系统需要双向兼容：\n向后兼容：新代码可以读旧数据。容易。 向前兼容：旧代码可以读新数据。难。 编码数据的格式 程序通常（至少）使用两种形式的数据：\n在内存中，数据保存在对象，结构体，列表，数组，哈希表，树等中。 这些数据结构针对CPU的高效访问和操作进行了优化（通常使用指针）。 如果要将数据写入文件，或通过网络发送，则必须将其 编码（encode） 为某种自包含的字节序列（例如，JSON文档）。 由于每个进程都有自己独立的地址空间，一个进程中的指针对任何其他进程都没有意义，所以这个字节序列表示会与通常在内存中使用的数据结构完全不同。 所以，需要在两种表示之间进行某种类型的翻译。\n从内存中表示到字节序列的转换称为 编码（Encoding） （也称为序列化（serialization） 或编组（marshalling））； 反过来称为解码（Decoding）ii（解析（Parsing），反序列化（deserialization），反编组( unmarshalling））译i。 语言特定的格式 许多编程语言都内建了将内存对象编码为字节序列的支持。例如，Java有java.io.Serializable ，Ruby有Marshal，Python有pickle .\n这些库很方便，但是有深层次问题：\n与特定的编程语言绑定。 为了恢复相同对象类型的数据，解码过程需要实例化任意类的能力，这是安全问题的来源。 数据版本控制不方便。 效率也不高。 只适合临时使用。\nJSON，XML和二进制变体 跨语言的编码：JSON，XML和CSV，属于文本格式，因此具有人类可读性。 除了语法问题外，还有问题：\n数值编码有歧义：XML 和 CSV 不能区分数字和字符串。JSON 不能区分整数和浮点数。 处理大数值困难。 JSON 和 XML 对 unicode（人类可读的文本）有很好的支持，但是不支持二进制。通过 base64 绕过这个限制。 XML 和 JSON 都有可选的模式支持。 CSV 没有模式，行列的含义完全由应用程序指定。格式模糊。 虽然问题多，但是大家对这些达成了意见一致。\n二进制编码 当数据很多的时候，数据格式的选择会有很大影响。 JSON比XML简洁，但与二进制格式相比还是太占空间。现在有很多二进制格式的 JSON（MessagePack，BSON，BJSON，UBJSON，BISON和Smile等）。 JSON 字符串是：\n1 2 3 4 5 6 { \u0026#34;userName\u0026#34;: \u0026#34;Martin\u0026#34;, \u0026#34;favoriteNumber\u0026#34;: 1337, \u0026#34;interests\u0026#34;: [\u0026#34;daydreaming\u0026#34;, \u0026#34;hacking\u0026#34;] } 二进制编码长度为66个字节，仅略小于文本JSON编码所取的81个字节（删除了空白）。\nThrift与Protocol Buffers Protocol Buffers最初是在Google开发的，Thrift最初是在Facebook开发的，并且在2007~2008年都是开源的，都是二进制编码库。 Thrift和Protocol Buffers都需要一个模式来编码任何数据。 接口定义语言（IDL） 来描述模式。\nThrift 比如： 1 2 3 4 5 struct Person { 1: required string userName, 2: optional i64 favoriteNumber, 3: optional list\u0026lt;string\u0026gt; interests } Protocol Buffers的等效模式定义看起来非常相似： 1 2 3 4 5 message Person { required string user_name = 1; optional int64 favorite_number = 2; repeated string interests = 3; } Thrift和Protocol Buffers每一个都带有一个代码生成工具，可以调用此代码对模式进行编码和解码。 Thrift 编码格式\nThrift 有两种不同的二进制编码格式，分别称为 BinaryProtocol 和 CompactProtocol BinaryProtocol\n\u0026ndash; 对上面的信息编码只需要59个字节\n每个字段都有一个类型注释（用于指示它是一个字符串，整数，列表等），还可以根据需要指定长度（字符串的长度，列表中的项目数） 。 最大的区别是没有字段名，而只有字段标签，即数字 1，2，3，就像别名。 CompactProtocol\n语义上等同于BinaryProtocol 将字段类型和标签号打包到单个字节中，并使用可变长度整数来实现 相同的信息打包成只有 34 个字节 将数字 1337 编码成为 2 个字节，每个字节的最高位标识是否还有更多的字节。 Protocol Buffers\n只有一种二进制编码格式，与Thrift的CompactProtocol非常相似。 同样的记录塞进了33个字节中。 字段是否为必须？\n如果字段没有设置值，则从编码记录中省略。 模式中每个字段标记为是否为必须，但对编码无影响。 区别在于，如果字段设置为必须，但是未设置，那么运行时检查会失败。 字段标签和模式演变 字段标记很重要！可以改字段的名字，但是不能改字段标记。 向前兼容：可以添加新字段，只要有一个新的标记号码。 向后兼容：在模式初始部署之后，添加的每个字段必须是可选的或具有默认值。否则之前的代码会检查失败。 删除字段：只能删除可选字段；不能再次使用相同的标签号码。 数据类型和模式演变 数据类型可以被改变: int32 升级 int64, 新代码可以读取旧代码写入的数据(补0)；但是旧的代码不能解析新的数据(int32 读取 int64 会被截断) Protobuf 一个细节：没有列表类型，只有 repeated，因此可以把可选字段改为重置字段。 Thrift不能更改为列表参数，但优点是可以嵌套列表. Avro Apache Avro 是另一种二进制编码格式 Avro 有两种模式语言：一种(Avro IDL) 用于人工编辑，一种（基于JSON）更易于机器读取。 举例:\n1 2 3 4 5 record Person { string userName; union { null, long } favoriteNumber = null; array\u0026lt;string\u0026gt; interests; } 等价的JSON表示:\n1 2 3 4 5 6 7 8 9 { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;userName\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteNumber\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;null\u0026#34;, \u0026#34;long\u0026#34;], \u0026#34;default\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;interests\u0026#34;, \u0026#34;type\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: \u0026#34;string\u0026#34;} ] } 注意：没有标签号码。 Avro二进制编码只有32个字节长，最紧凑的。 编码知识连在一起的值，不能识别字段和数据类型。 必须按照顺序遍历字段才能解码。 编码和解码必须使用完全相同的模式。 Writer模式与Reader模式 Avro的关键思想是Writer模式和Reader模式不必是相同的 - 他们只需要兼容。 当数据解码（读取）时，Avro库通过并排查看Writer模式和Reader模式并将数据从Writer模式转换到Reader模式来解决差异。（即数据读取的时候，会对比 Writer模式 和 Reader模式 的字段，然后就知道怎么读了） 模式演变规则 为了保持兼容性，您只能添加或删除具有默认值的字段。 但Writer模式到底是什么？ 对于一段特定的编码数据，Reader如何知道其Writer模式？ 答案取决于Avro使用的上下文。举几个例子：\n有很多记录的大文件\n○ Avro的一个常见用途 - 尤其是在Hadoop环境中 - 用于存储包含数百万条记录的大文件，所有记录都使用相同的模式进行编码。可以在文件的开头只包含一次Writer模式。\n支持独立写入的记录的数据库\n○ 最简单的解决方案是在每个编码记录的开始处包含一个版本号，并在数据库中保留一个模式版本列表。 通过网络连接发送记录 ○ 他们可以在连接设置上协商模式版本，然后在连接的生命周期中使用该模式。 动态生成的模式 Avro方法的一个优点是架构不包含任何标签号码。\n但为什么这很重要？在模式中保留一些数字有什么问题？\n不同之处在于Avro对动态生成的模式更友善。 ○ 方便从数据库生成 Avro 模式，导出数据 ○ 当数据库模式发生变化，直接生成新的 Avro 模式，导出数据。自动兼容。 ○ 而用 Thrift 或者 PB，需要手动写字段标签。 代码生成和动态类型的语言 Thrift 和 Protobuf 依赖于代码生成 在定义了模式之后，可以使用您选择的编程语言生成实现此模式的代码。 这在Java，C ++或C＃等静态类型语言中很有用，因为它允许将高效的内存中结构用于解码的数据，并且在编写访问数据结构的程序时允许在IDE中进行类型检查和自动完成。 在动态类型编程语言（如JavaScript，Ruby或Python）中，生成代码没有太多意义，因为没有编译时类型检查器来满足。 Avro为静态类型编程语言提供了可选的代码生成功能，但是它也可以在不生成任何代码的情况下使用。 模式的优点 Protocol Buffers，Thrift和Avro都使用模式来描述二进制编码格式。 他们的模式语言比XML模式或者JSON模式简单得多，也支持更详细的验证规则。 许多数据系统（如关系型数据库）也为其数据实现了某种专有的二进制编码。 基于模式的二进制编码相对于JSON，XML和CSV等文本数据格式的优点： 它们可以比各种“二进制JSON”变体更紧凑，因为它们可以省略编码数据中的字段名称。 模式是一种有价值的文档形式，因为模式是解码所必需的，所以可以确定它是最新的（而手动维护的文档可能很容易偏离现实）。 维护一个模式的数据库允许您在部署任何内容之前检查模式更改的向前和向后兼容性。 对于静态类型编程语言的用户来说，从模式生成代码的能力是有用的，因为它可以在编译时进行类型检查。 数据流的类型 数据在流程之间流动的一些常见的方式：\n通过数据库 通过服务调用 通过异步消息传递 数据库中的数据流 如果只有一个进程访问数据库，向后兼容性显然是必要的。 一般来说，会有多个进程访问数据库，可能会有某些进程运行较新代码、某些运行较旧的代码。因此数据库也经常需要向前兼容。 假设增加字段，那么较新的代码会写入把该值吸入数据库。而旧版本的代码将读取记录，理想的行为是旧代码保持领域完整。 用旧代码读取并重新写入数据库时，有可能会导致数据丢失。 在不同的时间写入不同的值 单一的数据库中，可能有一些值是五毫秒前写的，而一些值是五年前写的。 架构演变允许整个数据库看起来好像是用单个模式编码的，即使底层存储可能包含用模式的各种历史版本编码的记录。 归档存储 建立数据库快照，比如备份或者加载到数据仓库：即使有不同时代的模式版本的混合，但通常使用最新模式进行编码。 由于数据转储是一次写入的，以后不变，所以 Avro 对象容器文件等格式非常适合。 也是很好的机会，把数据编码成面向分析的列式格式。 服务中的数据流：REST与RPC 网络通信方式：常见安排是客户端+服务器 Web 服务：通过 GET 和 POST 请求 服务端可以是另一个服务的客户端：微服务架构。 微服务架构允许某个团队能够经常发布新版本服务，期望服务的新旧版本同时运行。 Web服务 当服务使用HTTP作为底层通信协议时，可称之为Web服务。 有两种流行的Web服务方法：REST和SOAP。 REST\n● REST不是一个协议，而是一个基于HTTP原则的设计哲学。 ● 它强调简单的数据格式，使用URL来标识资源，并使用HTTP功能进行缓存控制，身份验证和内容类型协商。 ● 与SOAP相比，REST已经越来越受欢迎，至少在跨组织服务集成的背景下，并经常与微服务相关。 ● 根据REST原则设计的API称为RESTful。 SOAP\nSOAP是用于制作网络API请求的基于XML的协议。 它最常用于HTTP，但其目的是独立于HTTP，并避免使用大多数HTTP功能。 SOAP Web服务的API使用称为Web服务描述语言（WSDL）的基于XML的语言来描述。 WSDL支持代码生成，客户端可以使用本地类和方法调用（编码为XML消息并由框架再次解码）访问远程服务。 尽管SOAP及其各种扩展表面上是标准化的，但是不同厂商的实现之间的互操作性往往会造成问题。 尽管许多大型企业仍然使用SOAP，但在大多数小公司中已经不再受到青睐。 远程过程调用（RPC）的问题 RPC模型试图向远程网络服务发出请求，看起来与在同一进程中调用编程语言中的函数或方法相同（这种抽象称为位置透明）。\nRPC的缺陷\n本地函数调用是可预测的，并且成功或失败仅取决于受您控制的参数。而网络请求是不可预知的。 本地函数调用要么返回结果，要么抛出异常，或者永远不返回（因为进入无限循环或进程崩溃）。网络请求有另一个可能的结果：由于超时，它可能会返回没有结果。无法得知远程服务的响应发生了什么。 如果您重试失败的网络请求，可能会发生请求实际上正在通过，只有响应丢失。在这种情况下，重试将导致该操作被执行多次，除非您在协议中引入除重（ 幂等（idempotence））机制。本地函数调用没有这个问题。 每次调用本地功能时，通常需要大致相同的时间来执行。网络请求慢得多，不可预知。 调用本地函数时，可以高效地将引用（指针）传递给本地内存中的对象。当你发出一个网络请求时，所有这些参数都需要被编码成可以通过网络发送的一系列字节。如果参数是像数字或字符串这样的基本类型倒是没关系，但是对于较大的对象很快就会变成问题。 客户端和服务端可以用不同的编程语言实现，RPC 框架必须把数据类型做翻译，可能会出问题。 RPC的当前方向 RPC 不会消失。\n● Thrift和Avro带有RPC支持 ● gRPC是使用Protocol Buffers的RPC实现 ● Finagle也使用Thrift ● Rest.li使用JSON over HTTP。 当前方向\n这种新一代的RPC框架更加明确的是，远程请求与本地函数调用不同。 其中一些框架还提供服务发现，即允许客户端找出在哪个IP地址和端口号上可以找到特定的服务。 REST似乎是公共API的主要风格。 REST 使用二进制编码性能更好 方便实验和调试 能被所有主流的编程语言和平台支持 大量可用的工具的生态系统 数据编码与RPC的演化 可演化性，重要的是可以独立更改和部署RPC客户端和服务器。 我们可以做个假定：假定所有的服务器都会先更新，其次是所有的客户端。 您只需要在请求上具有向后兼容性，并且对响应具有前向兼容性。 RPC方案的前后向兼容性属性从它使用的编码方式中继承：\nThrift，gRPC（Protobuf）和Avro RPC可以根据相应编码格式的兼容性规则进行演变。/li\u003e 在SOAP中，请求和响应是使用XML模式指定的。 RESTful API 通常使用 JSON（没有正式指定的模式）用于响应，以及用于请求的JSON或URI编码/表单编码的请求参数。 服务的提供者无法控制其客户，所以可能无限期保持兼容性。 对于 RESTful API，常用方法是在 URL 或者 HTTP Accept 头部使用版本号。\n消息传递中的数据流 与直接RPC相比，使用消息代理（消息队列）有几个优点：\n如果收件人不可用或过载，可以充当缓冲区，从而提高系统的可靠性。 它可以自动将消息重新发送到已经崩溃的进程，从而防止消息丢失。 避免发件人需要知道收件人的IP地址和端口号（这在虚拟机经常出入的云部署中特别有用）。 它允许将一条消息发送给多个收件人。 将发件人与收件人逻辑分离（发件人只是发布邮件，不关心使用者）。 与 PRC 相比，差异在于\n● 消息传递通常是单向的：发送者通常不期望收到其消息的回复。 ● 通信模式是异步的：发送者不会等待消息被传递，而只是发送它，然后忘记它。 消息代理 RabbitMQ，ActiveMQ，HornetQ，NATS和Apache Kafka这样的开源实现已经流行起来。 通常情况下，消息代理的使用方式如下： 一个进程将消息发送到指定的队列或主题； 代理确保将消息传递给那个队列或主题的一个或多个消费者或订阅者。 在同一主题上可以有许多生产者和许多消费者。 一个主题只提供单向数据流。但是，消费者本身可能会将消息发布到另一个主题上，或者发送给原始消息的发送者使用的回复队列（允许请求/响应数据流，类似于RPC）。 消息代理通常不会执行任何特定的数据模型，消息知识包含一些元数据的字节序列，可以用任何编码格式。 如果消费者重新发布消息到另一个主题，则消息保留未知字段，防止前面数据库环境中描述的问题。 分布式的Actor框架 Actor模型是单个进程中并发的编程模型。 逻辑被封装在actor中，而不是直接处理线程（以及竞争条件，锁定和死锁的相关问题）。 每个actor通常代表一个客户或实体，它可能有一些本地状态（不与其他任何角色共享），它通过发送和接收异步消息与其他角色通信。 不保证消息传送：在某些错误情况下，消息将丢失。 由于每个角色一次只能处理一条消息，因此不需要担心线程，每个角色可以由框架独立调度。 分布式Actor框架\n在分布式Actor框架中，此编程模型用于跨多个节点伸缩应用程序。 不管发送方和接收方是在同一个节点上还是在不同的节点上，都使用相同的消息传递机制。 如果它们在不同的节点上，则该消息被透明地编码成字节序列，通过网络发送，并在另一侧解码。 位置透明\n位置透明在actor模型中比在RPC中效果更好，因为actor模型已经假定消息可能会丢失，即使在单个进程中也是如此 尽管网络上的延迟可能比同一个进程中的延迟更高，但是在使用actor模型时，本地和远程通信之间的基本不匹配是较少的。 升级\n分布式的Actor框架实质上是将消息代理和actor编程模型集成到一个框架中。 升级仍然要担心向前和向后兼容问题。 三个流行的分布式actor框架处理消息编码如下：\n默认情况下，Akka使用Java的内置序列化，不提供前向或后向兼容性。 但是，你可以用类似Prototol Buffers的东西替代它，从而获得滚动升级的能力。 Orleans 默认使用不支持滚动升级部署的自定义数据编码格式; 要部署新版本的应用程序，您需要设置一个新的集群，将流量从旧集群迁移到新集群，然后关闭旧集群。 像Akka一样，可以使用自定义序列化插件。 在Erlang OTP中，对记录模式进行更改是非常困难的（尽管系统具有许多为高可用性设计的功能）。 滚动升级是可能的，但需要仔细计划。 本章小结 本章探讨了编码数据结构的方式。 许多服务需要支持滚动升级：向前、向后兼容性。 我们讨论了几种数据编码格式及其兼容性属性： 编程语言特定的编码仅限于单一编程语言，并且往往无法提供前向和后向兼容性。 JSON，XML和CSV等文本格式非常普遍，其兼容性取决于您如何使用它们。他们有可选的模式语言，这有时是有用的，有时是一个障碍。这些格式对于数据类型有些模糊，所以你必须小心数字和二进制字符串。 像Thrift，Protocol Buffers和Avro这样的二进制模式驱动格式允许使用清晰定义的前向和后向兼容性语义进行紧凑，高效的编码。这些模式可以用于静态类型语言的文档和代码生成。但是，他们有一个缺点，就是在数据可读之前需要对数据进行解码。 我们还讨论了数据流的几种模式，说明了数据编码重要性的不同场景： 数据库，写入数据库的进程对数据进行编码，并从数据库读取进程对其进行解码 RPC和REST API，客户端对请求进行编码，服务器对请求进行解码并对响应进行编码，客户端最终对响应进行解码 异步消息传递（使用消息代理或参与者），其中节点之间通过发送消息进行通信，消息由发送者编码并由接收者解码 结论：前向兼容性和滚动升级在某种程度上是可以实现的。\n","permalink":"https://csqread.top/posts/tech/ddia%E7%AC%AC%E5%9B%9B%E7%AB%A0/","summary":"数据编码与演化 应用程序总是增增改改。 修改程序大多数情况下也在修改存储的数据。 关系数据库通常假定数据库中的所有数据都遵循一个模式：尽管可以更改","title":"DDIA第四章"},{"content":"单向度的人：探寻马尔库塞的思想之旅 单向度社会：异化与标准化 《单向度的人》于20世纪60年代问世，正值工业和科技带来的变革迅猛发展。然而，马尔库塞并不乐观，他提出了“单向度”的概念，指的是人们的思想、欲望和行为受到同质化、标准化的社会结构制约，使个体丧失多样性和独立性。在单向度社会中，个体日益成为机械的、被规范化的社会工具，从而失去了真正的自由。\n马尔库塞强调了现代社会的异化现象，即个体与生产活动、社会关系和人性之间的脱节。在单向度社会中，人们似乎不断追逐着物质的满足，却忽视了更深层次的人类需求和价值。他以“欧洲人的追求”为例，揭示了虚假的欲望满足，以及消费主义背后的虚幻与空洞。\n思想控制：媒体与信息的角色 在《单向度的人》中，马尔库塞对媒体的影响进行了深入探讨。他指出，媒体在单向度社会中具有重要地位，通过传播大众化、标准化的信息，操控着人们的思想和意识。这种思想控制使个体丧失了独立思考和批判性思维的能力，成为被动、易于控制的群体。\n他特别强调了“幸福的自我放纵”现象，即媒体通过满足人们的消费和娱乐需求，将个体引入肤浅的享乐境界，导致他们远离深层次的社会问题和权力结构。这种自我放纵不仅削弱了人们的社会责任感，还进一步加深了个体与社会的脱节。\n解放与反抗的可能性 尽管《单向度的人》对现代社会进行了深刻的批判，马尔库塞并未认为个体完全无法摆脱单向度的束缚。他认为，解放与反抗的可能性依然存在。他呼吁个体通过重塑批判性思维、自由意志和人性的方式，寻找反抗和解放的途径。\n马尔库塞主张个体摆脱消费主义的诱惑，重拾对真正人类价值的关注。他认为，个体应通过文化创造和自我认知，唤醒批判性意识，看清社会中的虚假与异化。只有在个体不断地反思和行动中，社会的变革与进步才能实现。\n反思与重建：平衡个体与社会 《单向度的人》是一部充满洞察力的作品，揭示了现代社会中个体与社会之间的复杂关系。面对信息过载和消费主义的压力，我们应该保持独立思考的能力，不被虚假的追求所迷惑。通过个体的反思与行动，或许我们能够找到一条通向解放与平衡的道路。\n在当下，我们也可以从《单向度的人》中获得启示。以批判性思维审视自己的生活，重新思考个体与社会的关系，或许能够帮助我们摆脱单向度的束缚，追求更深层次的人类价值。\n一些摘录 ”可以肯定，”事物的客观秩序“本身是统治的结果；但同样的真实是，统治也正在产生更高的合理性，即一边维护等级结构，一边又更有效地剥削自然资源和智力资源，并在更大范围内分配剥削所得。“\n”他们相信他们正在为阶级而死，他们却是为党的儿子而死。他们相信自己正在为祖国而死，他们却是在为实业家而死。他们相信自己正在为人的自由而死，他们却是在为红利的自由而死。他们会相信自己正在为无产阶级而死，他们却是在为无产阶级的官僚而死“\n”不是工作的自由就是挨饿的自由，它给绝大多数人带来了艰辛、不安和焦虑。假如个人不再作为一个自由的经济主体被迫在市场上出售他自身，那么，这种自由的消失将是文明的最大成就之一。 社会要求个人在多大程度上作抑制性的发展，个人的需要本身及满足这种需要的权利就在多大程度上服从于凌驾其上的批判标准。 一切解放都有赖于对奴役状态的觉悟，而这种觉悟的出现却往往被占主导地位的需要和满足所阻碍，这些需要和满足在很大程度上已成为个人自己的需要和满足。发展的过程往往是用另一种制度取代预定的制度；而最可去的目标则是用真实的需要代替虚假的需要，抛弃异质性的满足。 决定人类自由程度的决定性因素，不是可供个人选择的范围，而是个人能够选择的是什么和实际选择的是什么。 何况个人自发地重复所强加的需要并不说明他的意志自由，而只能证明控制的有效性。 诚然，在当代社会最高度发达的地区，把社会需要移植成个人的需要是如此地有效，以致它们之间的差别似乎纯粹是理论上的事情。人们当真能对作为新闻与娱乐的工具和作为灌输与操纵力量的大众传播媒介作出区分吗？当真能对制造公害的汽车和提供方便的汽车作出区分吗？当真能对实用建筑的恐怖与舒适作出区分吗？当真能对为保卫国防和为公司营利的手段作出区分吗？当真能对提高生育率方面私人的乐趣和商业上、政治上的功能作出区分吗？ 小轿车、高清晰度的传真装置、错层式家庭住宅以及厨房设备成了人们生活的灵魂。把个人束缚于社会的机制已经改变，而社会控制就是在它所产生的新的需要中得以稳定的。 为了和平的组织不同于为了战争的组织；为了生存斗争服务的制度不能为生存和平服务。作为目的的生活本质上不同于作为手段的生活。 技术的合理性展示出它的政治特性，因为它变成更有效统治的得力工具，并创造出一个真正的极权主义领域，在这个领域中，社会和自然、精神和肉体为保卫这一领域而保持着持久动员的状态。“\n”当竞选领袖和政治家在电视、电台和舞台上说出自由、完善这些伟大的字眼的时候，这些字眼就变成了毫无疑义的声音，它们只有在宣传、商业、训练和消遣中才能获得意义。理想与现实同化到这种程度，说明理想已被超越。它被从心灵、精神或内心世界的高尚领域里拽了出来，并被转换为操作型术语和问题。“\n”今天的新奇之处是通过消除高层文化中对立的、异己的和超越性的因素——它们借助高层文化而构成现实的另一种向度——来消除文化和社会现实之间的对立。清除双向度文化的办法，不是否定和拒斥各种“文化价值”，而是把它们全部纳入已确立的秩序，并大规模地复制和显示它们。“\nEND 这部著作的中心论题是：当代工业社会是一个新型的极权主义社会，因为它成功地压制了这个社会中的反对派和反对意见，压制了人们内心中的否定性、批判性和超越性的向度，从而使这个社会成了单向度的社会，使生活于其中的人成了单向度的人。 在马尔库塞看来，极权主义的共同特征主要不是表现为是否施行恐怖与暴力，而是表现为是否允许对立派对、对立意见、对立向度的存在。 简而言之，由于技术进步的作用，发达工业社会虽是一个不自由的社会，但毕竟是一个舒舒服服的不自由社会；虽是一个更有效地控制着人的极权主义社会，但毕竟是一个使人安然自得的极权主义社会。\n推荐这本马尔库塞的《单向度的人》，虽然里边有很多看不懂的地方，但是看他的社会批判真的很过瘾。\n","permalink":"https://csqread.top/posts/read/%E5%8D%95%E5%90%91%E5%BA%A6%E7%9A%84%E4%BA%BA/","summary":"单向度的人：探寻马尔库塞的思想之旅 单向度社会：异化与标准化 《单向度的人》于20世纪60年代问世，正值工业和科技带来的变革迅猛发展。然而，马尔","title":"单向度的人"},{"content":"第三章：存储与检索 本章介绍了传统关系型数据库与“NoSQL”数据库的存储引擎。 我们会研究两大类存储引擎：日志结构（log-structured） 的存储引擎，以及面向页面（page-oriented） 的存储引擎（例如B树）。\n驱动数据库的数据结构 世界上最简单的数据库可以用两个Bash函数实现：\n1 2 3 4 5 6 7 8 #!/bin/bash db_set () { echo \u0026#34;$1,$2\u0026#34; \u0026gt;\u0026gt; database } db_get () { grep \u0026#34;^$1,\u0026#34; database | sed -e \u0026#34;s/^$1,//\u0026#34; | tail -n 1 } 这两个函数实现了键值存储的功能。\n执行 db_set key value ，会将 键（key）和值（value） 存储在数据库中。键和值（几乎）可以是你喜欢的任何东西，例如，值可以是JSON文档。 调用 db_get key ，查找与该键关联的最新值并将其返回。 麻雀虽小，五脏俱全：\n1 2 3 4 5 6 $ db_set 123456 \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;London\u0026#34;,\u0026#34;attractions\u0026#34;:[\u0026#34;Big Ben\u0026#34;,\u0026#34;London Eye\u0026#34;]}\u0026#39; $ $ db_set 42 \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;San Francisco\u0026#34;,\u0026#34;attractions\u0026#34;:[\u0026#34;Golden Gate Bridge\u0026#34;]}\u0026#39; $ db_get 42 {\u0026#34;name\u0026#34;:\u0026#34;San Francisco\u0026#34;,\u0026#34;attractions\u0026#34;:[\u0026#34;Golden Gate Bridge\u0026#34;]} huhu 底层的存储格式非常简单：\n一个文本文件，每行包含一条逗号分隔的键值对（忽略转义问题的话，大致与CSV文件类似）。 每次对 db_set 的调用都会向文件末尾追加记录，所以更新键的时候旧版本的值不会被覆盖。 查找最新值的时候，需要找到文件中键最后一次出现的位置（因此 db_get 中使用了 tail -n 1 ) 1 2 3 4 5 6 7 8 9 $ db_set 42 \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;San Francisco\u0026#34;,\u0026#34;attractions\u0026#34;:[\u0026#34;Exploratorium\u0026#34;]}\u0026#39; $ db_get 42 {\u0026#34;name\u0026#34;:\u0026#34;San Francisco\u0026#34;,\u0026#34;attractions\u0026#34;:[\u0026#34;Exploratorium\u0026#34;]} $ cat database 123456,{\u0026#34;name\u0026#34;:\u0026#34;London\u0026#34;,\u0026#34;attractions\u0026#34;:[\u0026#34;Big Ben\u0026#34;,\u0026#34;London Eye\u0026#34;]} 42,{\u0026#34;name\u0026#34;:\u0026#34;San Francisco\u0026#34;,\u0026#34;attractions\u0026#34;:[\u0026#34;Golden Gate Bridge\u0026#34;]} 42,{\u0026#34;name\u0026#34;:\u0026#34;San Francisco\u0026#34;,\u0026#34;attractions\u0026#34;:[\u0026#34;Exploratorium\u0026#34;]} db_set 函数对于极其简单的场景其实有非常好的性能，因为在文件尾部追加写入通常是非常高效的。 与db_set做的事情类似，许多数据库在内部使用了日志（log），也就是一个 仅追加（append-only） 的数据文件。 另一方面，如果这个数据库中有着大量记录，则这个db_get 函数的性能会非常糟糕。 每次你想查找一个键时，db_get 必须从头到尾扫描整个数据库文件来查找键的出现。 用算法的语言来说，查找的开销是 O(n) ：如果数据库记录数量 n 翻了一倍，查找时间也要翻一倍。 索引 为了高效查找数据库中特定键的值，我们需要一个数据结构：索引（index）。\n索引背后的大致思想是，保存一些额外的元数据作为路标，帮助你找到想要的数据。 索引是从主数据衍生的附加（additional）结构。 许多数据库允许添加与删除索引，这不会影响数据的内容，它只影响查询的性能。 维护额外的结构会产生开销，特别是在写入时。写入性能很难超过简单地追加写入文件，因为追加写入是最简单的写入操作。 任何类型的索引通常都会减慢写入速度，因为每次写入数据时都需要更新索引。 这是存储系统中一个重要的权衡：精心选择的索引加快了读查询的速度，但是每个索引都会拖慢写入速度。 哈希索引 键值存储与在大多数编程语言中可以找到的字典（dictionary）类型非常相似，通常字典都是用散列映射（hash map）（或哈希表（hash table））实现的。 假设我们的数据存储只是一个追加写入的文件，最简单的索引策略就是：保留一个内存中的哈希映射，其中每个键都映射到一个数据文件中的字节偏移量，指明了可以找到对应值的位置\n新的键值写入文件时，还要更新散列映射。 查找一个值时，使用哈希映射来查找数据文件中的偏移量，寻找（seek） 该位置并读取该值。 Bitcask实际上就是这么做的，非常适合每个键的值频繁更新的情况。 如果一直追加文件，怎么防止用完磁盘空间？ 将日志分为特定大小的段，当日志增长到特定尺寸时关闭当前段文件，并开始写入一个新的段文件。 然后，我们就可以对这些段进行压缩（compaction）。 压缩意味着在日志中丢弃重复的键，只保留每个键的最近更新。 也可以把多个段的内容压缩合并到一起。\n段被写入后永远不会被修改，所以合并的段被写入一个新的文件。 冻结段的合并和压缩可以在后台线程中完成，在进行时，我们仍然可以继续使用旧的段文件来正常提供读写请求。 合并过程完成后，我们将读取请求转换为使用新的合并段而不是旧段 —— 然后可以简单地删除旧的段文件。 每个段都有自己的内存散列表，将键映射到文件偏移量。查找键时，依次遍历所有的散列表。 重要的问题：\n文件格式 a. csv 不好，使用二进制格式更快。 2. 删除记录 a. 删除一个键，则必须在数据文件（有时称为逻辑删除）中附加一个特殊的删除记录。 b. 当日志段被合并时，逻辑删除告诉合并过程放弃删除键的任何以前的值。 3. 崩溃恢复 a. 如果数据库重新启动，则内存散列映射将丢失。 b. 可以根据日志文件重新恢复每个段的哈希映射，但段很大的时候，很费时间。 c. Bitcask通过存储加速恢复磁盘上每个段的哈希映射的快照，可以更快地加载到内存中。 4. 部分写入记录 a. 数据库可能随时崩溃，包括将记录附加到日志中途。 b. Bitcask文件包含校验和，允许检测和忽略日志的这些损坏部分。 5. 并发控制 a. 写操作是以严格顺序的顺序附加到日志中的，所以常见的方法是只有一个写线程。 b. 读操作可以有多个线程同时读取。 为什么采用追加模式，而不是不更新文件，用新值覆盖旧值？ 原因有几个：\n追加和分段合并是顺序写入操作，通常比随机写入快得多，尤其是在磁盘旋转硬盘上。在某种程度上，顺序写入在基于闪存的 固态硬盘（SSD） 上也是优选的。 如果段文件是附加的或不可变的，并发和崩溃恢复就简单多了。 合并旧段可以避免数据文件随着时间的推移而分散的问题。 哈希表索引的局限性：\n散列表必须能放进内存：当有很多键的时候，Hash 冲突，占用内存。 范围查询效率不高：不支持查一个取值区间内的所有键。 SSTables和LSM树 在之前的存储中，每个日志结构存储段都是一系列键值对。这些对按照它们写入的顺序出现，而不是键值对的顺序。 我们做一个简单的改变：我们要求键值对的序列按键排序。把这个格式称为排序字符串表（Sorted String Table），简称 SSTable。同时，要求每个键只在每个合并的段文件中出现一次（压缩过程已经保证）。\nSSTable 与散列索引日志段相比，有几个很大的优势：\n合并段是简单而高效的，即使文件大于可用内存。方法类似于归并排序。 a. 如果在几个输入段中出现相同的键，该怎么办？ ⅰ. 答：保留最近段的值，并丢弃旧段中的值。\n为了在文件中找到一个特定的键，你不再需要保存内存中所有键的索引。因为是有序的，可以先查找出键所处的范围，然后就找到这个键所在的偏移量的区间。比如可以从 handbag 和 handsome 的偏移量找到 handiwork 的区间。 构建和维护SSTables 如何让数据首先被按键排序呢？\n在内存中排序简单的多，比如红黑树、AVL 树； 存储工作的操作步骤：\n写入时，将其添加到内存中的平衡树数据结构（例如，红黑树）。这个内存树有时被称为内存表（memtable）。 当内存表大于某个阈值（通常为几兆字节）时，将其作为SSTable文件写入磁盘。这可以高效地完成，因为树已经维护了按键排序的键值对。新的SSTable文件成为数据库的最新部分。当SSTable被写入磁盘时，写入可以继续到一个新的内存表实例。 为了提供读取请求，首先尝试在内存表中找到关键字，然后在最近的磁盘段中，然后在下一个较旧的段中找到该关键字。 有时会在后台运行合并和压缩过程以组合段文件并丢弃覆盖或删除的值。 如何解决数据库崩溃，则最近的写入（在内存表中，但尚未写入磁盘）将丢失的问题？\n我们可以在磁盘上保存一个单独的日志，每个写入都会立即被附加到磁盘上， 该日志不是按排序顺序，但这并不重要，因为它的唯一目的是在崩溃后恢复内存表。 每当内存表写出到SSTable时，相应的日志都可以被丢弃。 用SSTables制作LSM树 LSM 树：在 LevelDB 和 RocksDB 使用。\n性能优化 当查找数据库中不存在的键时，LSM树算法可能会很慢：您必须检查内存表，然后将这些段一直回到最老的（可能必须从磁盘读取每一个），然后才能确定键不存在。 解决办法：布隆过滤器。 2. SSTables 被压缩和合并的顺序和时间 大小分层压实。 LevelDB和RocksDB使用平坦压缩（LevelDB因此得名）； HBase 使用大小分层； Cassandra 同时支持。 B树 像SSTables一样，B树保持按键排序的键值对，这允许高效的键值查找和范围查询。 不同：\n日志结构索引将数据库分解为可变大小的段，通常是几兆字节或更大的大小，并且总是按顺序编写段。 B 树将数据库分解成固定大小的块或页面，传统上大小为4KB（有时会更大），并且一次只能读取或写入一个页面。 这种设计更接近于底层硬件，因为磁盘也被安排在固定大小的块中。 每个页面都可以使用地址或位置来标识，这允许一个页面引用另一个页面 —— 类似于指针，但在磁盘而不是在内存中。\n查找过程： ● 一个页面会被指定为B树的根；在索引中查找一个键时，就从这里开始。 ● 该页面包含几个键和对子页面的引用。 ● 每个子页面负责一段连续范围的键，引用之间的键，指明了引用子页面的键范围。 ● 最后，我们可以看到包含单个键（叶页）的页面，该页面包含每个键的内联值，或者包含对可以找到值的页面的引用。\n更新过程： ● 搜索包含该键的叶页，更改该页中的值，并将该页写回到磁盘原来的位置（对该页的任何引用保持有效）\n插入过程： ● 找到其范围包含新键的页面，并将其添加到该页面。 ● 如果页面中没有足够的可用空间容纳新键，则将其分成两个半满页面，并更新父页面以解释键范围的新分区。 删除过程： ● 删除一个键（同时保持树平衡）就牵扯很多其他东西了。\n深度： ● 该算法确保树保持平衡：具有 n 个键的B树总是具有 $O(log n)$ 的深度。 ● 大多数数据库可以放入一个三到四层的 B 树，所以你不需要遵追踪多页面引用来找到你正在查找的页面。 （分支因子为 500 的 4KB 页面的四级树可以存储多达 256TB 。）\n让B树更可靠 ● B 树的基本底层写操作是用新数据覆盖磁盘上的页面。 ● 假定覆盖不改变页面的位置； ● 而日志结构索引（如LSM树）只附加到文件（并最终删除过时的文件），但从不修改文件。\n危险操作： ● 插入导致页面过度而拆分页面，则需要编写已拆分的两个页面，并覆盖其父页面以更新对两个子页面的引用。 ● 这是一个危险的操作，因为如果数据库在仅有一些页面被写入后崩溃，那么最终将导致一个损坏的索引（例如，可能有一个孤儿页面不是任何父项的子项） 。\n预写式日志（WAL） ● 为了使数据库对崩溃具有韧性，B树实现通常会带有一个额外的磁盘数据结构：预写式日志（WAL, write-ahead-log）（也称为重做日志（redo log））。 ● 这是一个仅追加的文件，每个B树修改都可以应用到树本身的页面上。 ● 当数据库在崩溃后恢复时，这个日志被用来使B树恢复到一致的状态.\n并发访问： ● 更新页面时，如果多个线程要同时访问B树，则需要仔细的并发控制，否则可能会看到树处于不一致的状态。 ● 通过使用锁存器（latches）（轻量级锁）保护树的数据结构来完成 ● 而 LSM 比较简单：在后台完成所有的合并，不干扰查询；通过「原子交换」把旧的分段变为新的分段。\nB树优化 ● LMDB 数据库中使用「写时复制方案 」，而不是不是覆盖页面并维护 WAL 进行崩溃恢复。 ○ 修改的页面被写入到不同的位置，并且树中的父页面的新版本被创建，指向新的位置。 ○ 这种方法对于并发控制也很有用。 ● 保存键的缩略信息，而不是完整的键。 ○ 键只用保存一个区间，而不是具体的数值（类似于 B+树）。 ○ 可以包含更多的键，减少树的层次。 ● 不方便扫描大部分关键词的范围查找。 ○ 许多B树实现尝试布局树，使得叶子页面按顺序出现在磁盘上。 ○ 但是，随着树的增长，维持这个顺序是很困难的。 ○ 相比之下，由于LSM树在合并过程中一次又一次地重写存储的大部分，所以它们更容易使顺序键在磁盘上彼此靠近。 ● 额外的指针已添加到树中。 ○ 例如，每个叶子页面可以在左边和右边具有对其兄弟页面的引用，这允许不跳回父页面就能顺序扫描。 ● B树的变体如分形树借用一些日志结构的思想来减少磁盘寻道（而且它们与分形无关）。\n比较B树和LSM树 通常LSM树的写入速度更快，而B树的读取速度更快。 LSM树上的读取通常比较慢，因为它们必须在压缩的不同阶段检查几个不同的数据结构和SSTables。\nLSM树的优点 ● B树索引必须至少两次写入每一段数据：一次写入预先写入日志，一次写入树页面本身（也许再次分页） ● 即使在该页面中只有几个字节发生了变化，也需要一次编写整个页面的开销。 ● 有些存储引擎甚至会覆盖同一个页面两次，以免在电源故障的情况下导致页面部分更新\n写放大 ● 反复压缩和合并SSTables，日志结构索引也会重写数据。 ● 在数据库的生命周期中写入数据库导致对磁盘的多次写入 —— 被称为写放大（write amplification）。 ● 存储引擎写入磁盘的次数越多，可用磁盘带宽内的每秒写入次数越少。\nLSM树通常能够比B树支持更高的写入吞吐量： ● 部分原因是它们有时具有较低的写放大（尽管这取决于存储引擎配置和工作负载） ● 部分是因为它们顺序地写入紧凑的SSTable文件而不是必须覆盖树中的几个页面 ● 这种差异在磁性硬盘驱动器上尤其重要，顺序写入比随机写入快得多。\n文件碎片： ● LSM树可以被压缩得更好，因此经常比B树在磁盘上产生更小的文件。 ● B树存储引擎会由于分割而留下一些未使用的磁盘空间 ● 由于LSM树不是面向页面的，并且定期重写SSTables以去除碎片，所以它们具有较低的存储开销，特别是当使用平坦压缩时\nLSM树的缺点 日志结构存储的缺点是压缩过程有时会干扰正在进行的读写操作。 B树的行为则相对更具可预测性。\n高写入吞吐量： ● 磁盘的有限写入带宽需要在初始写入（记录和刷新内存表到磁盘）和在后台运行的压缩线程之间共享。 ● 写入空数据库时，可以使用全磁盘带宽进行初始写入，但数据库越大，压缩所需的磁盘带宽就越多。\n压缩和读取的速度： ● 如果写入吞吐量很高，并且压缩没有仔细配置，压缩跟不上写入速率。 ● 在这种情况下，磁盘上未合并段的数量不断增加，直到磁盘空间用完，读取速度也会减慢，因为它们需要检查更多段文件。\n键出现的个数： ● B树的一个优点是每个键只存在于索引中的一个位置，而日志结构化的存储引擎可能在不同的段中有相同键的多个副本。 ● 有利于事务语义。 ● 在许多关系数据库中，事务隔离是通过在键范围上使用锁来实现的，在B树索引中，这些锁可以直接连接到树\n其他索引结构 前面讨论的是关键值索引，它们就像关系模型中的主键（primary key） 索引。主键唯一标识关系表中的一行、一个文档、一个顶点。\n二级索引 ● 在关系数据库中，您可以使用 CREATE INDEX 命令在同一个表上创建多个二级索引，而且这些索引通常对于有效地执行联接而言至关重要。 ● 一个二级索引可以很容易地从一个键值索引构建，区别是键不是唯一的。实现方式： ○ 通过使索引中的每个值，成为匹配行标识符的列表（如全文索引中的发布列表） ○ 通过向每个索引添加行标识符来使每个关键字唯一 ○ 无论哪种方式，B树和日志结构索引都可以用作辅助索引。\n将值存储在索引中 索引中的关键字是查询搜索的内容，它属于两种之一： ● 实际行（文档，顶点） ● 对存储在别处的行的引用。此时，行被存储的地方被称为堆文件（heap file），并且存储的数据没有特定的顺序 ○ 堆文件很常见。 ○ 它避免了在存在多个二级索引时复制数据 ○ 新值字节数不大于旧值，原地覆盖； ○ 新值更大，需要移到堆中有足够空间的新位置； ■ 此时，要么所有索引都更新指向新堆位置； ■ 要么在旧堆位置留一个转发指针。\n聚集索引 ● 从索引到堆文件的额外跳转性能损失太大，希望可以把索引行直接进存储到索引中。被叫做聚集索引。 ● 在 MySQL 的 InnoDB 存储引擎中，表的主键总是一个聚簇索引，二级索引用主键（而不是堆文件中的位置） ● 在SQL Server中，可以为每个表指定一个聚簇索引。\n包含列的索引或覆盖索引 ● 在 聚集索引（clustered index） （在索引中存储所有行数据）和 非聚集索引（nonclustered index） （仅在索引中存储对数据的引用）之间的折衷被称为 包含列的索引（index with included columns）或覆盖索引（covering index），其存储表的一部分在索引内。 ● 允许通过单独使用索引来回答一些查询。 ● 加快了读取速度，但是增加了额外的存储空间，增加了写入开销，还要事务保证。\n多列索引 上面讨论的都是一个 key 对应一个 value，如果需要同时根据一个表中的多个列（或文档中的多个字段）进行查询，则不行。 ● 连接索引（concatenated index） ○ 将一列的值追加到另一列后面，简单地将多个字段组合成一个键。 ○ 但是这不能做复杂查询。 ● 多维索引（multi-dimensional index） ○ 比如需要根据经纬度做二维范围查询，则 B 树和 LSM 树不能高效； ○ 普遍做法是用特殊化的空间索引，比如 R 树（TODO）。 ○ 多维索引除了地理位置，还可以用于其他很多地方：电子网站、天气数据。\n全文搜索和模糊索引 上文讨论的索引都是查询确切的值或者确定范围的值，如果搜索类似的键，需要用模糊查询。\n全文搜索引擎 ● 允许搜索一个单词以扩展为包括该单词的同义词，忽略单词的语法变体，并且搜索在相同文档中彼此靠近的单词的出现，并且支持各种其他功能取决于文本的语言分析。 ● 为了处理文档或查询中的拼写错误，Lucene能够在一定的编辑距离内搜索文本（编辑距离1意味着添加，删除或替换了一个字母）\nLucene ● 为其词典使用了一个类似于SSTable的结构。 ● 这个结构需要一个小的内存索引，告诉查询在排序文件中哪个偏移量需要查找关键字。 ● 在 LevelDB 中，这个内存中的索引是一些键的稀疏集合。 ● 但在 Lucene 中，内存中的索引是键中字符的有限状态自动机，类似于trie。 ● 这个自动机可以转换成 Levenshtein 自动机，它支持在给定的编辑距离内有效地搜索单词。\n在内存中存储一切 对于磁盘和SSD，如果要在读取和写入时获得良好性能，则需要仔细地布置磁盘上的数据。 磁盘优点：耐用，成本低。 内存变得便宜，促进了内存数据库发展。\n内存数据库 ● 某些内存中的键值存储（如Memcached）仅用于缓存，在重新启动计算机时丢失的数据是可以接受的。 ● 但其他内存数据库的目标是持久性，可以通过特殊的硬件（例如电池供电的RAM），将更改日志写入磁盘，将定时快照写入磁盘或通过复制内存来实现，记忆状态到其他机器。\n实现 ● 内存数据库重新启动时，需要从磁盘或通过网络从副本重新加载其状态（除非使用特殊的硬件）。 ● 尽管写入磁盘，它仍然是一个内存数据库，因为磁盘仅用作耐久性附加日志，读取完全由内存提供。 ● 写入磁盘也具有操作优势：磁盘上的文件可以很容易地由外部实用程序进行备份，检查和分析。\n常见产品 ● VoltDB，MemSQL和Oracle TimesTen等产品是具有关系模型的内存数据库 ● RAM Cloud是一个开源的内存键值存储器，具有持久性（对存储器中的数据以及磁盘上的数据使用日志结构化方法） ● Redis和Couchbase通过异步写入磁盘提供了较弱的持久性。\n内存数据库性能优势到底在哪？ ● 内存数据库的性能优势并不是因为它们不需要从磁盘读取的事实。 ● 即使是基于磁盘的存储引擎也可能永远不需要从磁盘读取，因为操作系统缓存最近在内存中使用了磁盘块。 ● 相反，它们更快的原因在于省去了将内存数据结构编码为磁盘数据结构的开销。\n除了性能还有什么优势？ ● 提供了难以用基于磁盘的索引实现的数据模型。 ● 例如，Redis为各种数据结构（如优先级队列和集合）提供了类似数据库的接口。因为它将所有数据保存在内存中，所以它的实现相对简单。\n内存不够用怎么办？ ● 反缓存（anti-caching） 方法通过在内存不足的情况下将最近最少使用的数据从内存转移到磁盘，并在将来再次访问时将其重新加载到内存中。 ● 这与操作系统对虚拟内存和交换文件的操作类似，但数据库可以比操作系统更有效地管理内存，因为它可以按单个记录的粒度工作，而不是整个内存页面。 ● 尽管如此，这种方法仍然需要索引能完全放入内存中。\n事务处理还是分析？ 事务处理 ● 早起的业务处理，典型的数据库写入与一笔商业交易（transaction）相对应，以后交易/事务（transaction） 仍留了下来。 ● 事务不一定具有ACID（原子性，一致性，隔离性和持久性）属性。 ● 事务处理只是意味着允许客户端进行低延迟读取和写入 —— 而不是只能定期运行（例如每天一次）的批量处理作业。 ● 数据库仍然常被用作在线事务处理（OLTP, OnLine Transaction Processing） 。\n数据分析 ● 数据库也开始越来越多地用于数据分析，需要扫描大量记录； ● 为了将这种使用数据库的模式和事务处理区分开，它被称为在线分析处理（OLAP, OnLine Analytice Processing）\n属性 事务处理 OLTP 分析系统 OLAP 主要读取模式 查询少量记录，按键读取 在大批量记录上聚合 主要写入模式 随机访问，写入要求低延时 批量导入（ETL），事件流 主要用户 终端用户，通过Web应用 内部数据分析师，决策支持 处理的数据 数据的最新状态（当前时间点） 随时间推移的历史事件 数据集尺寸 GB ~ TB TB ~ PB/td\u003e 起初，相同的数据库用于事务处理和分析查询，SQL 可以支持 OLTP 和 OLAP。现在有趋势在用数据仓库。 数据仓库 OLTP 系统对业务运作很重要，因而通常会要求 高可用 与 低延迟，不愿意做影响事务性能的分析查询。 数据仓库是一个独立的数据库，分析人员可以查询他们想要的内容而不影响OLTP操作 数据仓库包含公司各种OLTP系统中所有的只读数据副本。 从OLTP数据库中提取数据（使用定期的数据转储或连续的更新流），转换成适合分析的模式，清理并加载到数据仓库中。将数据存入仓库的过程称为“抽取-转换-加载（ETL）” 使用单独的数据仓库，而不是直接查询OLTP系统进行分析的一大优势是数据仓库可针对分析访问模式进行优化。上文的索引不适合数据仓库，需要单独的存储引擎。 #### OLTP数据库和数据仓库之间的分歧 数据仓库的数据模型通常是关系型的，因为SQL通常很适合分析查询。 表面上，一个数据仓库和一个关系OLTP数据库看起来很相似，因为它们都有一个SQL查询接口。 实际上，内部完全不同，因为对不同的查询模式进行了优化。 一般的数据库都把重点放在支持事务处理或分析工作负载上，而不是两者都支持。 星型和雪花型：分析的模式 事务处理领域中，使用了大量不同的数据模型。 分析型业务中，数据模型的多样性则少得多。许多数据仓库都以相当公式化的方式使用，被称为星型模式（也称为维度建模）。 用于数据仓库的星型模式的示例 在模式的中心是一个所谓的事实表（在这个例子中，它被称为 fact_sales）。 事实表的每一行代表在特定时间发生的事件（这里，每一行代表客户购买的产品）。 周围是维度表。 事实表可能非常大，有万亿行和数PB的数据，列通常超过 100 列。 被称为“星型模式”是因为事实表在中间，维度表在周围，像星星一样。 “雪花模式”：维度表进一步拆分成子维度表。 “星型模式”比较简单，是首选。 列存储 事实表的高效存储和查询是问题。 维度表比较小，不讨论。 在分析中经常只查询部分列，而不是所有列。 面向列的存储\n不是把每一行的值存储在一起，而是将每一列的所有值存放在一起。 面向列的存储布局依赖于每个列文件包含相同顺序的行。 列压缩 ● 除了仅从磁盘加载查询所需的列以外，我们还可以通过压缩数据来进一步降低对磁盘吞吐量的需求。 ● 面向列的存储通常很适合压缩。\n位图编码 当 n 非常大，大部分位图中将会有很多的零（我们说它们是稀疏的）。 位图可以另外进行游程编码，这可以使列的编码非常紧凑。 位图索引可以很方便的做各种查询：ADN/OR 内存带宽和向量处理 ● 需要扫描数百万行的数据仓库查询来说，瓶颈： ○ 是从磁盘获取数据到内存的带宽。 ○ 主存储器带宽到CPU缓存中的带宽，避免CPU指令处理流水线中的分支错误预测和泡沫，以及在现代中使用单指令多数据（SIMD）指令CPU ● 面向列的存储布局： ○ 可以将大量压缩的列数据放在CPU的L1缓存中，然后在紧密的循环中循环（即没有函数调用） ○ 更有利于 CPU 的计算。\n列存储中的排序顺序 ● 在列存储中，存储行的顺序并不一定很重要。 ● 按插入顺序存储它们是最简单的，因为插入一个新行只需要追加到每个列文件。 ● 每列独自排序是没有意义的，因为那样我们就不会知道列中的哪些项属于同一行 ● 即使按列存储数据，也需要一次对整行进行排序。\n好处 ● 速度快：这样数据库的管理员可以根据他们对常用查询的了解来选择表格应该被排序的列。 ● 压缩列：第一个排序键的压缩效果最强。\n几个不同的排序顺序 ● 不同的查询受益于不同的排序顺序，而无论如何，数据需要复制到多台机器， ● 在一个面向列的存储中有多个排序顺序有点类似于在一个面向行的存储中有多个二级索引。\n写入列存储 列存储的优点：大部分只用查询；压缩和排序都有助于更快地读取这些查询。 缺点：写入困难。插入必须始终更新所有列。\n解决方案：LSM树。 ● 现在内存中存储，添加到一个已排序的结构中，然后准备写入磁盘。\n聚合：数据立方体和物化视图 并不是每个数据仓库都必定是一个列存储。列存储在迅速普及。\n物化汇总 ● 数据仓库查询通常涉及一个聚合函数，如SQL中的COUNT，SUM，AVG，MIN或MAX。 ● 创建这种缓存的一种方式是物化视图（Materialized View）。\n物化数据立方体的优点是某些查询变得非常快，因为它们已经被有效地预先计算了。\n本章小结 优化 事务处理（OLTP） 或 在线分析（OLAP） 。这些用例的访问模式之间有很大的区别： ● OLTP系统通常面向用户，这意味着系统可能会收到大量的请求。为了处理负载，应用程序通常只访问每个查询中的少部分记录。应用程序使用某种键来请求记录，存储引擎使用索引来查找所请求的键的数据。磁盘寻道时间往往是这里的瓶颈。 ● 数据仓库和类似的分析系统会低调一些，因为它们主要由业务分析人员使用，而不是由最终用户使用。它们的查询量要比OLTP系统少得多，但通常每个查询开销高昂，需要在短时间内扫描数百万条记录。磁盘带宽（而不是查找时间）往往是瓶颈，列式存储是这种工作负载越来越流行的解决方案。\n在OLTP方面，我们能看到两派主流的存储引擎：\n日志结构学派\n只允许附加到文件和删除过时的文件，但不会更新已经写入的文件。 Bitcask，SSTables，LSM树，LevelDB，Cassandra，HBase，Lucene等都属于这个类别。\n就地更新学派\n将磁盘视为一组可以覆写的固定大小的页面。 B树是这种哲学的典范，用在所有主要的关系数据库中和许多非关系型数据库。 日志结构的存储引擎是相对较新的发展。他们的主要想法是，他们系统地将随机访问写入顺序写入磁盘，由于硬盘驱动器和固态硬盘的性能特点，可以实现更高的写入吞吐量。在完成OLTP方面，我们通过一些更复杂的索引结构和为保留所有数据而优化的数据库做了一个简短的介绍。\n","permalink":"https://csqread.top/posts/tech/ddia%E7%AC%AC%E4%B8%89%E7%AB%A0/","summary":"第三章：存储与检索 本章介绍了传统关系型数据库与“NoSQL”数据库的存储引擎。 我们会研究两大类存储引擎：日志结构（log-structure","title":"DDIA第三章"},{"content":"第二章：数据模型与查询语言 数据模型可能是软件开发中最重要的部分了，因为它们的影响如此深远：不仅仅影响着软件的编写方式，而且影响着我们的解题思路。 一个复杂的应用程序可能会有更多的中间层次，比如基于API的API，不过基本思想仍然是一样的：每个层都通过提供一个明确的数据模型来隐藏更低层次中的复杂性。这些抽象允许不同的人群有效地协作。\n关系模型与文档模型 现在最著名的数据模型可能是SQL。它基于Edgar Codd在1970年提出的关系模型【1】：数据被组织成关系（SQL中称作表），其中每个关系是元组（SQL中称作行)的无序集合。\nNoSQL的诞生 NoSQL：不仅是SQL（Not Only SQL） 采用NoSQL数据库的背后有几个驱动因素，其中包括： ○ 需要比关系数据库更好的可扩展性，包括非常大的数据集或非常高的写入吞吐量 ○ 相比商业数据库产品，免费和开源软件更受偏爱。 ○ 关系模型不能很好地支持一些特殊的查询操作 ○ 受挫于关系模型的限制性，渴望一种更具多动态性与表现力的数据模型 在可预见的未来，关系数据库似乎可能会继续与各种非关系数据库一起使用 - 这种想法有时也被称为混合持久化（polyglot persistence） 对象关系不匹配 应用程序使用面向对象的语言，需要一个转换层，才能转成 SQL 数据模型：被称为阻抗不匹配。 Hibernate这样的 对象关系映射（ORM object-relational mapping） 框架可以减少这个转换层所需的样板代码的数量，但是它们不能完全隐藏这两个模型之间的差异。 对于一份简历而言，关系型模型需要一对多（比如工作经历）。 而表述这样的简历，使用 JSON 是非常合适的。JSON 比多表模式有更好的局部性，可以一次查询出一个用户的所有信息。JSON 其实是一棵树。 多对一和多对多的关系 为什么在 SQL 中，地域和公司都以 ID，而不是字符串进行标识呢？ ○ ID 对人类没有任何意义，所以永远不需要改变，可以规范化人类的信息。那么就会存在多对一的关系（多个人对应了同一个 ID）。 ○ 在关系数据库 SQL 中，所有使用它的地方可以用 ID 来引用其他表中的行； ○ 但是文档数据库（比如 JSON），对连接支持很弱。 如果数据库不支持链接，那么就需要在应用代码中，对数据库执行多个查询进行模拟。执行连接的工作从数据库被转移到应用程序代码上。 哪怕最开始的应用适合无连接的文档模型，但是随着功能添加，数据会变得更加互联，比如对简历修改：\n组织和学校作为实体：假如组织和学校有主页 推荐：给别人做推荐，当别人的信息更改的时候，所有地方要同步更新。 文档数据库是否在重蹈覆辙？ 20 世纪 70 年代，最受欢迎的是层次模型（hierarchical model），它与文档数据库使用的JSON模型有一些惊人的相似之处。它将所有数据表示为嵌套在记录中的记录树。虽然能处理一对多的关系，但是很难应对多对多的关系，并且不支持链接。\n提出的解决方案：\n关系模型（relational model）（它变成了SQL，统治了世界） 网络模型（network model）（最初很受关注，但最终变得冷门） 网络模型 支持多对多，每条记录可能有多个父节点。 网络模型中记录之间的链接不是外键，而更像编程语言中的指针（同时仍然存储在磁盘上）。访问记录的唯一方法是跟随从根记录起沿这些链路所形成的路径。这被称为访问路径（access path）。 最简单的情况下，访问路径类似遍历链表：从列表头开始，每次查看一条记录，直到找到所需的记录。但在多对多关系的情况中，数条不同的路径可以到达相同的记录，网络模型的程序员必须跟踪这些不同的访问路径。 缺点：查询和更新数据库很麻烦。 关系模型 数据：一个 关系（表） 只是一个 元组（行） 的集合，很简单。 在关系数据库中，查询优化器自动决定查询的哪些部分以哪个顺序执行，以及使用哪些索引。这些选择实际上是“访问路径”，但最大的区别在于它们是由查询优化器自动生成的，不需要程序猿考虑。 与文档数据库相比 但是，在表示多对一和多对多的关系时，关系数据库和文档数据库并没有根本的不同：在这两种情况下，相关项目都被一个唯一的标识符引用，这个标识符在关系模型中被称为外键，在文档模型中称为文档引用。\n关系型数据库与文档数据库在今日的对比 ● 支持文档数据模型的主要论据是架构灵活性，因局部性而拥有更好的性能，以及对于某些应用程序而言更接近于应用程序使用的数据结构。 ● 关系模型通过为连接提供更好的支持以及支持多对一和多对多的关系来反击。\n哪个数据模型更方便写代码？ 文档模型： ● 优点： ○ 如果应用程序中的数据具有类似文档的结构（即，一对多关系树，通常一次性加载整个树），那么使用文档模型可能是一个好主意。 ● 缺点： ○ 不能直接引用文档中的嵌套的项目，而是需要说“用户251的位置列表中的第二项”（很像分层模型中的访问路径）。但是，只要文件嵌套不太深，这通常不是问题。 ○ 文档数据库对连接的糟糕支持也许或也许不是一个问题，这取决于应用程序。 ○ 如果应用程序使用多对多关系，那么文档模型就没有那么吸引人了。 ○ 对于高度相联的数据，选用文档模型是糟糕的，选用关系模型是可接受的，而选用图形模型是最自然的。\n文档模型中的架构灵活性 文档模型是「读时模式」 ○ 文档数据库有时称为无模式（schemaless），但这具有误导性，因为读取数据的代码通常假定某种结构——即存在隐式模式，但不由数据库强制执行 ○ 一个更精确的术语是读时模式（schema-on-read）（数据的结构是隐含的，只有在数据被读取时才被解释），相应的是写时模式（schema-on-write）（传统的关系数据库方法中，模式明确，且数据库确保所有的数据都符合其模式） ○ 读时模式类似于编程语言中的动态（运行时）类型检查，而写时模式类似于静态（编译时）类型检查。 模式变更 ○ 读时模式变更字段很容易，只用改应用代码 ○ 写时模式变更字段速度很慢，而且要求停运。它的这种坏名誉并不是完全应得的：大多数关系数据库系统可在几毫秒内执行ALTER TABLE语句。MySQL是一个值得注意的例外，它执行ALTER TABLE时会复制整个表，这可能意味着在更改一个大型表时会花费几分钟甚至几个小时的停机时间，尽管存在各种工具来解决这个限制。 查询的数据局部性 ● 文档通常以单个连续字符串形式进行存储，编码为JSON，XML或其二进制变体 ● 读文档： ○ 如果应用程序经常需要访问整个文档（例如，将其渲染至网页），那么存储局部性会带来性能优势。 ○ 局部性仅仅适用于同时需要文档绝大部分内容的情况。 ● 写文档： ○ 更新文档时，通常需要整个重写。只有不改变文档大小的修改才可以容易地原地执行。 ○ 通常建议保持相对小的文档，并避免增加文档大小的写入\n文档和关系数据库的融合 ● MySQL 等逐步增加了对 JSON 和 XML 的支持 ● 关系模型和文档模型的混合是未来数据库一条很好的路线。\n数据查询语言 ● 关系模型包含了一种查询数据的新方法：SQL是一种 声明式 查询语言，而IMS和CODASYL使用 命令式 代码来查询数据库。 ● 命令式语言：告诉计算机以特定顺序执行某些操作，比如常见的编程语言。 ● 声明式查询语言（如SQL或关系代数）：你只需指定所需数据的模式 - 结果必须符合哪些条件，以及如何将数据转换（例如，排序，分组和集合） - 但不是如何实现这一目标。数据库系统的查询优化器决定使用哪些索引和哪些连接方法，以及以何种顺序执行查询的各个部分。 ○ SQL相当有限的功能性为数据库提供了更多自动优化的空间。 ○ 声明式语言往往适合并行执行。\nWeb上的声明式查询 ● 声明式语言更加泛化，不用关心底层的数据存储变化 ● 在Web浏览器中，使用声明式CSS样式比使用JavaScript命令式地操作样式要好得多。 ● 类似地，在数据库中，使用像SQL这样的声明式查询语言比使用命令式查询API要好得多。\nMapReduce查询 ● 一些NoSQL数据存储（包括MongoDB和CouchDB）支持有限形式的MapReduce，作为在多个文档中执行只读查询的机制。 ● MapReduce既不是一个声明式的查询语言，也不是一个完全命令式的查询API，而是处于两者之间 ○ 查询的逻辑用代码片断来表示，这些代码片段会被处理框架重复性调用。 ○ 它基于map（也称为collect）和reduce（也称为fold或inject）函数，两个函数存在于许多函数式编程语言中。 ● map和reduce函数在功能上有所限制： ○ 它们必须是纯函数，这意味着它们只使用传递给它们的数据作为输入，它们不能执行额外的数据库查询，也不能有任何副作用。 ○ 这些限制允许数据库以任何顺序运行任何功能，并在失败时重新运行它们。 ○ MapReduce是一个相当底层的编程模型，用于计算机集群上的分布式执行。像SQL这样的更高级的查询语言可以用一系列的MapReduce操作来实现，但是也有很多不使用MapReduce的分布式SQL实现。 ○ MapReduce的一个可用性问题：必须编写两个密切合作的JavaScript函数，这通常比编写单个查询更困难。此外，声明式查询语言为查询优化器提供了更多机会来提高查询的性能。基于这些原因，MongoDB 2.2添加了一种叫做聚合管道的声明式查询语言的支持\n图数据模型 多对多关系是不同数据模型之间具有区别性的重要特征。\n文档模型：适合数据有一对多关系、不存在关系\n图数据模型：适合多对多关系\n一个图由两种对象组成： a. 顶点（vertices）（也称为节点（nodes） 或实体（entities）） b. 边（edges）（ 也称为关系（relationships）或弧 （arcs） ）。\n举例：社交图谱，网络图谱，公路或铁路网络\n图数据结构示例（以社交网络为例）\n存储方式：属性图，三元组\n属性图 在属性图模型中，每个顶点（vertex）包括： ● 唯一的标识符 ● 一组 出边（outgoing edges） ● 一组 入边（ingoing edges） ● 一组属性（键值对） 每条 边（edge） 包括： ● 唯一标识符 ● 边的起点/尾部顶点（tail vertex） ● 边的终点/头部顶点（head vertex） ● 描述两个顶点之间关系类型的标签 ● 一组属性（键值对） 使用关系模式来表示属性图\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TABLE vertices ( vertex_id INTEGER PRIMARY KEY, properties JSON ); CREATE TABLE edges ( edge_id INTEGER PRIMARY KEY, tail_vertex INTEGER REFERENCES vertices (vertex_id), head_vertex INTEGER REFERENCES vertices (vertex_id), label TEXT, properties JSON ); CREATE INDEX edges_tails ON edges (tail_vertex); CREATE INDEX edges_heads ON edges (head_vertex); 关于这个模型的一些重要方面是：\n任何顶点都可以有一条边连接到任何其他顶点。没有模式限制哪种事物可不可以关联。 给定任何顶点，可以高效地找到它的入边和出边，从而遍历图，即沿着一系列顶点的路径前后移动。（这就是为什么例2-2在tail_vertex和head_vertex列上都有索引的原因。） 通过对不同类型的关系使用不同的标签，可以在一个图中存储几种不同的信息，同时仍然保持一个清晰的数据模型。 Cypher查询语言 Cypher是属性图的声明式查询语言，为Neo4j图形数据库而发明 通常对于声明式查询语言来说，在编写查询语句时，不需要指定执行细节：查询优化程序会自动选择预测效率最高的策略，因此你可以继续编写应用程序的其他部分。 查找所有从美国移民到欧洲的人的Cypher查询 1 2 3 4 MATCH (person) -[:BORN_IN]-\u0026gt; () -[:WITHIN*0..]-\u0026gt; (us:Location {name:\u0026#39;United States\u0026#39;}), (person) -[:LIVES_IN]-\u0026gt; () -[:WITHIN*0..]-\u0026gt; (eu:Location {name:\u0026#39;Europe\u0026#39;}) RETURN person.name SQL中的图查询 用关系数据库表示图数据，那么也可以用SQL，但有些困难。 在关系数据库中，你通常会事先知道在查询中需要哪些连接。在图查询中，你可能需要在找到待查找的顶点之前，遍历可变数量的边。也就是说，连接的数量事先并不确定。 语法很复杂， 三元组存储和SPARQL 在三元组存储中，所有信息都以非常简单的三部分表示形式存储（主语，谓语，宾语）。例如，三元组 (吉姆, 喜欢 ,香蕉) 中，吉姆 是主语，喜欢 是谓语（动词），香蕉 是对象。 三元组的主语相当于图中的一个顶点。而宾语是下面两者之一： a. 原始数据类型中的值，例如字符串或数字。在这种情况下，三元组的谓语和宾语相当于主语顶点上的属性的键和值。例如，(lucy, age, 33)就像属性{“age”：33}的顶点lucy。 b. 图中的另一个顶点。在这种情况下，谓语是图中的一条边，主语是其尾部顶点，而宾语是其头部顶点。例如，在(lucy, marriedTo, alain)中主语和宾语lucy和alain都是顶点，并且谓语marriedTo是连接他们的边的标签。 当主语一样的时候，可以进行省略写法 1 2 3 4 5 @prefix : \u0026lt;urn:example:\u0026gt;. _:lucy a :Person; :name \u0026#34;Lucy\u0026#34;; :bornIn _:idaho. _:idaho a :Location; :name \u0026#34;Idaho\u0026#34;; :type \u0026#34;state\u0026#34;; :within _:usa _:usa a :Loaction; :name \u0026#34;United States\u0026#34;; :type \u0026#34;country\u0026#34;; :within _:namerica. _:namerica a :Location; :name \u0026#34;North America\u0026#34;; :type \u0026#34;continent\u0026#34;. 语义网络 语义网是一个简单且合理的想法：网站已经将信息发布为文字和图片供人类阅读，为什么不将信息作为机器可读的数据也发布给计算机呢？ 资源描述框架（RDF）的目的是作为不同网站以一致的格式发布数据的一种机制，允许来自不同网站的数据自动合并成一个数据网络 - 一种互联网范围内的“关于一切的数据库“。 现在已经凉了。 SPARQL查询语言 SPARQL是一种用于三元组存储的面向RDF数据模型的查询语言 查找从美国转移到欧洲的人 1 2 3 4 5 6 PREFIX : \u0026lt;urn:example:\u0026gt; SELECT ?personName WHERE { ?person :name ?personName. ?person :bornIn / :within* / :name \u0026#34;United States\u0026#34;. ?person :livesIn / :within* / :name \u0026#34;Europe\u0026#34;. } 基础：Datalog Datalog是比SPARQL或Cypher更古老的语言，在20世纪80年代被学者广泛研究 本章小结 在历史上，数据最开始被表示为一棵大树（层次数据模型），但是这不利于表示多对多的关系，所以发明了关系模型来解决这个问题。 最近，开发人员发现一些应用程序也不适合采用关系模型。新的非关系型“NoSQL”数据存储在两个主要方向上存在分歧：\n文档数据库的应用场景是：数据通常是自我包含的，而且文档之间的关系非常稀少。 图形数据库用于相反的场景：任意事物都可能与任何事物相关联。 文档数据库和图数据库有一个共同点，那就是它们通常不会为存储的数据强制一个模式，这可以使应用程序更容易适应不断变化的需求。但是应用程序很可能仍会假定数据具有一定的结构；这只是模式是明确的（写入时强制）还是隐含的（读取时处理）的问题。 每个数据模型都具有各自的查询语言或框架，我们讨论了几个例子：SQL，MapReduce，MongoDB的聚合管道，Cypher，SPARQL和Datalog。我们也谈到了CSS和XSL/XPath，它们不是数据库查询语言，而包含有趣的相似之处。 ","permalink":"https://csqread.top/posts/tech/ddia%E7%AC%AC%E4%BA%8C%E7%AB%A0/","summary":"第二章：数据模型与查询语言 数据模型可能是软件开发中最重要的部分了，因为它们的影响如此深远：不仅仅影响着软件的编写方式，而且影响着我们的解题思","title":"DDIA第二章"},{"content":"第一章：可靠性，可扩展性，可维护性 关于数据系统的思考 单个工具已经不能满足应用系统的需求，总体工作被拆分成一系列能被单个工具高效完成的任务，并通过应用代码将它们缝合起来。比如一个缓存、索引、数据库协作的例子：\n一个应用被称为数据密集型的，如果数据是其主要挑战（数据量，数据复杂度、数据变化速度）——与之相对的是计算密集型，即处理器速度是其瓶颈。 软件系统中很重要的三个问题：\n可靠性（Reliability）：系统在困境（硬件故障、软件故障、人为错误）中仍可正常工作 可扩展性（Scalability）：有合理的办法应对系统的增长（数据量、流量、复杂性） 可维护性（Maintainability）：许多不同的人在不同的生命周期，都能高效地在系统上工作。 可靠性 定义 造成错误的原因叫做故障（fault），能预料并应对故障的系统特性可称为容错（fault-tolerant）或者韧性（resilient）。讨论容错时，只有讨论特定类型的错误 故障（fault）不同于失效（failure）：故障指的是一部分状态偏离标准，而失效则是系统作为一个整体停止向用户提供服务。 通常倾向于容忍错误（而不是阻止错误），但也有预防胜于治疗的情况（比如安全问题） 硬件故障 一般都是增加单个硬件的冗余度 云平台的设计是优先考虑灵活性和弹性，而不是单机可靠性。 软件错误 这类软件故障的bug 通常潜伏很长时间，直到被异常情况触发为止。往往是某个假设出于某种原因最后不在成立了。 解决办法：仔细考虑假设和交互；彻底的测试；重启；监控。 人为错误 人是不可靠的，运维配置错误是导致服务中断的首要原因。 解决办法：最小化犯错机会的方式设计系统；容易犯错的地方解耦；测试；监控；培训。 可扩展性 定义 可扩展性（Scalability）是用来描述系统应对负载增长能力的术语。 描述负载 负载可以用负载参数的数字来描述，取决于系统架构\n推特的发推设计：\na. 推文放在全局推文集合中，查询的时候做 join\nb.推文插入到每个关注者的时间线中，「扇出」比较大，当有千万粉丝的大 V 发推压力大\nc.推特从方案一变成了方案二，然后变成了两者结合的方式\n描述性能 当描述好负载以后，问题变成了：\na. 增加负载参数并保持系统资源不变时，系统性能将受到什么影响？\nb. 增加负载参数并希望性能不变时，需要增加多少系统资源？ 批处理系统，通常关心吞吐量（throughput）；在线系统，通常更关心响应时间（response time） 对于系统响应时间而言，最好用百分位点，比如中位数、p99 等标识。 测量客户端的响应时间非常重要（而不是服务端），比如会出现头部阻塞、网络延迟等。 实践中的百分位点，可以用一个滑动的时间窗口（比如 10 分钟）进行统计。可以对列表进行排序，效率低的话，考虑一下前向衰减，t-digest 等方法近似计算。 应对负载的方法 纵向扩展：转向更强大的机器 横向扩展：将负载分布到多台小机器上 弹性系统：检测到负载增加时自动增加计算资源 跨多台机器部署无状态服务比较简单，但是把带状态的数据系统从单节点变成分布式配置则可能引入许多额外复杂度。因此，应该尽量将数据库放在单个节点上。 可维护性 在设计之初就尽量考虑尽可能减少维护期间的痛苦，从而避免自己的软件系统变成遗留系统。 三个设计原则：可操作性（Operability）便于运维团队保持系统平稳运行。简单性（Simplicity）从系统中消除尽可能多的复杂度（complexity），使新工程师也能轻松理解系统。（注意这和用户接口的简单性不一样。）可演化性（evolability）使工程师在未来能轻松地对系统进行更改，当需求变化时为新应用场景做适配。也称为可扩展性（extensibility），可修改性（modifiability）或可塑性（plasticity）。 可操作性：人生苦短，关爱运维 ● 尽量自动化\n简单性：管理复杂度 ● 消除额外的（accidental）的复杂度 ● 消除额外复杂度的最好工具之一是抽象（abstraction）\n可演化性：拥抱变化 ● 敏捷（agile） 工作模式为适应变化提供了一个框架\n● 简单易懂的系统通常比复杂系统更容易修改，即可演化性（evolvability）\n参考文章: https://www.yuque.com/fuxuemingzhu/ddia/hqu56x\n","permalink":"https://csqread.top/posts/tech/ddia%E7%AC%AC%E4%B8%80%E7%AB%A0/","summary":"第一章：可靠性，可扩展性，可维护性 关于数据系统的思考 单个工具已经不能满足应用系统的需求，总体工作被拆分成一系列能被单个工具高效完成的任务，并","title":"DDIA第一章"},{"content":"推荐序 这本书的适合所有后台开发工程师、大数据工程师，也很适合面试前复习系统设计的同学。\n什么是「数据密集型应用系统」？\n当数据（数据量、数据复杂度、数据变化速度）是一个应用的主要挑战，那么可以把这个应用称为数据密集型的。与之相对的是计算密集型——处理器速度是主要瓶颈。\n其实我们平时遇到的大部分系统都是数据密集型的——应用代码访问内存、硬盘、数据库、消息队列中的数据，经过业务逻辑处理，再返回给用户。\n很多应用都是在解决不同场景下的数据存储和检索问题——MySQL，Redis，HBase，Kafka，ElasticSearch…… 还有很多技术是围绕着数据展开——索引，编码（JSON, XML, Thrift, ProtoBuffer），行列存储…… 当数据在分布式处理时，要考虑——数据复制，分区，事务…… 大数据场景下，我们会使用——MapReduce，Spark，Flink 等批处理、流处理框架。 《数据密集型应用系统设计》这本书，把所有跟「数据」有关的知识点做了剖析、整理、总结，从一个很高的层次把各项技术的共性和区别讲得透彻。 当我们懂了底层原理之后，就明白了每项技术产生的背景是什么，解决了什么问题，有什么适用场景。\n这本书既有理论也有实践，基本没有公式，图很多，阅读起来很流畅，比较容易理解。\n这本书分为了三部分： ● 第一部分：数据系统的基石，包括数据模型与查询语言、存储与检索、数据编码与演化； ● 第二部分：分布式数据，包括复制、分片、事务、一致性与共识； ● 第三部分：衍生数据，包括批处理、流处理、数据系统的未来。\n阅读资源 这是一些阅读资源：\n《数据密集型应用系统设计》开源翻译仓库（9.3K star）： https://github.com/Vonng/ddia 开源版本在线阅读： https://vonng.gitbooks.io/ddia-cn/content/ 负雪明烛的读书笔记：数据密集型应用系统设计 《数据密集型应用系统设计》纸质书（翻译水平比开源在线阅读版好很多，强烈建议买书）：京东购买链接 辅助资料 ddia-references 这个仓库包含了《数据密集型应用系统设计》每章后面的所有参考文献对应的 pdf。 地址是：https://github.com/ept/ddia-references\nBook Review 这里有个很不错的 Book Review，是一个小哥讲了《DDIA》每一章的概述，作者很用心。 全英文的，在油管可以看到。地址是： https://www.youtube.com/watch?v=PdtlXdse7pw\u0026amp;list=PL4KdJM8LzAMecwInbBK5GJ3Anz-ts75RQ\n连载 后续我会把我读书笔记以及读书感想连载更新，这里是抄的一位大佬的序 https://www.yuque.com/fuxuemingzhu/ddia/kpqcs3\n","permalink":"https://csqread.top/posts/tech/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","summary":"推荐序 这本书的适合所有后台开发工程师、大数据工程师，也很适合面试前复习系统设计的同学。 什么是「数据密集型应用系统」？ 当数据（数据量、数据复杂","title":"数据密集型应用系统设计"},{"content":"分布式系统秒杀交易性能优化 1、背景 首先介绍一下这个交易的功能： 该交易是一个定期存款交易，利率非常高，存期有三个月，六个月、一年、两年、三年、五年，年化利率基本上每天发布的产品中都有4%以上的。其中六个月和一年的年化利率最受欢迎。这个定期产品只能每天9点开始购买，并且支持的渠道只有手机银行，因此在每天8点59-9点这个区间会收到非常的请求。根据指标控制中心查询的数据大概在9点到9点01秒这一秒中大概需要处理1000个请求。不过老核心那边怕给系统搞宕机做了一个并发控制，也就是达到性能瓶颈时会限制购买，打印“系统繁忙”反馈给前端.大概在不久前，老核心反馈给我们分布式核心说该交易也还需要持续进行优化，因为目前该产品实在是太火爆了，导致老核心也一直无法处理短时间内的大量请求，只能反馈系统繁忙。\n2、分析 我对老核心的这个交易逻辑进行了分析，其实他们已经做了不少优化了，不过目前看起来也还是很慢，从9点到9点1分我统计了这一分钟成功交易的平均耗时，大概是370ms左右。这个标准在老核心的交易中其实已经是比较慢的了。而分布式这边目前观察并行生产跑的性能更是非常慢，成功交易的平均耗时大概900ms左右。。。而这个交易又是我负责的产品，因此对它进行性能优化是当务之急。该交易的大致逻辑如下：\n前端请求进来，先做一些必输项校验，包括客户信息，密码校验等。接着是查询该产品的产品信息表，获取该产品信息，与客户信息做一些校验。后面会做一些额度的扣减。因为该款产品每次发布是有限额的。比如1000万美元的额度，抢完就没了，因此每次购买完了需要做一个额度的扣减。完了之后就需要记一笔账务。即从该客户的备用金账户扣除一笔钱到该客户的定期账户。最后再落一张表存储该客户购买的信息。\n其实老核心之前做的优化是把额度扣减从交易的前面移到了交易的最后。这样做的目的是，在并发请求打进来时，在做额度扣减的时候，会锁表，把这块逻辑放在最后做，能把锁表的时间减少的最短，如果放在前面做额度扣减，那么锁表的时间就是整个交易做完的时间，但是放在最后，就是最后几十ms。其他他们也没有做什么优化，\n但是分布式这边就很麻烦了。我这边也是把额度扣减放在最后的，按照他们的想法是把锁表的时间降低到最少。但是这样的话，因为我们是分布式，我得调两次额度服务，两次RPC所耗费得的时间也是很长的。而且分布式按照之前卡服务提供的接口，需要查询三次卡服务的表才能满足业务逻辑。所以我这边所做的优化大概如下：\n3、优化 首先，向卡组提需求，将三次RPC卡服务减少为RPC一次，一次查询，把所有需要的字段全部返回，减少两次RPC的时间 关于额度这块，我的想法是得把两次RPC减少为1次，而且也要减少锁表时间。。。一种是加缓存，把查询产品信息这块加缓存，这样就不用RPC额度了，但是这块有隐患，产品信息这张表是不断更新的，每天都有新的产品发布，按照redis缓存，是一些参数数据不太变化的数据加缓存，如果加缓存，生产上也得不断地去更新缓存。到时候查不到产品信息还是得去RPC额度服务。因此考虑了第二种方法。就是把更新额度和查询额度还是放到一次RPC中去做，并且放到交易得最前面。然后这块和整个交易不放在一个全局事务里面去做，分为两个事务去做。即更新额度与记账不在一起。这样的话。我们只需要try catch整个记账得逻辑，如果记账失败了，那我们就触发一个事件，去回滚前面那个事务做的额度扣减，把这个额度给加上。这样就不会造成长时间的锁表，也不会两次RPC。 最后，其实我对分布式每秒的高并发量是保持乐观态度的，因此在单元化之后已经将客户哈希到十个不同的服务上。因此就算1s钟500个并发进来那么也是随机平均的打到10个服务上面，而且进行容器化后，每个服务也是有很多机器来处理这些请求的，大家分担之后是完全可以处理这种秒杀交易的，虽然分布式这边的交易耗时没办法降到单核心交易耗时以下，但是系统处理能力也不是单核系统能比拟的。 4、总结 该交易涉及账务和额度增减，也就是说，如果交易失败，还需要进行事务回滚。在老核心单核系统处理起来是非常简单粗暴的。但是分布式由于各种RPC的原因，处理的手法就得麻烦点，不过也非常考验对分布式事务和消息队列的理解。其实整个系统达到1000tps在之前的性能测试中已经压到这个指标了，不过还没有涉及到这个秒杀的情况。虽然做了上面的这几个优化，但是肯定还是无法从900ms的平均耗时降下来很多。根据投产后的观察，大概能到500ms，和单核系统相比仍然差距很大。后续要需要持续的进行优化。\n这里涉及到的分布式事务知识点还是很重要的。而且我们在框架设计中使用的是乐观锁，也就是已经使得系统性能不会受到很大限制了。但是我觉得在这个秒杀交易这不太适合使用乐观锁。。。。。因为可能会导致很多请求冲突无功而返浪费时间\n大致的一个工作总结吧，其实这个产品我也买了。。，但是在查询的时候，明显的在9点的时候刷不到产品信息，提示系统繁忙，其实这个产品的查询也是需要优化的，因为大家在购买产品的时候刷新这个页面看产品信息的时候可能那一秒中的并发量就破万了，这可能就是单核的局限性吧，后面还分布式了应该不会出现这种尴尬的情况。不过仅仅是查询的化从后端的角度来说，优化其实也只是在缓存和SQL上面进行优化。我也是第一次在工作中实际遇到秒杀交易的优化，也让我更加深入的理解了一下这块所涉及的分布式系统设计和后端优化上的知识点。\n","permalink":"https://csqread.top/posts/tech/%E5%88%86%E5%B8%83%E5%BC%8F%E7%A7%92%E6%9D%80%E4%BA%A4%E6%98%93%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E8%AE%B0%E5%BD%95/","summary":"分布式系统秒杀交易性能优化 1、背景 首先介绍一下这个交易的功能： 该交易是一个定期存款交易，利率非常高，存期有三个月，六个月、一年、两年、三年、","title":"分布式秒杀交易性能优化记录"},{"content":"最近在看《重构》第二版，看到替换循环这里，想到我在工作中也是无脑的使用循环和if-else，最近也有空没事就重构之前自己写的代码以及别人留下来的代码。因此就自己尝试了一下管道。发现确实非常的简单明了。大致我举个例子说明下：\n大致需求是这样，从汽车List中找到2000年以后生产的汽车:\nCar类： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package test; public class Car { private String make; private String model; private String year; public Car(String make, String model, String year) { this.make = make; this.model = model; this.year = year; } public String getMake() { return make; } public void setMake(String make) { this.make = make; } public String getModel() { return model; } public void setModel(String model) { this.model = model; } public String getYear() { return year; } public void setYear(String year) { this.year = year; } } new 对象 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package test; import java.util.Arrays; import java.util.List; public class Iterating { public static List\u0026lt;Car\u0026gt; createCars(){ return Arrays.asList( new Car(\u0026#34;Jeep\u0026#34;, \u0026#34;Wrangler\u0026#34;, \u0026#34;2011\u0026#34;), new Car(\u0026#34;Jeep\u0026#34;, \u0026#34;Comanche\u0026#34;, \u0026#34;1990\u0026#34;), new Car(\u0026#34;Dodge\u0026#34;, \u0026#34;Avenget\u0026#34;, \u0026#34;2010\u0026#34;), new Car(\u0026#34;Buick\u0026#34;, \u0026#34;Cascada\u0026#34;, \u0026#34;2016\u0026#34;), new Car(\u0026#34;Ford\u0026#34;, \u0026#34;Focus\u0026#34;, \u0026#34;2012\u0026#34;), new Car(\u0026#34;Chevrolet\u0026#34;, \u0026#34;Geo Metro\u0026#34;, \u0026#34;1992\u0026#34;) ); } } 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 package test; import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; import java.util.List; import java.util.stream.Collectors; public class GetCarsModel { public static void main(String[] args) { List\u0026lt;Car\u0026gt; cars = Iterating.createCars(); System.out.println(getAfter2000years(cars)); System.out.println(getModelsAfter2000UsingPipeline(cars)); } /** * 使用for循环 * @param cars * @return */ private static List\u0026lt;String\u0026gt; getAfter2000years (List\u0026lt;Car\u0026gt; cars) { List\u0026lt;Car\u0026gt; carsDortedByYear = new ArrayList\u0026lt;\u0026gt;(); for (Car car : cars) { if (car.getYear().compareTo(\u0026#34;2000\u0026#34;) \u0026gt; 0) { carsDortedByYear.add(car); } } Collections.sort(carsDortedByYear, new Comparator\u0026lt;Car\u0026gt;() { @Override public int compare(Car o1, Car o2) { return (o1.getYear().compareTo(o2.getYear())); } }); List\u0026lt;String\u0026gt; models = new ArrayList\u0026lt;\u0026gt;(); for (Car car : carsDortedByYear) { models.add(car.getModel()); } return models; } /** * @deacription 使用管道 * @param cars * @return */ private static List\u0026lt;String\u0026gt; getModelsAfter2000UsingPipeline(List\u0026lt;Car\u0026gt; cars) { return cars.stream().filter(car -\u0026gt; car.getYear().compareTo(\u0026#34;2000\u0026#34;) \u0026gt; 0).sorted(Comparator.comparing(Car::getYear)).map(Car::getModel).collect(Collectors.toList()); } } 最后这俩结果是一样的，但是哪个更加简单明了一眼可知。\n只用了短短几行代码，代码的意图就很明显 — 给定一个汽车集合，过滤或提取仅在 2000 年或以后制造的汽车；然后按年份进行排序，将这些对象映射或转换为它们的型号，最后将结果收集到一个列表中。\n","permalink":"https://csqread.top/posts/tech/%E9%87%8D%E6%9E%84%E4%B9%8B%E7%AE%A1%E9%81%93%E6%9B%BF%E6%8D%A2%E5%BE%AA%E7%8E%AF/","summary":"最近在看《重构》第二版，看到替换循环这里，想到我在工作中也是无脑的使用循环和if-else，最近也有空没事就重构之前自己写的代码以及别人留下","title":"重构之管道替换循环"},{"content":"这几天一直在修并行环境的bug，也踩了不少坑， 记录一下\n1、关于对集合的排序循环问题。集合被修改之后不能再用于循环 原本的逻辑是这样的，在对C代码使用Java进行翻写的时候，因为C使用的是临时表，我这边用的是List集合，然后那边直接order By，而我用的是封装的一个对集合进行排序的方法。因此该集合在某种程度上是被修改了的，但是我仍然对让它进入了下一次循环，而没有break掉，或者使用一个新的集合来进行修改，导致并行生产环境报错。在不确定集合是否被修改的情况下，一定要New一个新的List来进行修改，保持原来的List进行循环。。。。\n2、spring boot redis 序列化报错 as a subtype of [simple type, class java.lang.Object]: no such class found 问题 大致场景是这样的， 有两个服务 A B, A服务用于授权， 授权成功会存储对象到redis中, B服务通过token去redis中拿到Object对象转换成业务对象。\n大致原因是： A服务存储对象到redis中时候会有一个全路径类名限定，在通过token进行取对象值并强制转换的时候，如果接受对象的全路径名与redis中保存的不一致的话就会转换失败报错。\n可能是因为之前项目类路径改造的时候，把这张表对应的类路径漏掉了，而redis那边不是实时从现有的数据库中获取的，而是根据我们各个业务组之前手动登记的路径进行修改的，因此并行生产出现这种问题。。。\n解决办法：\n1、把路径名称改成一致（使用了这个办法） 2、将保存对象的方式改成其他方式（这个存疑， 在网上看的）\n","permalink":"https://csqread.top/posts/tech/%E5%B7%A5%E4%BD%9C%E4%B8%8A%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91%E8%AE%B0%E5%BD%95/","summary":"这几天一直在修并行环境的bug，也踩了不少坑， 记录一下 1、关于对集合的排序循环问题。集合被修改之后不能再用于循环 原本的逻辑是这样的，在对C代","title":"工作上遇到的坑记录"},{"content":"进程的概念 在多道程序环境下，允许多个程序并发执行，此时他们将失去封闭性，并具有间断性和不可再现性的特征。为此引入了进程的概念，以便更好地描述和控制程序的并发执行，实现操作系统的并发行和共享性。为此引入了进程的概念，以便更好地描述和控制程序的并发执行，实现操作系统的并发性和共享性。\n为了是参与并发执行的程序能独立的运行，必须为之配置一个专门的数据结构，称之为进程控制块（process control block），系统利用PCB来描述进程的基本情况和运行状态，进而控制和管理进程。\n相应的，有程序段、相关数据段和PCB三部分构成了进程映像（进程实体）。所谓创建进程，实质上是创建进程映像中的PCB；而撤销进程，实质上是撤销进程的PCB。指的注意的是，进程影响是静态的，晋城市动态的。\n从不同的角度，进程可以有不同的定义，比较经典的定义有：\n1） 进程是程序的一次执行过程\n2） 进程是一个程序及其数据在处理器上顺序执行时所发生的活动。\n3） 进程是具有独立功能的程序在一个数据集合上运行的过程，他是系统进行资源分配和调度的一个独立单位。\n在引入了进程实体的概念后，我们可以吧传统的操作系统中的进程定义为：“进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位”。\n进程的特征 进程是由多程序的并发执行而引出的，他和程序是两个截然不同的概念。进程的基本特征是对比单个程序的顺序执行提出的，也是对进程管理提出的基本要求。\n1） 动态性：进程是程序的一次执行，他有着创建、活动、暂停、终止等过程，具有一定的生命周奇奇，是动态的产生、变化和消亡的。动态性是进程最基本的特征。\n2） 并发性：至多个进程实体，同存于内存中，能在一段时间内同时运行，并发性是进程的重要特征，同时也是操作系统的重要特征，引入进程的目的就是为了是程序能与去其他进程的程序并发执行，以提高资源利用率。\n3） 独立性：指进程实体是一个能独立运行、独立获得资源和独立接收调度的基本单位。范围建立PCB的程序都不能作为一个独立的单位参与运行。\n4） 异步性：由于进程的相互制约，是进程具有执行的间断性。也即进程按各自独立的、不可预知的速度向前推进。异步性会导致执行结果不可再现性，为此，在操作系统中必须配置相应的进程同步机制。\n5） 结构性：每个进程都配置一个PCB对其进行描述。从结构上来看，进程实体是由程序段、数据段和进程控制端三部分组成的。\n进程的状态与转换 进程在其生命周期内，由于系统中个进程之间的相互制约关系以及系统的运行环境的变化，使的进程的状态也在不断地发生着变化。通常进程有以下五种状态。前三种是进程的基本状态。\n1） 运行状态：进程正在处理器上运行。在单处理器的环境下，每一时刻最多只有一个进程处于运行状态。\n2） 就绪状态：进程已处于准备运行的状态，即进程获得了除CPU之外的一切所需资源，一旦得到处理器即可运行。\n3） 阻塞状态：又称为等待状态：进程正在等待某一事件而暂停运行，如等待某资源为可用（不包括处理器），或等待输入输出的完成。及时处理器空闲，该进程也不能运行。\n4） 创建状态：进程正在被创建，尚未转到就绪状态。创建进程通常需要多个步骤：首先申请一个空白的PCB，并向PCB中填写一些控制和管理进程的信息；然后由系统为该进程分配运行时所必须的资源；最后把该进程转入到就绪状态。\n5） 结束状态：进程正在从系统中消失，这可能是进程正常结束或其他原因中断退出运行。当进程需要结束运行时，系统首先必须置该进程为结束状态，然后再进一步处理资源释放和回收工作。\n注意区别就绪状态和等待状态：就绪状态是指进程仅缺少处理器，只要活得处理器资源就立即执行；而等待状态是指进程需要其他资源或等待某一事件，及时处理器空闲也不能运行。\n进程控制 进程控制的主要功能是对系统中所有进程实施有效地管理，她具有创建新进程、撤销已有进程、实现进程状态转换等功能。在操作系统中，一般把进程控制用的程序段成为原语，原语的特点是执行期间不允许中断，他是一个不可分割的基本单位。\n允许一个进程创建另一个进程。\n操作系统创建一个新进程的过程如下（创建原语）：\n1） 为新进程分配一个为我一个进程标示号，并申请一个空白的PCB。\n2） 为进程分配资源，为新进程的程序和数据，以及用户占分配必要的空间。\n3） 初始化PCB，主要包括初始化标识信息、初始化处理器状态信息和初始化处理器控制信息，以及设置进程的空闲及。\n4） 如果进程就绪队列能够接纳新进程，就将新进程插入到就绪队列，等待被调度运行。\n引起进程终止的时间主要有：正常结束、表示进程的任务已经完成和准备退出运行。异常结束是指进程在运行时，发生了某种异常事件，是程序无法继续运行，如：存储区越界、保护措、非法指令、特权指令错、IO故障等。外界干预是指进程外界的请求而终止，如操作员或操作系统干预、父进程请求和父进程终止。\n操作系统终止进程的过程如下：（撤消原语）\n1） 根据被终止进程的标示符，检索PCB，从中读出该进程的状态。\n2） 若被终止进程处于执行状态，立即终止该进程的执行，将处理器资源分配给其他进程。\n3） 若该进程还有子进程，则应将其所有子进程终止。\n4） 将该进程所拥有的资源、或归还给父进程或归还给操作系统。\n5） 将该PCB从所在队列（链表）中删除。\n进程的阻塞与唤醒 正在执行的进程，犹豫期待的某些时间为发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无心工作可做等，则由系统自动执行阻塞原语，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为。\n阻塞原语的执行过程为：找到将要被阻射进城的标识号对应的PCB，如果该进程为运行状态，则保护其现场，将其状态改为阻塞状态，停止运行，并把该PCB插入响应时间的等待队列中去；若为就绪状态，则将其状态改为阻塞状态，把它溢出就绪队列，插入到等待队列中去。\n当阻塞进程所期待的时间出现时，如它所启动的IO操作已完成或其所期待的数据已到达，则有关进程（比如，提供数据的进程），调用唤醒原语，将等待该事件的进程唤醒，唤醒原语的执行过程是：在该事件的等待队列中找到相应进程的PCB，然后把该PCB插入到就绪队列中，等待调度程序调度。\n需要注意的是，Block原语和Wakeup原语是一对作用刚好相反的原语，必须成对使用。Block原语是由被阻塞进程自我调用实现的，而Wakeup原语则是由一个与被唤醒进程相合作或被其他相关进程调用实现的。\n无论什么样的进程操作，都是在内核执行的。\n进程切换是指当前正在运行的进程被转换到其他状态后，再回到运行继续执行的过程，这个过程中，进程的运行环境产生了实质性的变化。进程切换的过程如下：\n1） 保存处理器上下文，包括程序计数器和其他寄存器。\n2） 更新PCB信息。\n3） 把进程的PCB移入相应的队列，如就绪、在某时间阻塞等队列。\n4） 选择另一个进程执行，并更新其PCB。更新内存管理的数据结构。\n5） 恢复处理器的上下文。\n进程控制块 进程创建时，操作系统就新建一个PCB结构，它之后就常驻内存，任意时刻可以存取。在进程结束时删除。PCB是进程实体的一部分，是进程存在的唯一标识。\nPCB主要包括：进程描述信息、进程控制和管理信息、资源分配清单和处理器相关信息等。\n在一个系统中，通常存在这许多进程，有的处于就绪状态，有的处于阻塞状态，而且阻塞的原因各不相同。为了方便进程的调度和管理，需要将各进程的PCB用适当的方法组织起来。目前，常用的组织方式有连接方式和索引方式两种。连接方式将同一状态的PCB连接成一个队列，不同状态对应不同的队列，也可以把处于阻塞状态的进程的PCB，根据其阻塞原因的不同，排成多个阻塞队列。索引方式是将同一状态的进程组织在一个索引表中，索引表的表项只想相应的PCB，不同状态对应不同的索引表，如就绪索引表和阻塞索引表等。\n程序段就是能北京城调度程度调度到CPU执行的程序代码段。注意，程序可以被多个进程共享，就是说多个进程可以运行同一个程序。\n一个进程的数据段，可以是进程对应的程序加工处理的原始数据，也可以是程序执行时产生的中间或最终结果。\n进程的通信 进程通信就是进程之间的数据交换。PV操作时低级通信方式2，高级通信方式是指以较高的效率传输大量数据的通信方式。高级通信方法可分为共享存储、消息传递和管道通信三大类。\n共享存储 在通信的进程之间存在着一款可以直接访问的共享空见，通过对这块共享空间的读写操作时间进程之间的信息交换。在共享存储方法中，需要使用同步互斥工具。\n需要注意的是：用户进程空间一般都是相互独立的，要想让两个用户进程共享空间，必须通过特殊系统调用实现，而进程内的线程是自然共享进程空间的。\n消息传递 在消息传递系统中，进程间的数据交换，是以格式化的小心Message为单位的。\n管道通信 管道通信是消息传递的一种特殊方式。。所谓管道，就是用于连接一个读进程和一个写进程以实现他们之间通信的一个共享文件，又名为pipe文件。向管道或共享文件提供输入的发送进程，以字符流的形势将大量的数据送入写管道；而接收管道输出的接收进城，则从管道中接受数据。为了协调双方的通信，关到极致必须他提供以下撒按方面的协调能力：互斥、同步和确定对方存在。\n线程概念和多线程模型 引入进程的目的，是为了是多道程序能并发执行，以提高资源利用率和系统吞吐量；而引入线程，则是为了减小程序在并发执行时所付出的时空开销，提高操作系统的并发性能。\n线程最直接的理解就是“轻量级进程”，它是一个基本的CPU执行单元，也是程序执行流的最小单元，由线程ID、程序计数器、寄存器集合和堆栈组成。线程是进程中的一个实体，是被系统独立调度和分派的基本单位。进程只作为除CPU以外的系统资源的分配单元，线程则作为处理器的分配单元。线程也有就绪、阻塞和运行三种基本状态。\n线程和进程的比较 1） 调度：在引入线程的操作系统中，线程是独立调度的基本单位，进程是资源拥有的基本单位。\n2） 拥有资源：进程是拥有资源的基本单位，而线程不拥有系统资源，单线程可以防伪其隶属进程的系统资源。\n3） 并发性：在引入线程的操作系统中，不仅进程之间可以并发执行，线程之间也可以并发执行，从而是操作系统具有更好的并发性，大大提高了系统的吞吐量。\n4） 系统开销：线程开销极小。\n5） 地址空间和其他资源：进程的地址空间之间相互独立，同一进程的各线程间共享进程的资源，进程内的线程对进程外的其他进程不可见。\n6） 通信方面：进程间通信需要进程同步和互斥手段的辅助，以保证数据的一致性，而线程间可以直接读写进程数据段来进行通信。\n线程的属性 在多线程操作系统中，八仙城作为独立运行的基本单位。此时的进程已不是一个基本可执行的实体。线程的主要属性如下：\n1） 线程是一个轻型实体，它不拥有系统资源，但每个线程都应有一个唯一的标识符和一个线程控制块，线程控制块记录了线程执行的寄存器和栈等现场情况。\n2） 不同的线程可以执行相同的程序，即同一个服务程序被不同的用户调用时，操作系统为他们创建不同的线程。\n3） 统一进程中的各个线程共享该进程所拥有的系统资源。\n4） 线程是处理器的独立调度单位，多个线程是可以并发执行的。\n5） 一个线程被创建后便开始了它的生命周期，直至终止，线程在生命周期内会经历等待态、就绪态和运行态等各种状态变化。\n线程的实现方法 线程的实现可以分为两类：用户级线程和内核级线程。\n多线程模型 有些系统同时支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程和内核级线程的连接方式。\n1） 多对一模型。多对一模型将多个用户级线程映射到一个内核级线程。线程管理在用户空间完成。\n2） 一对一模型。\n3） 多对多模型。\n特点：克服了多对一模型的并发度不高的缺点，又克服了一对一模型中一个用户进程占用太多内核级线程，开销太大的缺点。\n线程的调度 在多道程序系统中，进程的数量往往多于处理器的个数，进程争用处理器的情况在所难免。处理器调度是对处理器进行分配，就是从就绪队列中，按照一定的算法，选择一个进程并将处理器分配给他运行，以实现进程的并发执行。\n处理器调度是多道程序操作系统的基础，它是操作系统设计的核心问题。\n一个作业从提交开始知道完成，往往要经历一下三级调度：\n1）作业调度。作业调度又称高级调度：其主要任务是按一定的原则从外存上处于后备状态的作业中挑选一个或多个作业，给他们分配内存、输入输出设备等必要的资源。并建立相应的进程，以使他们获得竞争处理器的权利。\n多道批处理系统中大多配有作业调度，而其它系统中通常不需要配置作业调度。作业调度的执行频率较低，通常为几分钟一次。\n2）中级调度。中级调度又称内存调度。引入中级调度视为了提高内存利用率和系统吞吐率，为此，应使那些暂时不能运行的进程调至外存等待，把此时的进程状态称为挂起状态。当他们已具备运行条件且内存有稍有空闲时，由中级调度来决定，吧外存上那些已具备运行条件的就绪进程，在重新调入内存，并修改其状态为就绪状态，挂在就绪队列上等待。\n3）进程调度。进程调度又称为低级调度，其主要任务是按照某种方法和策略从就绪队列中选取一个进程，将处理器分配给它。进程调度是操作系统中最基本的一中调度，在一般操作系统中都不需配置进程调度。进程调度的频率很高，一般几十毫秒一次。\n作业调度从外存的后备队列中选择一批作业进入内存，为他们建立进程。这些进程被送入就绪队列。进程调度从就绪队列中选出一个进程，并把其状态改为运行状态，把CPU分配给它。中级调度是位于高级调度和低级调度之间的一种调度。为了提高内存的利用率，系统将那些暂时不能运行的进程挂起来。当内存空间宽松式，通过中级调度选择具备运行条件的进程，将其唤醒。\n调度的时机、切换与过程 进程调度和切换程序是操作系统内核程序。当请求调度的事件发生后，才可能会运行进程调度程序，当调度了新的就绪进程后，才会去进行进程间的切换。\n现在操作系统中，不能进行进程的调度与切换的情况有以下几种：\n1） 在处理中断的过程中：中断处理过程复杂，在实现上很难做到，而且中断处理时系统工作的一部分，逻辑上不属于某一进城，不应被剥夺处理器资源。\n2） 进程在操作系统内核程序临界区中：进入临界区后，需要独占式的访问共享数据，理论上必须加锁，以防止其他并行程序的进入，在解锁前不应该切换到其他进程，以加快该共享数据的释放。\n3） 其他需要完全屏蔽中断的原子操作过程中：如加锁、解锁、中断现场保护、恢复等等源自操作。在原子过程中，连中断都要屏蔽，更不应该进行进程的切换。\n如果在上述过程中发生了引起调度的条件，并不能马上进行调度和切换，应置系统请求调度标志，知道上述过程结束后才能进行相应的调度和切换。\n应该进行进程的调度与切换的情况有：\n1） 当发生引起调度条件且当前进程无法继续运行下去时，可以马上进行调度与切换。如果操作系统只在这种情况下进行进程调度，就是非剥夺调度。\n2） 当中断处理结束后或自陷处理结束后，返回被中断进程的用户态程序执行现场前，若置上请求调度标志，即可马上进行进程调度与切换。如果操作系统支持这种情况下的运行调度程序，就实现了剥夺方式的调度。\n进程切换往往在调度完成后立刻发生，它要求保存源进程当前切换点的县城信息，恢复被调度进程的现场信息。现场切换时，操作系统内核将远近程的现场信息推入到当前进程的内核对战来保存他们，并更新堆栈指针。内核完成从新进程的内核栈中装入新进程的县城信息、更新当前运行进程空间指针、重设PC寄存器等相关工作之后，开始运行新的进程。\n进程调度方式 所谓进程调度方式是指当某一个进程正在处理器上执行时，若有某个更为重要或紧迫的进程需要处理，既有优先权更高的进程进入就绪队列，此时应如何分配处理器。通常有一下两种进程调度方式：\n（1） 非剥夺调度方式 非剥夺调度方式又称为非抢占调度方式，是指当一个进程正在处理器上执行时，即使有某个更为重要或紧迫的进程进入就绪状态，仍然让正在执行的进程继续执行，知道该进程完成或发生某种时间而进入阻塞状态时，才把处理器分配给更为重要或紧迫的进程。\n（2） 剥夺调度方式 剥夺调度方式又称为抢占方式，是指当一个进程正在处理器上执行时，若有某个更为重要或紧迫的进程需要使用处理器，则立即暂停正在执行的进程，将处理器分配给这个更为重要或紧迫的进程。\n“剥夺”不是一种任意性行为，必须遵循一定的原则：优先权原则，短进程优先原则和时间片原则。\n调度的基本准则 不同的调度算法具有不同的特性，在选择调度算法时，必须考虑算法所具有的特性。为了比较处理器调度算法的性能，人们提出很多评价准则，下面介绍主要的几种准则：\n（1） CPU利用率 CPU是计算机系统中最重要的资源之一，所以应尽可能使CPU保持在忙状态，是这一资源利用率最高。\n（2） 系统吞吐量 系统吞吐量表示单位时间内CPU完成作业的数量。长作业需要消耗较长的处理器时间，因此会降低系统的吞吐量。而对于短作业，他们所需要消耗的处理器时间端，因此能提高系统的吞吐量。调度算法和方式的不同，也会对系统的吞吐量产生较大的影响。\n（3） 周转时间 周转时间是指从作业提交到作业完成所经历的时间，包括作业等待、在就绪队列中排队、在处理器上运行以及进行输入输出操作所花费的时间的总和。\n作业的周转时间=作业完成时间-作业提交时间\n（4） 等待时间 等待时间是指进程处于等处理器状态时间之和，等待时间越长，用户满意度越低。处理器调度算法实际上并不影响作业执行或输入输出操作时间，只影响作业在就绪队列中等待所花的时间。因此，衡量一个调度算法优劣常常只需简单地考察等待时间。\n（5） 响应时间 响应时间是指从用户提交请求到系统首次产生响应所有的时间。在交互式系统中，周转时间不可能是最好的评测准则，一般采用响应时间作为衡量调度算法的重要准则之一。从用户的角度来看，调度策略应尽量降低响应时间，使响应时间处在用户能够接受的范围之内。\n典型的调度算法 通常系统的设计目标不同，所采用的调度算法也不同。在操作系统中存在多种调度算法，其中有的调度算法适用于作业调度，有的调度算法适用于进程调度，有的调度算法两者都适用。下面介绍几种常用的调度算法：\n（1） FIFS先来先服务调度算法 特点：算法简单，但是效率低；有利于长作业，不利于短作业；有利于CPU繁忙型作业而不利于IO繁忙型作业。\n（2） SJF短作业优先调度算法 短作业（进程）优先调度算法是指对短作业祸端进程优先调度的算法。短作业优先调度算法是从后备队列中选择一个或若干个估计运算时间最短的作业，将他们呢掉入内存运行。\nSJF调度算法的缺点：\n1） 该算法对长作业不理。\n2） 该算法完全未考虑作业的紧迫程度\n3） 由于作业的长短只根据用户所提供的估计执行时间而定的，而用户又可能会有意或无意的缩短其作业的估计运行时间，致使该算法不一定能真正做到算作业优先调度。\n4） 注意：SJF调度算法的平均等待时间、平均周转时间最少。\n（3） 优先级调度算法 （4） 高响应比优先调度算法 高响应比优先调度算法主要用于作业调度。同时考虑从每个作业的等待时间和估计需要运行的时间。\n（5） 时间片轮转调度算法 时间片轮转调度算法主要适用于分时系统。\n（6） 多级反馈队列调度算法 多级反馈队列调度算法主要是时间片轮转调度算法和优先级调度算法的综合和发展。通过动态调整进程优先级和时间片大小，多级反馈队列调度算法可以兼顾多方面的系统目标。\n","permalink":"https://csqread.top/posts/tech/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/","summary":"进程的概念 在多道程序环境下，允许多个程序并发执行，此时他们将失去封闭性，并具有间断性和不可再现性的特征。为此引入了进程的概念，以便更好地描述","title":"进程与线程"},{"content":"有效数据生成以及插入数据库方案 先产生insert数据并存到备份文件中 因为有效数据生成的数量不大， 按照压测那边给我的需求大概每个交易4千笔数据左右，需要并发量比较高的交易也不过4万笔数据，涉及到转账的交易数据量比较高一点。并且需要做一个备份为了以后压测可以备用，因此我选择先生成到不同的表对应的表名文件中，然后再写一个批量执行SQL的程序执行这些文件中的insert语句\n下面的代码做了脱敏处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 import java.io.BufferedReader; import java.io.BufferedWriter; import java.io.FileReader; import java.io.FileWriter; import java.io.IOException; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.TimeUnit; import java.util.regex.Matcher; import java.util.regex.Pattern; public class MultiThreadScript { private static final int THREAD_POOL_SIZE = 4; // 线程池大小 private static final String INPUT_FILE_PATH = \u0026#34;input.sql\u0026#34;; // 输入文件路径 private static final String OUTPUT_FILE_PATH = \u0026#34;output.sql\u0026#34;; // 输出文件路径 private static final String INSERT_REGEX = \u0026#34;(?i)^insert into .* values\\\\s*\\\\((.*)\\\\);?$\u0026#34;; // insert语句的正则表达式 private static final String PK_REGEX = \u0026#34;\u0026#39;[0-9A-Za-z]+\u0026#39;\u0026#34;; // 主键的正则表达式 private static final int PK_INDEX = 0; // 主键在值列表中的索引 public static void main(String[] args) throws Exception { // 创建线程池 ExecutorService executor = Executors.newFixedThreadPool(THREAD_POOL_SIZE); try (BufferedReader reader = new BufferedReader(new FileReader(INPUT_FILE_PATH))) { String line; while ((line = reader.readLine()) != null) { if (isInsertStatement(line)) { executor.execute(new InsertTask(line)); } } } // 关闭线程池并等待所有任务完成 executor.shutdown(); executor.awaitTermination(1, TimeUnit.HOURS); } // 判断一行文本是否为insert语句 private static boolean isInsertStatement(String line) { return line.matches(INSERT_REGEX); } // 插入任务 private static class InsertTask implements Runnable { private final String originalSql; public InsertTask(String originalSql) { this.originalSql = originalSql; } @Override public void run() { try { // 提取主键 Pattern pkPattern = Pattern.compile(PK_REGEX); Matcher pkMatcher = pkPattern.matcher(originalSql); pkMatcher.find(); String originalPk = pkMatcher.group(); // 提取值列表 String valueList = originalSql.replaceAll(INSERT_REGEX, \u0026#34;$1\u0026#34;); String[] values = valueList.split(\u0026#34;,\u0026#34;); // 递增主键并生成新的SQL语句 StringBuilder newSqlBuilder = new StringBuilder(); for (int i = 0; i \u0026lt; 40000; i++) { String newPk = getNextPk(originalPk); String newValueList = valueList.replace(originalPk, newPk); String newSql = originalSql.replaceAll(valueList, newValueList); newSqlBuilder.append(newSql).append(\u0026#34;\\n\u0026#34;); } // 写入输出文件 synchronized (MultiThreadScript.class) { try (BufferedWriter writer = new BufferedWriter(new FileWriter(OUTPUT_FILE_PATH, true))) { writer.write(newSqlBuilder.toString()); } } } catch (IOException e) { e.printStackTrace(); } } // 获取下一个主键 private String getNextPk (String originalPk) { String prefix = originalPk.substring(0, originalPk.length() - 1); String suffix = originalPk.substring(originalPk.length() - 1); String newSuffix = getNextSuffix(suffix); return prefix + newSuffix; } // 获取下一个主键后缀 private String getNextSuffix(String suffix) { StringBuilder sb = new StringBuilder(); for (int i = 0; i \u0026lt; suffix.length(); i++) { char c = suffix.charAt(i); if (Character.isDigit(c)) { int digit = Character.getNumericValue(c); if (digit == 9) { sb.append(\u0026#39;A\u0026#39;); } else if (digit == 35) { sb.append(\u0026#39;a\u0026#39;); } else { sb.append(Character.forDigit(digit + 1, 36)); } } else if (Character.isLetter(c)) { if (c == \u0026#39;Z\u0026#39;) { sb.append(\u0026#39;0\u0026#39;); } else if (c == \u0026#39;z\u0026#39;) { sb.append(\u0026#39;0\u0026#39;); } else { sb.append((char) (c + 1)); } } else { sb.append(c); } } return sb.toString(); } } 上面的代码中，MultiThreadScript类是脚本的主类，它负责读取输入文件并创建线程池来处理每条insert语句。InsertTask类是插入任务类，它实现了Runnable接口，用于递增主键并生成新的SQL语句。为了避免多个线程同时写入输出文件，InsertTask类中使用了synchronized关键字来进行同步。\n在getNextPk()方法中，我使用了类似于Excel中列名的递增方式来递增主键。首先，将原始主键分为前缀和后缀两部分，其中前缀是主键的前面部分，后缀是主键的最后一位字符。然后，对后缀进行递增，并根据递增后的后缀重新生成新的主键。\n最后，需要注意的是，由于主键可能包含字母和数字，因此使用36进制来对主键进行递增。例如，对于主键值为\u0026quot;001\u0026quot;，它的下一个值为\u0026quot;002\u0026quot;；对于主键值为\u0026quot;AZ9\u0026quot;，它的下一个值为\u0026quot;BA0\u0026quot;。\n进行插入操作（脱敏处理后的代码）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 import java.io.BufferedReader; import java.io.FileReader; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; import java.sql.SQLException; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class InsertExecutor { private static final String URL = \u0026#34;jdbc:mysql://localhost:3306/mydatabase\u0026#34;; private static final String USER = \u0026#34;myuser\u0026#34;; private static final String PASSWORD = \u0026#34;mypassword\u0026#34;; private static final int THREAD_POOL_SIZE = 10; public static void main(String[] args) { try { // 读取insert语句文件 BufferedReader reader = new BufferedReader(new FileReader(\u0026#34;inserts.sql\u0026#34;)); String line; Queue\u0026lt;String\u0026gt; inserts = new LinkedList\u0026lt;\u0026gt;(); while ((line = reader.readLine()) != null) { inserts.add(line); } reader.close(); // 创建线程池 ExecutorService executorService = Executors.newFixedThreadPool(THREAD_POOL_SIZE); // 执行insert语句 while (!inserts.isEmpty()) { String insert = inserts.poll(); executorService.execute(new InsertWorker(insert)); } // 关闭线程池 executorService.shutdown(); } catch (IOException e) { e.printStackTrace(); } } static class InsertWorker implements Runnable { private String insert; public InsertWorker(String insert) { this.insert = insert; } @Override public void run() { try (Connection conn = DriverManager.getConnection(URL, USER, PASSWORD); PreparedStatement statement = conn.prepareStatement(insert)) { // 执行insert语句 statement.executeUpdate(); } catch (SQLException e) { // 主键冲突，跳过该语句 if (e.getErrorCode() == 1062) { System.out.println(\u0026#34;Skip duplicate insert: \u0026#34; + insert); } else { e.printStackTrace(); } } } } } 上面代码中我们把insert.sql取代为我们想要进行批量insert的sql文件即可，线程数量可根据CPU的情况来看，在不进行其他工作任务的情况下，可尽量压榨CPU的使用率以达到最高的效率。\n亿级别的无效数据生成并插入数据库方案 这里因为涉及到的数据量特别大， 一般是模拟生产环境，因此一张表可能有千万级别以及亿级别的数据量，因此我选择一边生成一边做insert操作。也就是一个生产者一个消费者，当然，这里都是多线程来操作的。一开始我是每次达到20个事务一次提交的，后来换了OceanBase后，只能一次提交一个事务了，效率也变满了一点点. 对于多线程插入数据库，将生成的SQL语句分配给多个线程，每个线程使用单独的数据库连接插入数据库，可以使用线程池来管理多个线程\n下面是脱敏后的代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 import java.io.BufferedReader; import java.io.FileReader; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; import java.sql.SQLException; import java.util.concurrent.BlockingQueue; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.LinkedBlockingQueue; import java.util.concurrent.TimeUnit; public class MultiThreadedSqlInsert { // 数据库连接信息 private static final String DB_URL = \u0026#34;jdbc:mysql://localhost:3306/mydb\u0026#34;; private static final String DB_USER = \u0026#34;root\u0026#34;; private static final String DB_PASSWORD = \u0026#34;mypassword\u0026#34;; // 主键列名和初始值 private static final String PK_COLUMN_NAME = \u0026#34;id\u0026#34;; private static final String PK_INITIAL_VALUE = \u0026#34;1000\u0026#34;; // 线程数和每个线程处理的主键值个数 private static final int THREAD_COUNT = 10; private static final int KEYS_PER_THREAD = 10000000; // 文件名和队列大小 private static final String FILE_NAME = \u0026#34;data.sql\u0026#34;; private static final int QUEUE_SIZE = 10000; public static void main(String[] args) throws Exception { // 读取文件中的 SQL 语句 String sql = readSqlFromFile(FILE_NAME); // 创建线程池和队列 ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT); BlockingQueue\u0026lt;String\u0026gt; queue = new LinkedBlockingQueue\u0026lt;\u0026gt;(QUEUE_SIZE); // 创建多个线程，为每个线程分配一段主键值的区间 for (int i = 0; i \u0026lt; THREAD_COUNT; i++) { int start = i * KEYS_PER_THREAD; int end = (i + 1) * KEYS_PER_THREAD - 1; executor.submit(new SqlGenerator(sql, start, end, queue)); } // 创建多个数据库连接，为每个连接分配一个线程 for (int i = 0; i \u0026lt; THREAD_COUNT; i++) { Connection conn = DriverManager.getConnection(DB_URL, DB_USER, DB_PASSWORD); executor.submit(new SqlExecutor(conn, queue)); } // 等待所有线程执行完毕 executor.shutdown(); executor.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS); } // 从文件中读取 SQL 语句 private static String readSqlFromFile(String fileName) throws Exception { try (BufferedReader reader = new BufferedReader(new FileReader(fileName))) { StringBuilder sb = new StringBuilder(); String line; while ((line = reader.readLine()) != null) { sb.append(line).append(\u0026#34;\\n\u0026#34;); } return sb.toString(); } } // 生成新的 SQL 语句 private static String generateSql(String sql, int key) { String pkValue = PK_INITIAL_VALUE + key; return sql.replaceFirst(PK_COLUMN_NAME, pkValue); } // 生成新的 SQL 语句的线程 private static class SqlGenerator implements Runnable { private final String sql; private final int start; private final int end; private final BlockingQueue\u0026lt;String\u0026gt; queue; public SqlGenerator(String sql, int start, int end, BlockingQueue\u0026lt;String\u0026gt; queue) { this.sql = sql; this.start = start; this.end = end; this.queue = queue; } @Override public void run() { for (int i = start; i \u0026lt;= end; i++) { String newSql = generateSql(sql, i); try { queue.put(newSql); } catch (InterruptedException e) { Thread.currentThread().interrupt(); return; } } } } // 执行 SQL 语句的线程 private static class SqlExecutor implements Runnable { private final Connection conn; private final BlockingQueue\u0026lt;String\u0026gt; queue; public SqlExecutor(Connection conn, BlockingQueue\u0026lt;String\u0026gt; queue) { this.conn = conn; this.queue = queue; } @Override public void run() { try (PreparedStatement stmt = conn.prepareStatement(\u0026#34;\u0026#34;)) { while (true) { String sql = queue.take(); if (sql == null) { break; } stmt.addBatch(sql); if (stmt.getBatchSize() \u0026gt;= 1000) { stmt.executeBatch(); } } stmt.executeBatch(); } catch (SQLException | InterruptedException e) { e.printStackTrace(); } } } 这里使用了两个线程池，一个用于生成新的 SQL 语句，一个用于执行 SQL 语句。生成 SQL 语句的线程将生成的 SQL 语句存储到一个线程安全的队列中，执行 SQL 语句的线程从队列中取出 SQL 语句并执行插入操作。程序使用了 JDBC 连接 MySQL 数据库，并使用了 PreparedStatement 批量执行 SQL 语句，以提高插入效率。\n需要注意的是，为了避免多个线程同时操作数据库导致数据不一致的问题，每个线程使用了自己的数据库连接。此外，程序还使用了线程安全的队列和加锁机制来保证线程安全。\n总结 以上就是我在工作中遇到的问题之一，做一个小结，用到了很多线程和线程池的地方，以及操作数据库相关的知识。\n","permalink":"https://csqread.top/posts/tech/%E5%8E%8B%E6%B5%8B%E6%95%B0%E6%8D%AE%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90%E5%B9%B6%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%80%BB%E7%BB%93/","summary":"有效数据生成以及插入数据库方案 先产生insert数据并存到备份文件中 因为有效数据生成的数量不大， 按照压测那边给我的需求大概每个交易4千笔数据","title":"压测数据快速生成并插入数据库总结"},{"content":"JVM 内存结构 Java 虚拟机的内存空间分为 5 个部分：\n程序计数器 Java虚拟机栈 本地方法栈 堆 方法区 JDK 1.8 同 JDK 1.7 比，最大的差别就是：元数据区取代了永久代。元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元数据空间并不在虚拟机中，而是使用本地内存。\n程序计数器(PC寄存器) 程序计数器的定义 程序计数器是一块较小的内存空间，是当前线程正在执行的那条字节码指令的地址。若当前线程正在执行的是一个本地方法，那么此时程序计数器为Undefined。在Java虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制的指示器，分支，循环，跳转、异常处理、线程恢复等基础功能都需要这个计数器来完成。\n程序计数器的作用 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制。 在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次线程执行到哪了 ","permalink":"https://csqread.top/posts/tech/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/","summary":"JVM 内存结构 Java 虚拟机的内存空间分为 5 个部分： 程序计数器 Java虚拟机栈 本地方法栈 堆 方法区 JDK 1.8 同 JDK 1.7 比，最大的差别就是：元数据区取代了永久代。元","title":"Java虚拟机知识总结"},{"content":"什么是Netty 1、Netty 是一个 基于 NIO 的 client-server(客户端服务器)框架，使用它可以快速简单地开发网络应用程序。 2、它极大地简化并优化了 TCP 和 UDP 套接字服务器等网络编程,并且性能以及安全性等很多方面甚至都要更好。 3、支持多种协议 如 FTP，SMTP，HTTP 以及各种二进制和基于文本的传统协议。 用官方的总结就是：Netty 成功地找到了一种在不妥协可维护性和性能的情况下实现易于开发，性能，稳定性和灵活性的方法。\n除了上面之外，很多开源项目比如我们常用的 Dubbo、RocketMQ、Elasticsearch、gRPC 等等都用到了 Netty\n相比于直接使用 JDK 自带的 NIO 相关的 API 来说更加易用。 统一的 API，支持多种传输类型，阻塞和非阻塞的。 简单而强大的线程模型。 自带编解码器解决 TCP 粘包/拆包问题。 自带各种协议栈。 真正的无连接数据包套接字支持。 比直接使用 Java 核心 API 有更高的吞吐量、更低的延迟、更低的资源消耗和更少的内存复制。 安全性不错，有完整的 SSL/TLS 以及 StartTLS 支持。 社区活跃、成熟稳定，经历了大型项目的使用和考验，而且很多开源项目都使用到了 Netty， 比如我们经常接触的 Dubbo、RocketMQ 等等。 应用场景 NIO 可以做的事情 ，使用 Netty 都可以做并且更好。Netty 主要用来做网络通信 : 作为 RPC 框架的网络通信工具 ：我们在分布式系统中，不同服务节点之间经常需要相互调用，这个时候就需要 RPC 框架了。不同服务节点之间的通信是如何做的呢？可以使用 Netty 来做。比如我调用另外一个节点的方法的话，至少是要让对方知道我调用的是哪个类中的哪个方法以及相关参数吧！ 实现一个自己的 HTTP 服务器 ：通过 Netty 我们可以自己实现一个简单的 HTTP 服务器，这个大家应该不陌生。说到 HTTP 服务器的话，作为 Java 后端开发，我们一般使用 Tomcat 比较多。一个最基本的 HTTP 服务器可要以处理常见的 HTTP Method 的请求，比如 POST 请求、GET 请求等等。 实现一个即时通讯系统 ：使用 Netty 我们可以实现一个可以聊天类似微信的即时通讯系统， 实现消息推送系统 ：市面上有很多消息推送系统都是基于 Netty 来做的。 Netty 的高性能表现 心跳，对服务端：会定时清除闲置会话 inactive(netty5)，对客户端:用来检测会话是否断开，是否重来，检测网络延迟，其中 idleStateHandler 类 用来检测会话状态 串行无锁化设计，即消息的处理尽可能在同一个线程内完成，期间不进行线程切换，这样就避免了多线程竞争和同步锁。表面上看，串行化设计似乎 CPU 利用率不高，并发程度不够。但是，通过调整 NIO 线程池的线程参数，可以同时启动多个串行化的线程并行运行，这种局部无锁化的串行线程设计相比一个队列-多个工作线程模型性能更优。 可靠性，链路有效性检测：链路空闲检测机制，读/写空闲超时机制；内存保护机制：通过内存池重用 ByteBuf;ByteBuf 的解码保护；优雅停机：不再接收新消息、退出前的预处理操作、资源的释放操作。 Netty 安全性：支持的安全协议：SSL V2 和 V3，TLS，SSL 单向认证、双向认证和第三方 CA认证。 高效并发编程的体现：volatile 的大量、正确使用；CAS 和原子类的广泛使用；线程安全容器的使用；通过读写锁提升并发性能。IO 通信性能三原则：传输（AIO）、协议（Http）、线程（主从多线程） 流量整型的作用（变压器）：防止由于上下游网元性能不均衡导致下游网元被压垮，业务流中断；防止由于通信模块接受消息过快，后端业务线程处理不及时导致撑死问题 Netty核心组件 Bootstrap和ServerBootstrap 当需要连接客户端或者服务器绑定指定端口是需要使用Bootstrap，ServerBootstrap有两种类型，一种是用于客户端的Bootstrap，一种是用于服务端 的ServerBootstrap。不管程序使用哪种协议，无论是创建一个客户端还是服务器都需要使 用“引导”。\nBootstrap 是客户端的启动引导类/辅助类\n1 2 3 4 5 6 7 8 9 10 11 12 13 EventLoopGroup group = new NioEventLoopGroup(); try { //创建客户端启动引导/辅助类： Bootstrap Bootstrap b = new Bootstrap(); //指定线程模型 b.group(group). ...... // 尝试建立连接 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); } finally { // 优雅关闭相关线程组资源 group.shutdownGracefully(); } ServerBootstrap 客户端的启动引导类/辅助类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 1.bossGroup 用于接收连接，workerGroup 用于具体的处理 EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //2.创建服务端启动引导/辅助类： ServerBootstrap ServerBootstrap b = new ServerBootstrap(); //3.给引导类配置两大线程组,确定了线程模型 b.group(bossGroup, workerGroup). ...... // 6.绑定端口 ChannelFuture f = b.bind(port).sync(); // 等待连接关闭 f.channel().closeFuture().sync(); } finally { //7.优雅关闭相关线程组资源 bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } Bootstrap 通常使用 connet() 方法连接到远程的主机和端口，作为一个 Netty TCP 协议通信中的客户端。另外，Bootstrap 也可以通过 bind() 方法绑定本地的一个端口，作为 UDP 协议通信中的一端。 ServerBootstrap通常使用 bind() 方法绑定本地的端口上，然后等待客户端的连接。\nBootstrap 只需要配置一个线程组— EventLoopGroup，而 ServerBootstrap需要配置两个线程组— EventLoopGroup ，一个用于接收连接，一个用于具体的处理。\n一个 ServerBootstrap 可以认为有2个 Channel 集合，\n第一个集合包含一个单例 ServerChannel，代表持有一个绑定了本地端口的 socket;\n第二集合包含所有创建的 Channel，处理服务器所接收到的客户端进来的连接。\nEventLoop和EventLoopGroup EventLoop 定义了 Netty 的核心抽象，用于处理连接的生命周期中所发生的事件。\nEventLoop 的主要作用实际就是负责监听网络事件并调用事件处理器进行相关 I/O 操作的处理。\nChannel 和 EventLoop 直接有啥联系呢？\nChannel 为 Netty 网络操作(读写等操作)抽象类，EventLoop 负责处理注册到其上的Channel 处理 I/O 操作，两者配合参与 I/O 操作。\nEventLoopGroup包含多个EventLoop，每个EventLoop通常内部包含一个线程。EventLoop在处理IO事件时在自己的Thread线程上进行，从而保证线程安全\nNioEventLoopGroup在未指定线程数时，默认时当前cpu线程数*2\nEventLoopGroup 是一组 EventLoop 的抽象，Netty 为了更好的利用多核 CPU 资源，一般会有多个 EventLoop 同时工作，每个 EventLoop 维护着一个 Selector 实例。 EventLoopGroup 提供 next 接口，可以从组里面按照一定规则获取其中一个EventLoop来处理任务。在 Netty 服务器端编程中，我们一般都需要提供两个EventLoopGroup，例如:BossEventLoopGroup 和 WorkerEventLoopGroup。 通常一个服务端口即一个ServerSocketChannel对应一个Selector和一个EventLoop 线程。BossEventLoop 负责接收客户端的连接并将 SocketChannel 交给 WorkerEventLoopGroup 来进行 IO 处理\nBossEventLoopGroup 通常是一个单线程的 EventLoop，EventLoop 维护着一个注册了ServerSocketChannel 的Selector 实例BossEventLoop 不断轮询Selector 将连接事件分离出来 通常是 OP_ACCEPT 事件，然后将接收到的 SocketChannel 交给WorkerEventLoopGroup WorkerEventLoopGroup 会由 next 选择其中一个 EventLoop来将这个SocketChannel 注册到其维护的Selector 并对其后续的 IO 事件进行处理 EventLoop继承图\nChannel通道 Channel 接口是 Netty 对网络操作抽象类，它除了包括基本的 I/O 操作，如 bind()、connect()、read()、write() 等。\n比较常用的Channel接口实现类是NioServerSocketChannel（服务端）和NioSocketChannel（客户端），这两个 Channel 可以和 BIO 编程模型中的ServerSocket以及Socket两个概念对应上。Netty 的 Channel 接口所提供的 API，大大地降低了直接使用 Socket 类的复杂性。\n1 2 3 4 5 6 7 Channel channel = ...; // 获取channel的引用 ByteBuf buf = Unpooled.copiedBuffer(\u0026#34;your data\u0026#34;, CharsetUtil.UTF_8); //1 ChannelFuture cf = channel.writeAndFlush(buf); //2 cf.addListener(new ChannelFutureListener() { //3 @Override public void operationComplete(ChannelFuture future) { if (future.isSuccess()) { //4 } }); 创建 ByteBuf 保存写的数据 写数据，并刷新 添加 ChannelFutureListener 即可写操作完成后收到通知 写操作没有错误完成 写操作完成时出现错误 channel声明周期 | 状态 | 描述 | | —- | —- | | ChannelUnregistered | Channel 已经被创建，但还未注册到EventLoop | | ChannelRegistered | Channel 已经被注册到了EventLoop | | ChannelActive | Channel 处于活动状态（已经连接到它的远程节点）。它现在可以接收和发送数据了 | | ChannelInactive | Channel 没有连接到远程节点 |\nselector 作用：\nI/O 的就绪与选择 是 NIO 网络编程的基础 SelectonKey 状态 OP_ACCEPT 操作集位用于插座接受操作。 OP_CONNECT 用于套接字连接操作的操作集位。 OP_READ 读操作的操作位。 OP_WRITE 写操作的操作位。 1 2 3 4 5 Selector selector = new Selector.open() SelectorKey selectorKey = channel.register(selector, SelectionKey.OP_READ); int selectNum = selector.select(); Set\u0026lt;Selection\u0026gt; selectionkeys = selector.selectdkeys(); ","permalink":"https://csqread.top/posts/tech/netty%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/","summary":"什么是Netty 1、Netty 是一个 基于 NIO 的 client-server(客户端服务器)框架，使用它可以快速简单地开发网络应用程序。 2、它极大","title":"Netty相关总结"},{"content":"面试题目 记录下社招面试拼多多的总结与心得，以及失败的原因吧。\n1、服务注册是如何发现的，eureka的基本原理， 容器ip是动态的还是静态的\n答：eureka, 基本原理大概说个一些：包括服务注册，服务发现，心跳机制，服务下线， 自我保护机制等等。\n2、转账，支付如何保证数据一致性的， 说一下分布式事务的实现， 消息的生产和消费机制。\n这个基本上说出个一二三来。其实就是分布式事务，保证这个数据的一致性就是要保证事务的原子性。即，事务要么全部成功，要么全部失败。我就提了下XA协议和TCC模式，具体如何实现的我也不太清楚。\n3、mysql 索引优化，子查询优化\n这里基本上都讲出来了。之前做过很多压测，包括让sql走上索引，参数表添加缓存等等。\n4、线上有排查过什么问题.\n5、MQ的实现原理\n5、图算法题\n还有些问题已经忘了。\n总结与回顾 其实关于这次面试，我还是没有做好完全的准备，而且是近三年以来的第一次面试，心里难免还是有点紧张。导致我有些东西知道的知识可能一时半会想不起来。后面把这些问到的知识点再复习一下。基本上只是浅浅的了解了一下，细说一下底层原理我就懵了。大概知道我们有这么个流程，知道哪里出了问题该找谁来看。因为现在吧，大公司基本上就是这么个情况， 包括中间件团队，数据库团队，DTF团队，DCF团队等等。基本上我们只用知道这些东西有，然后找相应团队的负责人帮忙看下问题就能解决。我们都是在脚手架上做着CURD。\n但是还是要把面试问到的东西基本原理做一个小小的总结和记录:\nEureka Eureka是Netflix开源的一款提供服务注册和发现的产品， 开源地址为 Eureka, 注册中心是分布式开发的核心组件之一\n而eureka是spring cloud推荐的注册中心实现, Eureka是一个REST (Representational State Transfer)服务 它主要用于AWS云，用于定位服务，以实现中间层服务器的负载平衡和故障转移，我们称此服务为Eureka服务器\nEureka也有一个基于java的客户端组件，Eureka客户端，这使得与服务的交互更加容易，同时客户端也有一个内置的负载平衡器，它执行基本的循环负载均衡。\n自我保护机制 自我保护机制主要在Eureka Client和Eureka Server之间存在网络分区的情况下发挥保护作用，在服务器端和客户端都有对应实现.\n假设在某种特定的情况下（如网络故障）, Eureka Client和Eureka Server无法进行通信，此时Eureka Client无法向Eureka Server发起注册和续约请求，Eureka Server中就可能因注册表中的服务实例租约出现大量过期而面临被剔除的危险，然而此时的Eureka Client可能是处于健康状态的（可接受服务访问），如果直接将注册表中大量过期的服务实例租约剔除显然是不合理的，自我保护机制提高了eureka的服务可用性。\n当自我保护机制触发时，Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务，仍能查询服务信息并且接受新服务注册请求，也就是其他功能是正常的。\n这里思考下，如果eureka节点A触发自我保护机制过程中，有新服务注册了然后网络回复后，其他peer节点能收到A节点的新服务信息，数据同步到peer过程中是有网络异常重试的，也就是说，是能保证最终一致性的。\n服务发现原理 eureka server可以集群部署，多个节点之间会进行（异步方式）数据同步，保证数据最终一致性，Eureka Server作为一个开箱即用的服务注册中心，提供的功能包括：服务注册、接收服务心跳、服务剔除、服务下线等。\n需要注意的是，Eureka Server同时也是一个Eureka Client，在不禁止Eureka Server的客户端行为时，它会向它配置文件中的其他Eureka Server进行拉取注册表、服务注册和发送心跳等操作。\neureka server端通过appName和instanceInfoId来唯一区分一个服务实例，服务实例信息是保存在哪里呢？其实就是一个Map中：\n1 2 // 第一层的key是appName，第二层的key是instanceInfoIdprivate final ConcurrentHashMap\u0026lt;String, Map\u0026lt;String, Lease\u0026lt;InstanceInfo\u0026gt;\u0026gt;\u0026gt; registry = new ConcurrentHashMap\u0026lt;String, Map\u0026lt;String, Lease\u0026lt;InstanceInfo\u0026gt;\u0026gt;\u0026gt;(); 服务注册 Service Provider启动时会将服务信息（InstanceInfo）发送给eureka server，eureka server接收到之后会写入registry中，服务注册默认过期时间DEFAULT_DURATION_IN_SECS = 90秒。InstanceInfo写入到本地registry之后，然后同步给其他peer节点，对应方法com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#replicateToPeers。\n写入本地redistry 服务信息（InstanceInfo）保存在Lease中，写入本地registry对应方法com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#register，Lease统一保存在内存的ConcurrentHashMap中，在服务注册过程中，首先加个读锁，然后从registry中判断该Lease是否已存在，如果已存在则比较lastDirtyTimestamp时间戳，取二者最大的服务信息，避免发生数据覆盖。使用InstanceInfo创建一个新的InstanceInfo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 if (existingLastDirtyTimestamp \u0026gt; registrationLastDirtyTimestamp) { // 已存在Lease则比较时间戳，取二者最大值 registrant = existingLease.getHolder(); } Lease\u0026lt;InstanceInfo\u0026gt; lease = new Lease\u0026lt;InstanceInfo\u0026gt;(registrant, leaseDuration); if (existingLease != null) { // 已存在Lease则取上次up时间戳 lease.setServiceUpTimestamp(existingLease.getServiceUpTimestamp()); } public Lease(T r, int durationInSecs) { holder = r; registrationTimestamp = System.currentTimeMillis(); // 当前时间 lastUpdateTimestamp = registrationTimestamp; duration = (durationInSecs * 1000); } 同步给其他peer InstanceInfo写入到本地registry之后，然后同步给其他peer节点，对应方法com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#replicateToPeers。如果当前节点接收到的InstanceInfo本身就是另一个节点同步来的，则不会继续同步给其他节点，避免形成“广播效应”；InstanceInfo同步时会排除当前节点。\nInstanceInfo的状态有依以下几种：Heartbeat, Register, Cancel, StatusUpdate, DeleteStatusOverride，默认情况下同步操作时批量异步执行的，同步请求首先缓存到Map中，key为requestType+appName+id，然后由发送线程将请求发送到peer节点。\nPeer之间的状态是采用异步的方式同步的，所以不保证节点间的状态一定是一致的，不过基本能保证最终状态是一致的。结合服务发现的场景，实际上也并不需要节点间的状态强一致。在一段时间内（比如30秒），节点A比节点B多一个服务实例或少一个服务实例，在业务上也是完全可以接受的（Service Consumer侧一般也会实现错误重试和负载均衡机制）。所以按照CAP理论，Eureka的选择就是放弃C，选择AP。 如果同步过程中，出现了异常怎么办呢，这时会根据异常信息做对应的处理，如果是读取超时或者网络连接异常，则稍后重试；如果其他异常则打印错误日志不再后续处理。\n服务续约 Renew（服务续约）操作由Service Provider定期调用，类似于heartbeat。主要是用来告诉Eureka Server Service Provider还活着，避免服务被剔除掉。renew接口实现方式和register基本一致：首先更新自身状态，再同步到其它Peer，服务续约也就是把过期时间设置为当前时间加上duration的值。\n注意：服务注册如果InstanceInfo不存在则加入，存在则更新；而服务预约只是进行更新，如果InstanceInfo不存在直接返回false。\n服务失效剔除 Eureka Server中有一个EvictionTask，用于检查服务是否失效。Eviction（失效服务剔除）用来定期（默认为每60秒）在Eureka Server检测失效的服务，检测标准就是超过一定时间没有Renew的服务。默认失效时间为90秒，也就是如果有服务超过90秒没有向Eureka Server发起Renew请求的话，就会被当做失效服务剔除掉。失效时间可以通过eureka.instance.leaseExpirationDurationInSeconds进行配置，定期扫描时间可以通过eureka.server.evictionIntervalTimerInMs进行配置。\n服务剔除#evict方法中有很多限制，都是为了保证Eureka Server的可用性：比如自我保护时期不能进行服务剔除操作、过期操作是分批进行、服务剔除是随机逐个剔除，剔除均匀分布在所有应用中，防止在同一时间内同一服务集群中的服务全部过期被剔除，以致大量剔除发生时，在未进行自我保护前促使了程序的崩溃。\n服务信息拉取 Eureka consumer服务信息的拉取分为全量式拉取和增量式拉取，eureka consumer启动时进行全量拉取，运行过程中由定时任务进行增量式拉取，如果网络出现异常，可能导致先拉取的数据被旧数据覆盖（比如上一次拉取线程获取结果较慢，数据已更新情况下使用返回结果再次更新，导致数据版本落后），产生脏数据。对此，eureka通过类型AtomicLong的fetchRegistryGeneration对数据版本进行跟踪，版本不一致则表示此次拉取到的数据已过期。\nfetchRegistryGeneration过程是在拉取数据之前，执行fetchRegistryGeneration.get获取当前版本号，获取到数据之后，通过fetchRegistryGeneration.compareAndSet来判断当前版本号是否已更新。 注意：如果增量式更新出现意外，会再次进行一次全量拉取更新。\nEureka server的伸缩容 Eureka Server是怎么知道有多少Peer的呢？Eureka Server在启动后会调用EurekaClientConfig.getEurekaServerServiceUrls来获取所有的Peer节点，并且会定期更新。定期更新频率可以通过eureka.server.peerEurekaNodesUpdateIntervalMs配置。\n这个方法的默认实现是从配置文件读取，所以如果Eureka Server节点相对固定的话，可以通过在配置文件中配置来实现。如果希望能更灵活的控制Eureka Server节点，比如动态扩容/缩容，那么可以override getEurekaServerServiceUrls方法，提供自己的实现，比如我们的项目中会通过数据库读取Eureka Server列表。\neureka server启动时把自己当做是Service Consumer从其它Peer Eureka获取所有服务的注册信息。然后对每个服务信息，在自己这里执行Register，isReplication=true，从而完成初始化。\nService Provider Service Provider启动时首先时注册到Eureka Service上，这样其他消费者才能进行服务调用，除了在启动时之外，只要实例状态信息有变化，也会注册到Eureka Service。需要注意的是，需要确保配置eureka.client.registerWithEureka=true。register逻辑在方法AbstractJerseyEurekaHttpClient.register中，Service Provider会依次注册到配置的Eureka Server Url上，如果注册出现异常，则会继续注册其他的url。\nRenew操作会在Service Provider端定期发起，用来通知Eureka Server自己还活着。 这里instance.leaseRenewalIntervalInSeconds属性表示Renew频率。默认是30秒，也就是每30秒会向Eureka Server发起Renew操作。这部分逻辑在HeartbeatThread类中。在Service Provider服务shutdown的时候，需要及时通知Eureka Server把自己剔除，从而避免客户端调用已经下线的服务，逻辑本身比较简单，通过对方法标记@PreDestroy，从而在服务shutdown的时候会被触发。\nService Consumer Service Consumer这块的实现相对就简单一些，因为它只涉及到从Eureka Server获取服务列表和更新服务列表。Service Consumer在启动时会从Eureka Server获取所有服务列表，并在本地缓存。需要注意的是，需要确保配置eureka.client.shouldFetchRegistry=true。由于在本地有一份Service Registries缓存，所以需要定期更新，定期更新频率可以通过eureka.client.registryFetchIntervalSeconds配置。\n总结 我们为什么要使用Eureka呢，在分布式开发架构中， 任何单点的服务都不能保证不会中断，因此需要服务发现机制，某个节点中断后，服务消费者能及时感知到保证服务高可用。注册中心除了Eureka之外，还有Zookeeper、consul、nacos等解决方案，实现原理不同， 各自适用于不同业务场景。\n数据一致性问题 事务 严格意义上的事务实现应该是具备原子性、一致性、隔离性和持久性，简称ACID。\n原子性(Atomicity) ， 可以理解为一个事务内的所有操作要么都执行，要么都不执行。 一致性(Consistency)， 数据是满足完整性约束的，也就是不会存在中间状态的数据，比如说你账户上有400， 我账户上有100， 你给我打200块，此时你账户上的钱应该是200， 我账户上的钱应该是300， 不会存在我账户上的钱加了，你账户上的钱没扣的中间状态 隔离性(Lsolation) ，指的是多个事务并发执行的时候不会互相干扰，即事务内部的数据对于其他事务来说是隔离的 持久性(Durability), 指的是一个事务完成了之后数据就被永远保存下来，之后的其他操作或故障都不会对事务的结果产生影响 而通俗意义上事务就是为了使得一些更新操作要么都成功，要么都失败。\n分布式事务 分布式事务顾名思义就是要在分布式系统中实现事务，它其实是由多个本地事务组合而成。\n对于分布式事务而言几乎满足不了ACID，其实对于单机事务而言大部分情况下也没有满足ACID，不然怎么会有四种隔离级别呢？所以更不用说分布在不用数据库或者不同应用上的分布式事务了。\n2PC 2PC（Two-phase commit protocol），中文叫二阶段提交。 二阶段提交是一种强一致性设计，2PC 引入一个事务协调者的角色来协调管理各参与者（也可称之为各本地资源）的提交和回滚，二阶段分别指的是准备（投票）和提交两个阶段。\n注意这只是协议或者说是理论指导，只阐述了大方向，具体落地还是有会有差异的。\n让我们来看下两个阶段的具体流程。\n准备阶段协调者会给各参与者发送准备命令，你可以把准备命令理解成除了提交事务之外啥事都做完了。\n同步等待所有资源的响应之后就进入第二阶段即提交阶段（注意提交阶段不一定是提交事务，也可能是回滚事务）。\n假如在第一阶段所有参与者都返回准备成功，那么协调者则向所有参与者发送提交事务命令，然后等待所有事务都提交成功之后，返回事务执行成功。\n假如在第一阶段有一个参与者返回失败，那么协调者就会向所有参与者发送回滚事务的请求，即分布式事务执行失败。\n那第二阶段提交失败的话呢？\n这里有两种情况。\n第一种是第二阶段执行的是回滚事务操作，那么答案是不断重试，直到所有参与者都回滚了，不然那些在第一阶段准备成功的参与者会一直阻塞着。\n第二种是第二阶段执行的是提交事务操作，那么答案也是不断重试，因为有可能一些参与者的事务已经提交成功了，这个时候只有一条路，就是头铁往前冲，不断的重试，直到提交成功，到最后真的不行只能人工介入处理。\n大体上二阶段提交的流程就是这样，我们再来看看细节。\n首先 2PC 是一个同步阻塞协议，像第一阶段协调者会等待所有参与者响应才会进行下一步操作，当然第一阶段的协调者有超时机制，假设因为网络原因没有收到某参与者的响应或某参与者挂了，那么超时后就会判断事务失败，向所有参与者发送回滚命令。\n在第二阶段协调者的没法超时，因为按照我们上面分析只能不断重试！\n协调者故障分析 协调者是一个单点，存在单点故障问题\n假设协调者在发送准备命令之前挂了， 还行，等于事务没开始。\n假设协调者在发送准备命令之后挂了，这就不太行了，有些参与者等于都执行了处于事务资源锁定的状态。不仅事务执行不下去，还会因为锁定了一些公共资源而阻塞系统其他操作。\n假设协调者在发送事务回滚命令之前挂了，那么事务也是执行不下去，且在第一阶段那些准备成功参与者都阻塞着。\n假设协调者在发送回滚事务命令之后挂了，这个还行，至少命令发出去了，很大概率都会回滚成功，资源都会释放。但是如果出现网络分区问题，某些参与者将因为收不到命令而阻塞着。\n假设协调者在发送提交事务命令之前挂了，这个不行，这下所有资源都阻塞着。\n假设协调者在发送提交事务命令之后挂了，很大概率都会提交成功，然后释放资源。但是如果出现网络分区问题某些参与者因为收不到命令而阻塞着。\n协调者故障，通过选举得到新的协调者 因为协调者单点问题，因此我们可以通过选举等操作选出一个新协调者来顶替。\n如果处于第一阶段，其实影响不大都回滚好了，在第一阶段事务肯定还没提交。\n如果处于第二阶段，假设参与者都没挂，此时新协调者可以向所有参与者确认它们自身情况来推断下一步的操作。\n假设有个别参与者挂了！这就有点僵硬了，比如协调者发送了回滚命令，此时第一个参与者收到了并执行，然后协调者和第一个参与者都挂了。\n此时其他参与者都没收到请求，然后新协调者来了，它询问其他参与者都说OK，但它不知道挂了的那个参与者到底O不OK，所以它傻了。\n问题其实就出在每个参与者自身的状态只有自己和协调者知道，因此新协调者无法通过在场的参与者的状态推断出挂了的参与者是什么情况。\n虽然协议上没说，不过在实现的时候我们可以灵活的让协调者将自己发过的请求在哪个地方记一下，也就是日志记录，这样新协调者来的时候不就知道此时该不该发了？\n但是就算协调者知道自己该发提交请求，那么在参与者也一起挂了的情况下没用，因为你不知道参与者在挂之前有没有提交事务。\n如果参与者在挂之前事务提交成功，新协调者确定存活着的参与者都没问题，那肯定得向其他参与者发送提交事务命令才能保证数据一致。\n如果参与者在挂之前事务还未提交成功，参与者恢复了之后数据是回滚的，此时协调者必须是向其他参与者发送回滚事务命令才能保持事务的一致。\n所以说极端情况下还是无法避免数据不一致问题。\ntalk is cheap 让我们再来看下代码，可能更加的清晰。以下代码取自 Distributed System: Principles and Paradigms。\n这个代码就是实现了 2PC，但是相比于2PC增加了写日志的动作、参与者之间还会互相通知、参与者也实现了超时。这里要注意，一般所说的2PC，不含上述功能，这都是实现的时候添加的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 协调者: write START_2PC to local log; //开始事务 multicast VOTE_REQUEST to all participants; //广播通知参与者投票 while not all votes have been collected { wait for any incoming vote; if timeout { //协调者超时 write GLOBAL_ABORT to local log; //写日志 multicast GLOBAL_ABORT to all participants; //通知事务中断 exit; } record vote; } //如果所有参与者都ok if all participants sent VOTE_COMMIT and coordinator votes COMMIT { write GLOBAL_COMMIT to local log; multicast GLOBAL_COMMIT to all participants; } else { write GLOBAL_ABORT to local log; multicast GLOBAL_ABORT to all participants; } 参与者: write INIT to local log; //写日志 wait for VOTE_REQUEST from coordinator; if timeout { //等待超时 write VOTE_ABORT to local log; exit; } if participant votes COMMIT { write VOTE_COMMIT to local log; //记录自己的决策 send VOTE_COMMIT to coordinator; wait for DECISION from coordinator; if timeout { multicast DECISION_REQUEST to other participants; //超时通知 wait until DECISION is received; /* remain blocked*/ write DECISION to local log; } if DECISION == GLOBAL_COMMIT write GLOBAL_COMMIT to local log; else if DECISION == GLOBAL_ABORT write GLOBAL_ABORT to local log; } else { write VOTE_ABORT to local log; send VOTE_ABORT to coordinator; } 每个参与者维护一个线程处理其它参与者的DECISION_REQUEST请求： while true { wait until any incoming DECISION_REQUEST is received; read most recently recorded STATE from the local log; if STATE == GLOBAL_COMMIT send GLOBAL_COMMIT to requesting participant; else if STATE == INIT or STATE == GLOBAL_ABORT; send GLOBAL_ABORT to requesting participant; else skip; /* participant remains blocked */ } 至此已经详细分析了2PC的各种细节，总结如下：\n2PC是一种尽量保证强一致性的分布式事务，因此它是同步阻塞的，而同步阻塞就导致长久的资源锁定问题，总体而言效率低，并且存在单点故障问题，在极端条件下存在数据不一致的风险。\n当然具体的实现可以变形，比如Tree 2PC、Dynamic 2PC\n2PC适用于数据库层面的分布式事务场景，而我们业务需求有时候不仅仅关乎数据库，也有可能是上传一张图片或者发送一条短信。\n而且像Java中的JTA, 它是基于XA规范实现的事务接口，这里的XA可以简单理解为基于数据库的XA规范来实现的2PC。\n解决方案 XA方案 2PC的传统方案是在数据库层面实现的，如 Oracle、MySQL 都支持 2PC 协议，为了统一标准减少行业内不必要的对接成本，需要制定标准化的处理模型及接口标准，国际开放标准组织 Open Group 定义了分布式事务处理模型DTP（Distributed Transaction Processing Reference Model）。\n整个 2PC 的事务流程涉及到三个角色 AP、RM、TM。AP 指的是使用 2PC 分布式事务的应用程序；RM 指的是资源管理器，它控制着分支事务；TM 指的是事务管理器，它控制着整个全局事务。\n（1）在准备阶段 RM 执行实际的业务操作，但不提交事务，资源锁定\n（2）在提交阶段 TM 会接受 RM 在准备阶段的执行回复，只要有任一个RM执行失败，TM 会通知所有 RM 执行回滚操作，否则，TM 将会通知所有 RM 提交该事务。提交阶段结束资源锁释放。\nXA方案的问题\n需要本地数据库支持XA协议。 资源锁需要等到两个阶段结束才释放，性能较差。\nSeata方案 Seata 是由阿里中间件团队发起的开源项目 Fescar，后更名为 Seata，它是一个是开源的分布式事务框架。\n传统 2PC 的问题在 Seata 中得到了解决，它通过对本地关系数据库的分支事务的协调来驱动完成全局事务，是工作在应用层的中间件。主要优点是性能较好，且不长时间占用连接资源，它以高效并且对业务 0 侵入的方式解决微服务场景下面临的分布式事务问题，它目前提供 AT 模式（即 2PC）及 TCC 模式的分布式事务解决方案。\nSeata 的设计思想如下: Seata 的设计目标其一是对业务无侵入，因此从业务无侵入的 2PC 方案着手，在传统 2PC的基础上演进，并解决 2PC 方案面临的问题。\nSeata 把一个分布式事务理解成一个包含了若干分支事务的全局事务。全局事务的职责是协调其下管辖的分支事务达成一致，要么一起成功提交，要么一起失败回滚。此外，通常分支事务本身就是一个关系数据库的本地事务。\nSeata实现2PC与传统2PC的差别\n架构层次方面：传统 2PC 方案的 RM 实际上是在数据库层，RM 本质上就是数据库自身，通过 XA 协议实现，而 Seata 的 RM 是以 jar 包的形式作为中间件层部署在应用程序这一侧的。\n两阶段提交方面：传统 2PC无论第二阶段的决议是 commit 还是 rollback ，事务性资源的锁都要保持到 Phase2 完成才释放。而 Seata 的做法是在 Phase1 就将本地事务提交，这样就可以省去 Phase2 持锁的时间，整体提高效率。\n","permalink":"https://csqread.top/posts/tech/%E6%8B%BC%E5%A4%9A%E5%A4%9A%E9%9D%A2%E7%BB%8F/","summary":"面试题目 记录下社招面试拼多多的总结与心得，以及失败的原因吧。 1、服务注册是如何发现的，eureka的基本原理， 容器ip是动态的还是静态的 答：","title":"拼多多面经"},{"content":"ArrayList 1. 概览 实现了 RandomAccess 接口，因此支持随机访问。这是理所当然的，因为 ArrayList 是基于数组实现的。\n1 2 public class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。\n1 private static final int DEFAULT_CAPACITY = 10; 2. 扩容 添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1)，也就是旧容量的 1.5 倍。\n扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length \u0026gt; 0) grow(minCapacity); } private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } 3. 删除元素 需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看出 ArrayList 删除元素的代价是非常高的。\n1 2 3 4 5 6 7 8 9 10 public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue; } 4. Fail-Fast modCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。\n在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i\u0026lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); } } 5. 序列化 ArrayList 基于数组实现，并且具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。\n保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化。\n1 transient Object[] elementData; // non-private to simplify nested class access ArrayList 实现了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size \u0026gt; 0) { // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i\u0026lt;size; i++) { a[i] = s.readObject(); } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i\u0026lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); } } 序列化时需要使用 ObjectOutputStream 的 writeObject() 将对象转换为字节流并输出。而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。反序列化使用的是 ObjectInputStream 的 readObject() 方法，原理类似。\n1 2 3 ArrayList list = new ArrayList(); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file)); oos.writeObject(list); ","permalink":"https://csqread.top/posts/tech/java-arraylist%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%8E%E6%80%BB%E7%BB%93/","summary":"ArrayList 1. 概览 实现了 RandomAccess 接口，因此支持随机访问。这是理所当然的，因为 ArrayList 是基于数组实现的。 1 2 public class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。 1 private static final int DEFAULT_CAPACITY =","title":"Java ArrayList源码分析与总结"},{"content":"缓存优化 1、对应参数表数据量如果小于10M，数据要同时缓存到Redis和本地，查询遵循的缓存原则为:优先查询本地缓存，本地没有查询Redis缓存，Redis没有再查询数据库.\n2、对参数表的缓存要分类进行配置：黑名单类参数表查询，Redis缓存为空时不再查询数据库：常规类参数表查询，Redis缓存为空的时候需要再查询数据库\n3、如果参数表出现大量RPC。确定SQL都是等值的情况下，一般是缓存索引配置的不合适/\n4、如果出现大量RPC，在SQL非等值情况下，需要通过等值SQL去查询，然后在程序代码中去判断数据是否符合要求。\n5、针对查询比较多并且修改也比较多的数据，可以针对部分不变的数据配置缓存。 （比如针对内部合约相关的数据，需要频繁的调用内部账的科目存储字段，虽然内部户的一整条数据会经常变动，但是内部户的科目存储字段基本上不会改变，就可以把这些不变的数据配置缓存，提升查询效率，并且可以减少RPC的次数）。\n6、根据不同条件调用他组缓存表的接口时，可以现根据他组配置缓存索引条件进行查询，然后在程序中再根据非索引条件过滤查询结果。\n编码优化 1、当某业务构件执行缓慢时，除了要排查是否有非必要RPC时或环境影响因素外，还需要检查构件中是否有重复执行的代码和SQL，第一次执行向后传递可以提升传递效率。\n2、如果通过实现JAR包调用他组接口还RPC了的，首先检查他们是否缓存表以及缓存表索引配置是否正确，如以上没问题，则可能是JAR包中未打入实现类。\n3、因为Java接口中传递对象是通过引用方式传递的，如果接口对传入的对象进行了修改，当执行完接口在接口外部拿到的该对象中的数据是被修改的，此时在接口外部继续获取对象中被修改的原数据时容易得到意想不到的结果。\n4、调用某构件的时候，构件中又要获取一些数据进行RPC，如果调用构件之前已有相关数据，可以调用构件时传递进去，可减少RPC。\n5、根据不同条件多次使用其他组的接口进行查询时，可以提取多个条件的交集，根据交集条件查询所有数据，然后在程序中分别根据非交集的条件进行过滤。\n6、对于多次循环调用他组同一个方法时，每次输入的值不同，可将所有输入值包装成LIST一次性传入，得到一个查询列表集合作为类似本地缓存，然后对这个LSIT集合进行循环整理。\n7、RPC接口要尽量简单，输入输出接口不要继承一些不需要的东西，既能减少网络传输消耗，还能减少序列化和反序列化的时间。\n数据库及SQL优化 1、尽量避免使用SELECT * 来查询所有字段，仅查询必要数据行及字段，既能减少网络传输消耗，还能提高命中覆盖索引的概率。\n2、写SQK的时候永远记得WHERE条件要和主键或者索引匹配。\n3、对于查询量非常大修改比较少的表， 且SELECT的字段正好比索引多一个或者两个字段的时候，可以采取把多余的一个或者两个字段冗余到索引上，这种一空间换取时间的方式虽然一定程度上增加了索引空间的开销，但是对于非常高频的SQL直接命中了覆盖索引，避免了回表查询，有助于提升查询效率。\n4、对于修改量非常大的交易，要及其精简索引，能不要的索引最好不要建，在修改表的时候可以减少索引的维护，有助于提升修改效率。\n以上是工作中的性能优化总结，后续如果有新的总结再来记录\n","permalink":"https://csqread.top/posts/tech/%E5%88%86%E5%B8%83%E5%BC%8F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/","summary":"缓存优化 1、对应参数表数据量如果小于10M，数据要同时缓存到Redis和本地，查询遵循的缓存原则为:优先查询本地缓存，本地没有查询Redis","title":"分布式性能优化总结"},{"content":"字符操作 编码与解码 编码就是把字符转换为字节，而解码是把字节重新组合成字符。\n如果编码和解码过程使用不同的编码方式那么就出现了乱码。\nGBK 编码中，中文字符占 2 个字节，英文字符占 1 个字节； UTF-8 编码中，中文字符占 3 个字节，英文字符占 1 个字节； UTF-16be 编码中，中文字符和英文字符都占 2 个字节。 UTF-16be 中的 be 指的是 Big Endian，也就是大端。相应地也有 UTF-16le，le 指的是 Little Endian，也就是小端。\nJava 的内存编码使用双字节编码 UTF-16be，这不是指 Java 只支持这一种编码方式，而是说 char 这种类型使用 UTF-16be 进行编码。char 类型占 16 位，也就是两个字节，Java 使用这种双字节编码是为了让一个中文或者一个英文都能使用一个 char 来存储。\nString 的编码方式 String 可以看成一个字符序列，可以指定一个编码方式将它编码为字节序列，也可以指定一个编码方式将一个字节序列解码为 String。\n1 2 3 4 String str1 = \u0026#34;中文\u0026#34;; byte[] bytes = str1.getBytes(\u0026#34;UTF-8\u0026#34;); String str2 = new String(bytes, \u0026#34;UTF-8\u0026#34;); System.out.println(str2); 在调用无参数 getBytes() 方法时，默认的编码方式不是 UTF-16be。双字节编码的好处是可以使用一个 char 存储中文和英文，而将 String 转为 bytes[] 字节数组就不再需要这个好处，因此也就不再需要双字节编码。getBytes() 的默认编码方式与平台有关，一般为 UTF-8。\n1 byte[] bytes = str1.getBytes(); Reader 与 Writer 不管是磁盘还是网络传输，最小的存储单元都是字节，而不是字符。但是在程序中操作的通常是字符形式的数据，因此需要提供对字符进行操作的方法。\nInputStreamReader 实现从字节流解码成字符流； OutputStreamWriter 实现字符流编码成为字节流。 实现逐行输出文本文件的内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public static void readFileContent(String filePath) throws IOException { FileReader fileReader = new FileReader(filePath); BufferedReader bufferedReader = new BufferedReader(fileReader); String line; while ((line = bufferedReader.readLine()) != null) { System.out.println(line); } // 装饰者模式使得 BufferedReader 组合了一个 Reader 对象 // 在调用 BufferedReader 的 close() 方法时会去调用 Reader 的 close() 方法 // 因此只要一个 close() 调用即可 bufferedReader.close(); } ","permalink":"https://csqread.top/posts/tech/java%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E6%93%8D%E4%BD%9C/","summary":"字符操作 编码与解码 编码就是把字符转换为字节，而解码是把字节重新组合成字符。 如果编码和解码过程使用不同的编码方式那么就出现了乱码。 GBK 编码中，中","title":"Java中的字符操作"},{"content":"自旋锁 互斥同步进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求一个共享数据的锁时执行忙循环（自旋）一段时间，如果在这段时间内能获得锁，就可以避免进入阻塞状态。\n自旋锁虽然能避免进入阻塞状态从而减少开销，但是它需要进行忙循环操作占用 CPU 时间，它只适用于共享数据的锁定状态很短的场景。\n在 JDK 1.6 中引入了自适应的自旋锁。自适应意味着自旋的次数不再固定了，而是由前一次在同一个锁上的自旋次数及锁的拥有者的状态来决定。\n锁消除 锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。\n锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。\n对于一些看起来没有加锁的代码，其实隐式的加了很多锁。例如下面的字符串拼接代码就隐式加了锁：\n1 2 3 public static String concatString(String s1, String s2, String s3) { return s1 + s2 + s3; } String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作：\n1 2 3 4 5 6 7 public static String concatString(String s1, String s2, String s3) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString(); } 每个 append() 方法中都有一个同步块。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是说，sb 的所有引用永远不会逃逸到 concatString() 方法之外，其他线程无法访问到它，因此可以进行消除。\n锁粗化 如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。\n上一节的示例代码中连续的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部。对于上一节的示例代码就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。\n轻量级锁 JDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。\n以下是 HotSpot 虚拟机对象头的内存布局，这些数据被称为 Mark Word。其中 tag bits 对应了五个状态，这些状态在右侧的 state 表格中给出。除了 marked for gc 状态，其它四个状态已经在前面介绍过了。\n下图左侧是一个线程的虚拟机栈，其中有一部分称为 Lock Record 的区域，这是在轻量级锁运行过程创建的，用于存放锁对象的 Mark Word。而右侧就是一个锁对象，包含了 Mark Word 和其它信息。\n轻量级锁是相对于传统的重量级锁而言，它使用 CAS 操作来避免重量级锁使用互斥量的开销。对于绝大部分的锁，在整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步，如果 CAS 失败了再改用互斥量进行同步。\n当尝试获取一个锁对象时，如果锁对象标记为 0 01，说明锁对象的锁未锁定（unlocked）状态。此时虚拟机在当前线程的虚拟机栈中创建 Lock Record，然后使用 CAS 操作将对象的 Mark Word 更新为 Lock Record 指针。如果 CAS 操作成功了，那么线程就获取了该对象上的锁，并且对象的 Mark Word 的锁标记变为 00，表示该对象处于轻量级锁状态。\n如果 CAS 操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的虚拟机栈，如果是的话说明当前线程已经拥有了这个锁对象，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。\n偏向锁 偏向锁的思想是偏向于让第一个获取锁对象的线程，这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。\n当锁对象第一次被线程获得的时候，进入偏向状态，标记为 1 01。同时使用 CAS 操作将线程 ID 记录到 Mark Word 中，如果 CAS 操作成功，这个线程以后每次进入这个锁相关的同步块就不需要再进行任何同步操作。\n当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向（Revoke Bias）后恢复到未锁定状态或者轻量级锁状态。\n","permalink":"https://csqread.top/posts/tech/java%E9%94%81%E4%BC%98%E5%8C%96%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/","summary":"自旋锁 互斥同步进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求","title":"Java锁优化相关笔记"},{"content":"C. x的最低有效字节中的位都等于1 D. x的最高有效字节中的位都等于0 代码应该遵循位级整数编码规则，另外还有一个限制，不能使用(==)和(!=)测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 /* * 2.61.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; int A(int x) { return !~x; } int B(int x) { return !x; } int C(int x) { return A(x | ~0xff); } int D(int x) { return B((x \u0026gt;\u0026gt; ((sizeof(int)-1) \u0026lt;\u0026lt; 3)) \u0026amp; 0xff); } int main(int argc, char* argv[]) { int all_bit_one = ~0; int all_bit_zero = 0; assert(A(all_bit_one)); assert(!B(all_bit_one)); assert(C(all_bit_one)); assert(!D(all_bit_one)); assert(!A(all_bit_zero)); assert(B(all_bit_zero)); assert(!C(all_bit_zero)); assert(D(all_bit_zero)); // test magic number 0x1234ff assert(!A(0x1234ff)); assert(!B(0x1234ff)); assert(C(0x1234ff)); assert(D(0x1234ff)); // test magic number 0x1234 assert(!A(0x1234)); assert(!B(0x1234)); assert(!C(0x1234)); assert(D(0x1234)); return 0; } 2.62编写一个函数int_shifts_are_arithemtic()，在对int类型的数使用算数右移的机器上运行时这个函数生成1，而其他情况下生成0.你的代码应该可以运行在任何字长的机器上。在几种机器上测试你的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /* * int-shifts-are-arithemetic.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; int int_shifts_are_arithemetic() { int num = -1; return !(num ^ (num \u0026gt;\u0026gt; 1)); } int main(int argc, char* argv[]) { assert(int_shifts_are_arithemetic()); return 0; } 2.63 将下面的c代码补充完整。函数srl用算术右移(由值xsra给出)来完成逻辑右移，后面的其他操作不包括右移或者除法。函数sra用逻辑右移(由值xsrl给出)来完成算术右移，后面的其他操作不包括右移或者除法。可以通过计算8*sizeof(int)来确定数据类型int中的位数w。位移量k的取值范围为0~w-1.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 /* * srl-sra.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; unsigned srl(unsigned x, int k) { unsigned xsra = (int) x \u0026gt;\u0026gt; k; int w = sizeof(int) \u0026lt;\u0026lt; 3; int mask = (int) -1 \u0026lt;\u0026lt; (w - k); return xsra \u0026amp; ~mask; } int sra(int x, int k) { int xsrl = (unsigned) x \u0026gt;\u0026gt; k; int w = sizeof(int) \u0026lt;\u0026lt; 3; int mask = (int) -1 \u0026lt;\u0026lt; (w - k); //let mask remain unchanged when the first bit of x is 1, otherwise 0. int m = 1 \u0026lt;\u0026lt; (w - 1); mask \u0026amp;= ! (x \u0026amp; m) - 1; return xsrl | mask; } int main(int argc, char* argv[]) { unsigned test_unsigned = 0x12345678; int test_int = 0x12345678; assert(srl(test_unsigned, 4) == test_unsigned \u0026gt;\u0026gt; 4); assert(sra(test_int, 4) == test_int \u0026gt;\u0026gt; 4); test_unsigned = 0x87654321; test_int = 0x87654321; assert (srl (test_unsigned, 4) == test_unsigned \u0026gt;\u0026gt; 4); assert (sra (test_int, 4) == test_int \u0026gt;\u0026gt; 4); return 0; } 2.64 写出代码实现如下函数：\n1 2 /*Return 1 when any odd bit of x equals 1; 0 otherwise. Assume w=32 */ int any_odd_one(unsigned x) 函数应该遵循位级整数编码规则，不过你可以假设数据类型int有w=32位。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /* * any-odd-one.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; int any_odd_one(unsigned x) { return !!(0xAAAAAAAA \u0026amp; x); } int main(int argc, char* argv[]) { assert(any_odd_one(0x2)); assert(!any_odd_one(0x4)); return 0; } 2.65 写出代码实现如下函数：\n1 2 /* return 1 when x contains an odd number of 1s; 0 otherwise. Assume w =32 */ int odd_ones(unsigned x); 函数应该遵循位级整数编码规则，不过你可以假设数据类型int有w=32位。你的代码最多只能包含12个算数运算，位运算和逻辑运算。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /* * odd-ones.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; int odd_ones(unsigned x) { x ^= x \u0026gt;\u0026gt; 16; x ^= x \u0026gt;\u0026gt; 8; x ^= x \u0026gt;\u0026gt; 4; x ^= x \u0026gt;\u0026gt; 2; x ^= x \u0026gt;\u0026gt; 1; x \u0026amp;= 0x1; return x; } int main(int argc, char* argv[]) { assert(odd_ones(0x10101011)); assert(!odd_ones(0x01010101)); return 0; } 代码参考自GitHub\n","permalink":"https://csqread.top/posts/tech/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F-chapter-2-practice/","summary":"C. x的最低有效字节中的位都等于1 D. x的最高有效字节中的位都等于0 代码应该遵循位级整数编码规则，另外还有一个限制，不能使用(==)和(!=)测","title":"深入理解计算机系统 Chapter 2 Practice"},{"content":" 我大概是从3月中旬发现这部剧集的，那个时候还是因为在tg的某句台词引起了我的兴趣，在这里就不说了，说了这篇公众号就发不出去了，说不定我的号也就没了。然后我就开始在各种电影网站搜罗资源，结果发现低端影视已经出了好几集了，ddys站长真的是良心大大滴好，以后有钱了我一定要donate，现在就白嫖吧。\n说到Alex Garland，我不得不想起来以前看过的他的两部科幻电影。一部是《机械姬》，另一部是《湮灭》。两部电影分别从人工智能和生物科学的主题进行探讨，从这两部以及现在的《devs》来看，他的电影的画面都有那种给虚无缥缈的一种奇幻的氛围，\n不过，这部Devs从量子力学的角度来探讨在今天还是很热门，一开始看得时候我有点懵，不懂什么叫做量子力学，看完第一集我去维基百科查看了一下量子力学的定义：\n量子力学（英语：quantum mechanics）是物理学的分支学科。它主要描写微观的事物，与相对论一起被认为是现代物理学的两大基本支柱，许多物理学理论和科学，如原子物理学、固体物理学、核物理学和粒子物理学以及其它相关的学科，都是以其为基础。\n19世纪末，人们发现旧有的经典理论无法解释微观系统，于是经由物理学家的努力，在20世纪初创立量子力学，解释了这些现象。量子力学从根本上改变人类对物质结构及其相互作用的理解。除了透过广义相对论描写的引力外，迄今所有基本相互作用均可以在量子力学的框架内描述（量子场论）。\n量子理论的重要应用包括量子化学、量子光学、量子计算、超导磁体、发光二极管、激光器、晶体管和半导体如微处理器等。\n这里面涉及到的东西太多，我看了半天也就了解了几个概念的定义，比如量子纠缠，量子退相干等等，总之一句话：不知道是什么玩意\n所以人们经常有一个说法：人类如果能够突破量子力学，也就能够确认灵魂是否存在。在众多的科幻小说里面，量子力学主要是突破意识的决定存在。我们经常所说的“薛定谔的猫”，“电子双缝实验”以及“上帝掷骰子”也有些相关性。不过在影视作品中这个概念特别吃香：自我最早看到的变形金刚系列开始的美帝影视作品中就开始：遇事不决，量子力学\n而《Devs》是一部与量子力学强相关的作品。\n何为强相关？强相关又称高度相关，即当一列变量变化时，与之相应的另一列变量增大（或减少）的可能性非常大。在坐标图上则表现为散点图较为集中在某条直线的周围。-摘自百度百科；\n剧中有专门研究量子力学的公司，并且取得了突破性的成果。不过不仅仅如此，这部剧中还涉及到了多重宇宙，自由意志以及万事万物之间都有因果关系这些概念，看起来还是挺有意思的，能够感受到美帝的电影人在这方面确实有些功底，或者说下了不少功夫。\n并且，我认为，这还是一部不可多得的悬疑剧，和之前的《误杀》一样足以调动观众的胃口，感觉不巧的是同时和《西部世界》第三季上映，让很多人没有能够了解到这部剧集。\n第一集主要介绍了一下这个量子力学的公司以及男主女主的出现：\n阿玛雅\u0026ndash;量子未来 故事开始，阿玛雅的工程师谢尔盖和团队争取到了15分钟，在CEO面前演示自己团队的成果\u0026ndash;线虫仿真映射实验。\n什么是线虫？：\n线虫动物门是动物界中最大的门之一，为假体腔动物，有超过28,000个已被记录的物种，尚有大量种尚未命名。绝大多数体小呈圆柱形，又称圆虫（roundworms）。它们在淡水、海水、陆地上随处可见，不论是个体数或物种数都往往超越其他动物，并在极端的环境如南极和海沟都可发现。此外，有许多种的线虫是寄生性的(超过16,000种)，包括许多植物及人类在内的动物的病原体。只有节肢动物比线虫更多样化-线虫也是目前世界上唯一被完整绘制出神经网络的生物，因此可以很好的进行观测，并用计算机来模拟实验\n通过这次实验，他们团队展示了他们可以通过计算机追踪线虫的神经系统培养数据，并在电脑中模拟出一条仿真线虫。不仅仅如此，它可以预测该线虫在接下来10秒中的行为。这种科技成果意味着什么，可想而知。\n接下来，谢尔盖被CEO邀请加入Devs Devs，阿玛雅公司迄今为止最机密，最隐秘的实验项目，没有人知道它是做什么的。在加入Devs之前还做了严格的审查，在这里又爆出了美剧中常黑的中俄 不过，谢尔盖还是顺利通过了审查，CEO亲自带领谢尔盖进入Devs内部。\n在这里，就可以发现Alex Garland电影的某一共同之处，就是营造神秘气氛的手法，通过灯光变化，明暗交替以及一些硬科学的概念。\n带着谢尔盖简单的观光之后，CEO神秘的对他说，坐下来，read code，你就会明白这里的一切。谢尔盖读完代码后盯着屏幕良久，一脸懵逼，跑到厕所又哭又吐。看到这里我也是一脸懵逼加好奇。这代码有什么？还能让一个人感到这么恶心？\n谢尔盖从厕所出来后问到项目负责人Katie:这只是理论上的还是真的？代码你们已经运行过了吗？Katie淡定的回答：代码我们已经运行过了，并且已经有了结果。“但并不会改变什么，这就是意义所在”。\n晚上谢尔盖神色匆匆的离开Devs实验室，被CEO Forest撞个正着。Forest直接说出了谢尔盖的真实身份：俄罗斯特工（看到这里我有点想diss，多少年了，美帝还是在疯狂在电影里搞阴谋论）。但是这个时候Forest并没有责怪谢尔盖，而是试图让他理解，你所做的并非出于自己的自由意志，你的选择只是一系列的因果，这句话在后面也好像经常被提起过，前面的剧集好久之前看的了，有些忘记了。“如果我们生活在确定的宇宙中，这些决定，只能是之前一些事情的结果：你出生在哪，你如何长大，你大脑的物理构造，这就是先天后天矩阵，，就像你仿真演示的线虫，不过更复杂，但仍然是遵循因果。”感觉有点像我们佛家的因果循环。善有善报，恶有恶报，一切因皆有果的样子。感觉有点玄学的味道了。然后，谢尔盖被Forest的贴身保镖用塑料袋窒息，谢尔盖卒，谢尔盖女友开始着急，并开始探寻事情的真相。第一集结束\n第一集做了一个很好的铺垫，让我想知道这个Devs到底是干嘛的。后面的剧情慢慢的给出了解释：\n通过无数次的量子计算，Devs投射出了2000多年前耶稣受难的影像 “这不是想象，也不是重绘，这是对过去的量子直播\u0026quot;\n\u0026ldquo;Devs\u0026quot;在追寻神的起源\n不过看到最后就会发现Devs不仅仅可以追溯神的起源，它还可以让你变成Deus，这也就是这两张图片的寓意： 最后这两个人都变成了另一个世界的神。具体怎么变成的，最后一集中是这样描述的：他们两因为某种”宿命“或者什么原因，在Devs实验室死去，这也是他们都知道会发生的结果，CEO Forest让Katie捕获了他们死亡那个时刻的数据，通过Devs项目在计算机中进行模拟，也就是说，他们在系统中又重新复活了，并且有死亡之前的记忆，可以在另一个世界，一个真实的世界仿真的存在，并且，只要他们愿意，他们就可以重新来过。\n在理论物理中，时间并不是一个切实的存在\n只是因为意识的存在或者说生命的有限才造成了这种概念\n回想之前的线虫实验，生命体的神经活动可以被预测，而大脑的构造是写进\u0026quot;DNA\u0026quot;的，这意味着从一颗胚胎开始，未来就可以被决定。这里又让我想起来《Brave New World》，哈哈哈。\n不过这部剧不适合有巨恐的人观看，因为那个巨娃娃看着挺瘆人的 诺贝尔奖得主尤金维·格纳认为，因为存在观察者的意识，宇宙的存在才有意义\n线虫的观察者是人类，那么人类的观察者的是什么？宇宙的观察者是什么？\n是否存在一个最高的宇宙意识在观察整个宇宙？\n这里我想到了三体中的宇宙观察者的概念，一维二维三维到十维的概念。更高维度的观察者是否此刻正在观察着我们的一举一动？\n今天总算是追完了这部剧，接下来可以安安心心的看《西部世界》了。Anyway，又是胡思乱想的时刻，在这里胡思乱想。 Big Cat Is Watching You ! ","permalink":"https://csqread.top/posts/read/%E5%BC%80%E5%8F%91%E8%80%85%E5%BD%B1%E8%AF%84/","summary":"我大概是从3月中旬发现这部剧集的，那个时候还是因为在tg的某句台词引起了我的兴趣，在这里就不说了，说了这篇公众号就发不出去了，说不定我的号也","title":"《开发者》影评"},{"content":"刚开始我没想到用爬虫，就直接手创了一个字典\u0026hellip; 需要安装模块pyechart and pyechart-china-provinces-pypkg\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from pyecharts.charts import Map from pyecharts import options as opts #省和直辖市 province_distribution = { \u0026#39;湖北\u0026#39;: 4586, \u0026#39;浙江\u0026#39;: 428, \u0026#39;广东\u0026#39;: 311, \u0026#39;湖南\u0026#39;: 277, \u0026#39;河南\u0026#39;: 278, \u0026#39;安徽\u0026#39;: 200, \u0026#39;重庆\u0026#39;: 165, \u0026#39;山东\u0026#39;: 145, \u0026#39;江西\u0026#39;: 162, \u0026#39;四川\u0026#39;: 142, \u0026#39;江苏\u0026#39;: 129, \u0026#39;北京\u0026#39;: 111, \u0026#39;福建\u0026#39;: 101, \u0026#39;上海\u0026#39;: 101, \u0026#39;广西\u0026#39;: 78, \u0026#39;陕西\u0026#39;: 56, \u0026#39;河北\u0026#39;: 48, \u0026#39;云南\u0026#39;: 44, \u0026#39;海南\u0026#39;: 43, \u0026#39;黑龙江\u0026#39;: 43, \u0026#39;辽宁\u0026#39;: 39, \u0026#39;山西\u0026#39;: 35, \u0026#39;天津\u0026#39;: 28, \u0026#39;甘肃\u0026#39;: 26, \u0026#39;内蒙古\u0026#39;: 16, \u0026#39;新疆\u0026#39;: 14, \u0026#39;宁夏\u0026#39;: 12, \u0026#39;贵州\u0026#39;: 12, \u0026#39;吉林\u0026#39;: 14, \u0026#39;台湾\u0026#39;: 8, \u0026#39;香港\u0026#39;: 10, \u0026#39;澳门\u0026#39;: 7, \u0026#39;青海\u0026#39;: 6, \u0026#39;西藏\u0026#39;: 1 } # maptype = \u0026#39;china\u0026#39; 只显示全国直辖市和省级 map = Map() map.set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;2020中国疫情地图\u0026#34;), visualmap_opts=opts.VisualMapOpts(max_=3600, is_piecewise=True, pieces=[ {\u0026#34;max\u0026#34;: 5000, \u0026#34;min\u0026#34;: 1001, \u0026#34;label\u0026#34;: \u0026#34;\u0026gt;1000\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#8A0808\u0026#34;}, {\u0026#34;max\u0026#34;: 1000, \u0026#34;min\u0026#34;: 500, \u0026#34;label\u0026#34;: \u0026#34;500-1000\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#B40404\u0026#34;}, {\u0026#34;max\u0026#34;: 499, \u0026#34;min\u0026#34;: 100, \u0026#34;label\u0026#34;: \u0026#34;10-99\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#78181\u0026#34;}, {\u0026#34;max\u0026#34;: 99, \u0026#34;min\u0026#34;: 1, \u0026#34;label\u0026#34;: \u0026#34;1-9\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#F5A9A9\u0026#34;}, {\u0026#34;max\u0026#34;: 0, \u0026#34;min\u0026#34;: 0, \u0026#34;label\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#FFFFFF\u0026#34;}, ] ) ) map.add(\u0026#34;20200130中国疫情地图\u0026#34;, data_pair=province_distribution.items(), maptype=\u0026#34;china\u0026#34;, is_roam=True) map.render(\u0026#39;20200130中国1疫情地图.html\u0026#39;) 然后生成了一张图，看起来还行 但是昨天学习了API调用，以及进行可视化，今天怎么能用这么粗糙的方法来做统计，直接爬下来放到字典里然后用pyechart进行可视化岂不美哉\nmain.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 import matplotlib.pyplot as plt import numpy as np import json import requests from matplotlib.font_manager import FontProperties import re import os from pyecharts.charts import Line, Pie, Map from pyecharts import options as opts import pygal from pygal.style import LightColorizedStyle as LCS, LightenStyle as LS # 获取各省市今日的数据，存入josn文件 def get_province_data(month, day, province_name=None): file = open(\u0026#39;results/%d%d.json\u0026#39; % (month, day), \u0026#39;r\u0026#39;, encoding=\u0026#39;UTF-8\u0026#39;) json_array = json.loads(file.read()) file.close() if not province_name: return json_array for json_object in json_array: if json_object[\u0026#39;provinceName\u0026#39;] == province_name: return json_object if json_object[\u0026#39;provinceShortName\u0026#39;] == province_name: return json_object return None def get_province_status(month, day, province_name=None): if province_name: print(province_name) json_object = get_province_data(month, day, province_name) data = [] for city in json_object[\u0026#39;cities\u0026#39;]: data.append((city[\u0026#39;cityName\u0026#39;], city[\u0026#39;confirmedCount\u0026#39;])) data.sort(key=lambda x: -x[1]) title = \u0026#39;%s2020年%d月%d日确诊病例\u0026#39; % (province_name, month, day) else: json_array = get_province_data(month, day, province_name) data = [] for province in json_array: data.append((province[\u0026#39;provinceShortName\u0026#39;], province[\u0026#39;confirmedCount\u0026#39;])) data.sort(key=lambda x: -x[1]) title = \u0026#39;全国2020年%d月%d日确诊病例\u0026#39; % (month, day) labels = [d[0] for d in data] counts = [d[1] for d in data] return labels, counts, title def show_province_status(month, day, province_name=None): labels, counts, title = get_province_status(month, day, province_name) # draw_pie(month, day, labels, counts, title) get_pyecharts_pie(month, day, labels, counts, title) #饼状图 def draw_pie(month, day, labels, counts, title): if len(labels) == 0: return labels = np.array(labels) counts = np.array(counts) title += \u0026#39;-%d例\u0026#39; % sum(counts) fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\u0026#34;equal\u0026#34;)) explode = np.zeros(len(labels)) explode[np.argmax(counts)] = 0.1 wedges, texts = ax.pie(counts, wedgeprops=dict(width=0.5), startangle=-40, explode=explode) font = FontProperties(fname=\u0026#39;font/ZiXinFangYunYuanTi-2.ttf\u0026#39;) bbox_props = dict(boxstyle=\u0026#34;square,pad=0.3\u0026#34;, fc=\u0026#34;w\u0026#34;, ec=\u0026#34;k\u0026#34;, lw=0.72) kw = dict(arrowprops=dict(arrowstyle=\u0026#34;-\u0026#34;), bbox=bbox_props, zorder=0, va=\u0026#34;center\u0026#34;, fontproperties=font) for i, p in enumerate(wedges[:6]): ang = (p.theta2 - p.theta1) / 2. + p.theta1 y = np.sin(np.deg2rad(ang)) x = np.cos(np.deg2rad(ang)) horizontalalignment = {-1: \u0026#34;right\u0026#34;, 1: \u0026#34;left\u0026#34;}[int(np.sign(x))] connectionstyle = \u0026#34;angle,angleA=0,angleB={}\u0026#34;.format(ang) kw[\u0026#34;arrowprops\u0026#34;].update({\u0026#34;connectionstyle\u0026#34;: connectionstyle}) ax.annotate(\u0026#39;%s-%d例\u0026#39; % (labels[i], counts[i]), xy=(x, y), xytext=(1.35 * np.sign(x), 1.4 * y), horizontalalignment=horizontalalignment, **kw) ax.set_title(title, fontproperties=font) root = \u0026#39;charts/%d%d\u0026#39; % (month, day) create_dir(root) plt.savefig(\u0026#39;%s/%s.jpg\u0026#39; % (root, title)) plt.show() def create_dir(root): if not os.path.exists(root): os.makedirs(root) def draw(month, day): provinces = get_province_data(month, day) for p in provinces: show_province_status(month, day, p[\u0026#39;provinceShortName\u0026#39;]) show_province_status(month, day) def get_html(month, day): import requests url = \u0026#39;http://3g.dxy.cn/newh5/view/pneumonia\u0026#39; response = requests.get(url) html = str(response.content, \u0026#39;UTF-8\u0026#39;) html_file = open(\u0026#39;results/%d%d.html\u0026#39; % (month, day), \u0026#39;w\u0026#39;, encoding=\u0026#39;UTF-8\u0026#39;) html_file.write(html) html_file.close() json_file = open(\u0026#39;results/%d%d.json\u0026#39; % (month, day), \u0026#39;w\u0026#39;, encoding=\u0026#39;UTF-8\u0026#39;) matches = re.findall(\u0026#39;\\[[^\u0026gt;]+\\]\u0026#39;, html) for match in matches: if \u0026#39;provinceName\u0026#39; in json.loads(match)[0]: json_file.write(match) break json_file.close() def compare(m1, d1, m2, d2): ps1 = get_province_data(m1, d1) ps2 = get_province_data(m2, d2) ps_dict1 = {} ps_dict2 = {} for p in ps1: ps_dict1[p[\u0026#39;provinceShortName\u0026#39;]] = p[\u0026#39;confirmedCount\u0026#39;] for p in ps2: ps_dict2[p[\u0026#39;provinceShortName\u0026#39;]] = p[\u0026#39;confirmedCount\u0026#39;] data = [] for key in ps_dict2: increased_count = ps_dict2[key] if key in ps_dict1: increased_count -= ps_dict1[key] data.append((key, increased_count)) data.sort(key=lambda x: -x[1]) labels = [d[0] for d in data] counts = [d[1] for d in data] title = \u0026#39;2020年%d月%d日全国新增确诊病例\u0026#39; % (m2, d2) draw_pie(m2, d2, labels, counts, title) get_pyecharts_pie(m2, d2, labels, counts, title) def get_pyecharts_pie(month, day, labels, counts, title): title += \u0026#39;-%d例\u0026#39; % (sum(counts)) c = ( Pie(init_opts=opts.InitOpts(width=\u0026#39;1200px\u0026#39;, height=\u0026#39;700px\u0026#39;)) .add( \u0026#34;\u0026#34;, [list(z) for z in zip(labels, counts)], radius=[\u0026#34;40%\u0026#34;, \u0026#34;80%\u0026#34;], center=[\u0026#39;50%\u0026#39;, \u0026#39;60%\u0026#39;], ) .set_global_opts( title_opts=opts.TitleOpts(title=title), legend_opts=opts.LegendOpts( orient=\u0026#34;vertical\u0026#34;, pos_top=\u0026#34;15%\u0026#34;, pos_left=\u0026#34;2%\u0026#34; ), ) .set_series_opts(label_opts=opts.LabelOpts(formatter=\u0026#34;{b}: {c}\u0026#34;)) ) root = \u0026#39;html-charts/%d%d\u0026#39; % (month, day) create_dir(root) c.render() c.render(\u0026#39;%s/%s.html\u0026#39; % (root, title)) return c def draw_tendency(month, day): dates = [\u0026#39;1-%d\u0026#39; % i for i in range(16, 30)] v0 = [4, 17, 59, 78, 92, 149, 131, 259, 444, 688, 769, 1771, 1459, 1576 ] v1 = [4, 17, 59, 77, 72, 105, 69, 105, 180, 323, 371, 1291, 840, 1032] c = ( Line() .add_xaxis(dates) .add_yaxis(\u0026#34;全国新增确诊病例\u0026#34;, v0, is_smooth=True, linestyle_opts=opts.LineStyleOpts(width=4, color=\u0026#39;#B44038\u0026#39;), itemstyle_opts=opts.ItemStyleOpts( color=\u0026#39;#B44038\u0026#39;, border_color=\u0026#34;#B44038\u0026#34;, border_width=5 )) .add_yaxis(\u0026#34;湖北新增确诊病例\u0026#34;, v1, is_smooth=True, linestyle_opts=opts.LineStyleOpts(width=2, color=\u0026#39;6FA0A7\u0026#39;), label_opts=opts.LabelOpts(position=\u0026#39;bottom\u0026#39;), itemstyle_opts=opts.ItemStyleOpts( color=\u0026#39;#6FA0A7\u0026#39;, border_color=\u0026#34;#6FA0A7\u0026#34;, border_width=3 )) .set_global_opts(title_opts=opts.TitleOpts(title=\u0026#34;\u0026#34;), yaxis_opts=opts.AxisOpts( type_=\u0026#34;log\u0026#34;, name=\u0026#34;y\u0026#34;, splitline_opts=opts.SplitLineOpts(is_show=True), is_scale=True, axisline_opts=opts.AxisLineOpts(is_show=False) ) ) ) c.render(\u0026#39;results/%d%d-新增病例趋势图.html\u0026#39; % (month, day)) return c def draw_map(month, day): labels, counts, title = get_province_status(month, day, None) c = ( Map() .add(\u0026#34;\u0026#34;, [list(z) for z in zip(labels, counts)], \u0026#34;china\u0026#34;) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;新型肺炎全国确诊病例\u0026#34;), visualmap_opts=opts.VisualMapOpts( pieces=[ {\u0026#39;min\u0026#39;: 1000, \u0026#39;color\u0026#39;: \u0026#39;#450704\u0026#39;}, {\u0026#39;max\u0026#39;: 999, \u0026#39;min\u0026#39;: 100, \u0026#39;color\u0026#39;: \u0026#39;#75140B\u0026#39;}, {\u0026#39;max\u0026#39;: 99, \u0026#39;min\u0026#39;: 10, \u0026#39;color\u0026#39;: \u0026#39;#AD2217\u0026#39;}, {\u0026#39;max\u0026#39;: 9, \u0026#39;min\u0026#39;: 1, \u0026#39;color\u0026#39;: \u0026#39;#DE605B\u0026#39;}, {\u0026#39;max\u0026#39;: 0, \u0026#39;color\u0026#39;: \u0026#39;#FFFEE7\u0026#39;}, ], is_piecewise=True ), ) ) c.render(\u0026#39;results/%d%d-疫情地图.html\u0026#39; % (month, day)) if __name__ == \u0026#39;__main__\u0026#39;: m, d = 1, 30 #get_html(m, d) #draw(m, d) #compare(1, 28, 1, 29) #show_province_status(m, d, \u0026#39;云南\u0026#39;) #show_province_status(m, d, None) draw_tendency(m, d) draw_map(m, d) 函数直接都放一个文件里面了，其实进行重构一下代码会更简洁一些\n","permalink":"https://csqread.top/posts/tech/2020-ncov%E7%88%AC%E8%99%AB%E7%BB%9F%E8%AE%A1/","summary":"刚开始我没想到用爬虫，就直接手创了一个字典\u0026hellip; 需要安装模块pyechart and pyechart-china-provinces-pypkg 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25","title":"2020 NCov爬虫统计"},{"content":"《拍电影时我在想的事》是戛纳电影节金棕榈奖得主是枝裕和历史8年，写下的首部自传性随笔集。是枝裕和在书中回顾三十余年的创作生涯，讲述每一部经典作品背后的传奇故事、缘起和理念，记录各个创作时期对电影的创作和思考，以及对世界和人生的看法。 其中不仅汇集了电影大师的哲思与灵光，更讲述了一位导演脚踏实地，从赊账拍片到斩获世界各大电影节奖项的励志旅程。《拍电影时我在想的事》在日本出版后，连续六次紧急加印，得到《朝日新闻》等各大媒体的高度评价，被盛赞到：哪怕再过100年，这本书也一定是作者的圣经，世界如此精彩，日常就很美丽，生命本身就是奇迹。是枝裕和打动世界的理由，都在这本书里。\n关于是枝裕和，大概是我在两年之前接触到的一位导演，那是因为室友推荐的一部电影《无人知晓》，看完之后感觉特别棒，就继续搜罗这他的电影资源，把他的电影大都看了一遍。其中不仅仅是无人知晓让我印象深刻，《如父如子》，《海街日记》，《比海更深》，《奇迹》都挺好看的。我就不一一剧透了，想看的话资源在页面最下面，是枝裕和蓝光合集。 关于他的这本书，看了之后发现其实是枝裕和开始的时候是拍纪录片起家的。那个时候和侯孝贤和杨德昌是同一个时代的导演。甚至是他俩的后来者。记得书中他说过，侯孝贤导演对他在电影上面也有过帮助呢。在开始的时候，是枝裕和使用分镜完成了他的处女作，幻之光，看了他画的分镜，挺有意思的，发现原来电影就是这样开始先是被简单的画出来的啊\n他的第一部电视纪录片的拍摄过程就他别传奇。采访自杀官员的遗孀，对方在一堆采访者中唯独接受是枝裕和的访问，其实是枝裕和也不知道为什么，事后她才道出原委：“你第一天来这里的时候，拘谨地坐在榻榻米上。当时的你，跟我丈夫和我相亲的样子特别像” 我讲述的电影语言，与以电影为母语的创作者所讲述的不同是带着电视口音的方言，也就是说，在语法上是不规范的。对电视的养育之情，我心怀感激，也坦率地承认自己“电视人”的身份。与此同时，我对目前所处的环境感到了某种责任。\n一些笔记： 关于分镜，他还曾被侯孝贤导演指出过： ‘知道被侯孝贤导演指出来，我才意识到自己“被分镜图绑住了手脚”’。 侯导来到日本参加东京电影节，见面时，他对我说：“技术很厉害，但是在拍摄之前，你早就画好了所有的分镜图吧？” “是的，画了，当时的我特别没有自信”我回答。 “不是应该看了演员的表演之后，才确定摄影机的位置吗？你以前是拍记录片的应该知道啊。”\n他对于虚构作品和纪录片的理解： “我一直认为，虚构作品要令观众‘沉醉’，而纪录片”则要让观众清醒。\n“比起有意义的死，不如去发现有意义却丰富的生”作为想法，这是正确的。但是从拍摄的电影来看，与带着这种意识拍摄面成的《花之舞者》相比，将生的实感通过细节表现出来的下一部电影《步履不停》，更明显地体现了这种价值观。\n电影并非空喊口号的东西，它是为了表达生命真实丰富的感受而1存在的。现在我正义这一点为目标而努力\n“在《无人知晓》中，我不想探讨谁对谁错的问题，也不想追究大人应该如何对待孩子，以及围绕孩子的法律应该如何修改等等。所谓的批判，教训和建议都不是我想讲的。我真正想做的是讲述孩子们的日常生活，以及在一旁观察他们，倾听他们的声音。这样一来，孩子们的话语就不再是独白，而是变成了对话。同样孩子们也通过双眼观察着我们”-（这应该是是枝裕和拍纪录片时养成的习惯）\n“我仍然要坚持这样来拍摄《无人知晓》，并非从单纯的黑与白的对立出发，而是从灰色的视角记录世界。没有纯粹的英雄或坏人只是如实的描述我们生活的这个由相对主义价值观构筑的世界”\n“明显的不同在于，在西方人看来，死亡始于生命的终结，也就是说生与死是两个对立的概念。但是在东方人（特别是日本人）看来，生与死是表里一本的，两者的关系甚至有点亲近。死亡未必始于生命的终结，死常常存在于圣的内部。这个观念一直以来都存在于我的思想中”\n“在欧洲，我反复被问到‘为什么您的作品中经常并不出现的死者，为什么不讲述死亡，而是常常讲述死后的世界’，我一直苦于如何回答，当时却不知不觉地说出了这样的答案：日本到某个时期为止，一直都有‘无颜面对祖先’地观念。日本没有绝对权威的神明，但是在日常生活中存在这一种伦理观：应该活得对得起死去的人。我也怀着这样的伦理观。因此在日本文化中的‘死者’代替了西方文化中的‘神’。死去的人并不是就这样离开了世间，而是从外部批判我们的生活，承担着伦理规范的作用1.也就是说，从故事外部批判我们的是死者，而站在故事内部承担这一角色的是孩子” -原来如父如子也是这样的观念啊\n对于死亡，是枝裕和所认为的与村上春树在《挪威的森林》中所写的几乎一模一样：“生常常存在与死的内部”。他们都认为，日本传统文化中一味看重“有意义的死”并非病态文化，没有实实在在生活过的实感，是无法去探讨死亡的意义的。这点和黑泽明在《梦》中所表现的也一样。死去的桃树化为桃树神，从外部批判着我们的生活。日本是一个泛灵的国家，万物皆有灵气，这一点在《千与千寻》，《幽灵公主》等宫崎骏的动漫中也可得以一窥。在《菊与刀》中更是系统的阐述了这一点。\n“游走在网络上的人为u什么普遍是右翼，或者是国家主义者？思考这个问题，会发现与他人缺乏紧密联系的人容易沉迷于网络世界，‘国家’这种概念轻轻松松就会将他们收编，成为他们内心唯一的价值观。在现代日本，所谓的地区共同体已经趋向崩溃，企业共同体随着终身雇佣制度一起消亡，家庭内的关系也越发越远。因此，如果没有可以代替共同体和家庭的事务、场所和价值观，将会有越来越多的人陷入虚幻的国家主义之中” 这么说来来我们人均陷入了虚幻的国家主义，难道因为天朝是家国情怀？\n一些想法： 看完这本书我知道了《无人知晓》是如何拍出来的，《小偷家族》是在表达什么。是枝裕和的电影观念可以这样表达：比起有意义的死，不如去发现无意义却丰富的生。\n电影并非高喊口号的东西，它就是为了表达生命真实丰富的感受而存在的。 有人问是枝裕和，在《无人知晓》中，作为导演你没有对电影中的人物进行道德上的审判，甚至没有指责抛弃孩子的母亲。是枝裕和是这样回答：\n电影不是用来审判人的，电影导演也不是法官。设计一个坏蛋可能会让故事（世界）更易于理解，但是不这样做，反而能让观众将电影中的问题带入日常生活中去思考。 因为观众是要回归日常生活的，每一个观众都要在电影结束后离开电影院。如果一个观众能够看完电影之后，对生活的看法会有所改变，这或许会成为他们带着批判性的视角观察日常生活的契机。\n是的，我喜欢这样的电影哲学，所谓的批判留给其他人，电影只带给观众真实生活的感受。杨德昌说：电影发明之后，人类的生命，比以前延长了三倍。而在观看好莱坞大片时，我们的生活近乎暂停了两个小时。但在观看这种电影时，我们并没有离开生活。所有的电影观众，无一例外的都要在电影散场之后，必须回到他们的日常生活。是枝裕和就这样影响了我们的生活，像他所说 “如果说我的电影中更有共通的东西，那就是无法取代的珍贵之物不在日常生活之外，而是蕴藏在日常生活的细枝末节里”\n是枝裕和合集： 链接：https://pan.baidu.com/s/1dUz8weNnzyc9s3zpKUCO_Q 提取码：janz\n","permalink":"https://csqread.top/posts/read/%E6%88%91%E5%9C%A8%E6%8B%8D%E7%94%B5%E5%BD%B1%E6%97%B6%E6%83%B3%E7%9A%84%E4%BA%8B-%E6%98%AF%E6%9E%9D%E8%A3%95%E5%92%8C/","summary":"《拍电影时我在想的事》是戛纳电影节金棕榈奖得主是枝裕和历史8年，写下的首部自传性随笔集。是枝裕和在书中回顾三十余年的创作生涯，讲述每一部经典","title":"‘我在拍电影时想的事‘ 是枝裕和"},{"content":"上午两节课睡过了，就没去了，吃了个早饭，还有一堆事情没有做，就开始先把昨晚没弄完的python词频分析的代码给写完了。做一个记录吧，防止以后用得着\n工具 工具：python3.7 Vscode wordcloud jieba 等 获取数据源 点击Tim左上角头像 选择安工程失物招领群1，导出消息记录 要用txt导出到任意盘，接下来就是要对导出的文件进行数据分析 下载对应库 到官方网站下载对应包 传送门 重要提醒：通过cmd中输入 Python -V 来查看python版本并下载相应的安装包，同时注意python是32位的还是64位的，我这里是32位的 下载完成后，进入刚刚下载该文件的路径，使用pip3 install wordcloud-1.3.3-cp-37-cp-37-win_amd32-whl命令开始安装， 这样 wordcloud就安装完成了。 接下来还要安装jieba matplotlib scipy 均使用 pip3 install xxx 即可 代码 首先过滤掉txt文件中无用的信息，比如时间，以及聊天的名片，避免词云中都是无效信息，并用jieba进行分词 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import jieba newtext = [] # 打开E盘下的聊天记录文件qq.txt for word in open(\u0026#39;E:\\\\qq.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;): tmp = word[0:4] if (tmp == \u0026#34;2017\u0026#34; or tmp == \u0026#34;====\u0026#34;or tmp == \u0026#34;2018\u0026#34;): # 过滤掉聊天记录的时间和qq名称 continue tmp = word[0:2] if (tmp[0] == \u0026#39;[\u0026#39; or tmp[0] == \u0026#39;/\u0026#39;or tmp[0] == \u0026#39;@\u0026#39;): # 过滤掉图片和表情，例如[图片]，/滑稽 continue newtext.append(word) # 将过滤掉图片和表情和时间信息和qq名称剩下的文字重新写入E盘下的q1.txt文件中去 with open(\u0026#39;E:\\\\q1.txt\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: for i in newtext: f.write(i) # 打开新生成的聊天记录文件 text = open(\u0026#39;E:\\\\q1.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() word_jieba = jieba.cut(text, cut_all=True) word_split = \u0026#34; \u0026#34;.join(word_jieba) 通过这步在E盘中得到了一个q1.txt文件，打开会发现变的整洁干净了许多.\n2.再新建一个.py文件，用到wordcloud库来绘制词云图\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator import matplotlib.pyplot as plt from scipy.misc import imread text = open(\u0026#39;E:\\\\q1.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() # 打开过滤好的txt文件 print(text) bg_pic = imread(\u0026#39;E:\\\\sjt.jpg\u0026#39;) # 导入词云背景 wordcloud = WordCloud(mask=bg_pic, background_color=\u0026#39;white\u0026#39;, scale=1.5, font_path=\u0026#39;C:/Windows/Fonts/simhei.ttf\u0026#39;, width=1000,height=600,stopwords={\u0026#39;表情\u0026#39;,\u0026#39;糊脸\u0026#39;,\u0026#39;拍桌\u0026#39;,\u0026#39;拍头\u0026#39;},min_font_size=10,max_font_size=36,font_step=4, ).generate(text) # 定义词云的各种变量，可以控制词云的形式，这里的控制变量可以去网上查找，stopwords={\u0026#39;表情\u0026#39;,\u0026#39;糊脸\u0026#39;,\u0026#39;拍桌\u0026#39;,\u0026#39;拍头\u0026#39;\u0026#39;是为了过滤掉里面的部分表情信息 image_colors = ImageColorGenerator(bg_pic) plt.imshow(wordcloud) plt.axis(\u0026#39;off\u0026#39;) plt.show() wordcloud.to_file(\u0026#39;E:\\\\text.jpg\u0026#39;) # 输出词云 做出的结果图\n挺好玩的，就多测试了几个群，就这样\n对词频出现次数进行统计，并生成统计表 直接上代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import jieba fr=open(\u0026#39;q1.txt\u0026#39;,\u0026#39;r\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) //打开上面经过分词后的txt文件 s=\u0026#34;\u0026#34; data={} for line in fr: line=line.strip() if len(line)==0: continue if line[0]==\u0026#39;2\u0026#39;: continue for x in range(0,len(line)): if line[x] in [\u0026#39; \u0026#39;,\u0026#39;\\t\u0026#39;,\u0026#39;\\n\u0026#39;,\u0026#39;。\u0026#39;,\u0026#39;，\u0026#39;,\u0026#39;[\u0026#39;, \u0026#39;]\u0026#39;, \u0026#39;（\u0026#39;, \u0026#39;）\u0026#39;, \u0026#39;:\u0026#39;, \u0026#39;-\u0026#39;, \u0026#39;？\u0026#39;, \u0026#39;！\u0026#39;, \u0026#39;《\u0026#39;, \u0026#39;》\u0026#39;, \u0026#39;、\u0026#39;, \u0026#39;；\u0026#39;, \u0026#39;“\u0026#39;, \u0026#39;”\u0026#39;, \u0026#39;……\u0026#39;,\u0026#39;0\u0026#39;,\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;5\u0026#39;,\u0026#39;6\u0026#39;,\u0026#39;7\u0026#39;,\u0026#39;8\u0026#39;,\u0026#39;9\u0026#39;,\u0026#39;=\u0026#39;,\u0026#39;~\u0026#39;,\u0026#39;…\u0026#39;]: //去除一些不希望被统计的东西 continue s+=str(line[x]) print(s) seg_list = jieba.cut(s, cut_all=False, HMM=True) for word in seg_list: if len(word)\u0026gt;=2: if not data.__contains__(word): data[word]=0 data[word]+=1 data=sorted(data.items(),key=lambda d:d[1],reverse=True) fw=open(\u0026#39;安工程失物招领群1词频统计.csv\u0026#39;,\u0026#39;w\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) //生成词频统计表 fw.write(str(len(data))+\u0026#39;\\n\u0026#39;) for item in data: fw.write(item[0]+\u0026#39;,\u0026#39;+str(item[1])+\u0026#39;\\n\u0026#39;) fr.close() fw.close() 这段代码结合上面分词的代码，即可生成统计表，近期来群里关键字出现的字眼一目了然 失物招领群1词频统计 谢谢的次数即代表我们一个月发过的消息总数，因为我们要求每条消息后面必须加个谢谢 由图可见，我们一个月发了1910条消息 其中校园卡类876条 寻物消息585条 失物消息526条 身份证消息41条 等等\n","permalink":"https://csqread.top/posts/tech/python%E5%AF%B9%E5%A4%B1%E7%89%A9%E6%8B%9B%E9%A2%86%E7%BE%A4%E8%BF%9B%E8%A1%8C%E8%AF%8D%E9%A2%91%E5%88%86%E6%9E%90/","summary":"上午两节课睡过了，就没去了，吃了个早饭，还有一堆事情没有做，就开始先把昨晚没弄完的python词频分析的代码给写完了。做一个记录吧，防止以后","title":"Python对失物招领群进行词频分析"},{"content":"“可是为什么要禁掉呢？”野人问道。遇见一位读过莎士比亚的人，使他一时忘了形。 元首耸耸肩膀：“”因为这本书太旧了，这是主要原因。旧东西在我们这儿是毫无用处的。 “即使他们是美好的？” “特别因为他们是美好的。美好便有吸引力了，而我们不要人们被旧的东西吸引。我们要他们喜欢新的。” “可是新的东西确是那么的愚昧和可怕。那些戏剧，空洞无物，只有直升机飞来飞去，而你感受到人家再接吻”他蹙眉颦额，“一群山羊和猴子！”只有《奥赛罗》里的字句才能适切地表达他的轻蔑和憎恨。 “然而是驯养好的好兽呢。”元首低声插嘴。 “你为什么不换成《奥赛罗》给他们看呢？” “我告诉过你了，那个旧了。此外，它们不可能懂得。” 对，这是真话。他记起汉姆赫兹怎样地嘲笑《罗密欧与朱丽叶》。“好吧，那么，”他停顿了一下，“一些像《奥赛罗》地新东西，他们能懂得东西” “那正是我们一直在写的”，汉姆赫兹打破了长时间的沉默说道。 “而那也正是你永远写不出来的。”元首说，“因为，如果那真的像《奥赛罗》，无论怎么新也不会有人懂得。而如果是新的就不可能像《奥赛罗》。” “为什么不可能？” “因为我们的世界不像奥赛罗的世界。没有钢铁你就造不出汽车，同理，没有不安定的社会你就造不出悲剧。今天的世界是安定的。人们很快乐，他们要什么就会得到什么，而他们永远不会要他们得不到的。”\n后面的这些对话确实发人深省\n2503年，一个婴儿养育室里。护士们在地板上摆了一堆图书和鲜花，然后把一群长得一摸一样的、8个月大的婴儿放到了地板上。婴儿们看到图书和鲜花，飞快地爬过去，拿起来玩耍。这时候，长官一声令下，护士长启动电路装置，一时间，刺耳的警报响起，地板被通上了电，触电的婴儿们在痛苦中痉挛并尖叫不已。过了一会儿，护士长关上了电闸。 “这样的试验大约重复200次左右，”长官微笑着对参观者说：“这些孩子们就会对图书和花朵形成本能的憎恨，他们的条件反射就这样被限定了。” “限定”，大约是《Brave New World》一书中的最关键词汇。在Aldous Huxley笔下的那个美好盛世里，人从受精开始就被“限定”了。精子和卵子在试管里被调制好，不健康的胚胎被“限定”出局，健康胎儿在孵化器里大。然后从婴儿养育室开始，孩子们一路被“限定”得厌恶书籍和自然、厌恶独处、厌恶家庭、厌恶宗教和艺术，同时被“限定”得热爱集体、热爱消费、热爱滥交。 当然，并不是所有的人被限定的方式都一样。美好新世界里，人类被分成了五级，Alpha、Beta、Gamma、Delta以及Epsilon——Alpha被限定得聪明漂亮，而Gamma以下的人不但被限定得矮小愚钝，还批量生产。不过 没关系，虽然在那个世界里人有等级贵贱，但是他们都一样幸福——因为无论哪个等级，其接受的“睡梦教育”都会告诉他，他所在的等级最美好最幸运。 这样的世界，有什么问题吗？ 美好新世界的首长Mustapha，问质疑者“野人”John。 有什么人类跋山涉水追求了几千年的东西，新世界里没有呢？经济发展？新世界里如此富足，上至Alphas下至Epsilons，人们不愁吃穿。健康？生物学家们早就把人类限定得不再有疾病。青春？这里人们青春永驻，直到突然死亡。美女帅哥的青睐？这个更不用担心，因为新世界里“每个人都属于他人”，滥交是最大的美德，你要是长期只跟一个美女上床，会成为该世界里骇人的丑闻。 不错，这个世界里没有艺术、诗歌、撕心裂肺的爱情、没有毕加索或者莎士比亚，但是，当你每天都幸福得晕眩时，为什么还会需要毕加索或者莎士比亚？文学艺术往往是为了表达冲突超越痛苦，那么，在一个冲突和痛苦根本不存在的世界里，文学艺术也就变成了社会的阑尾。更不要说“爱情”，那简直是高速公路上突然蹦出来的一头羚羊，如此危险，通通地，限定了之。 所以，这样的世界，有什么问题吗？ 柏拉图估计不会觉得有什么问题，因为新世界里政治家和科学家就是智慧非凡的哲学王。老子估计也不会觉得有什么问题，“劳心者治人，劳力者治于人”在这个桃花源里被充分实施。希特勒更是会欣喜若狂，因为将人类的未来当作一个巨大的生物工程来建设，简直是他的毕生追求。还有斯大林，荡漾在新世界人们脸上的微笑，与沉浸在丰收喜悦里的社会主义农民如出一辙，而新世界的“睡梦教育”，简直可以说是对苏式灌输教育赤裸裸的抄袭。所有那些信奉“精英治国”、信奉“稳定高于一切”、信奉“老百姓无非就是关心吃饱穿暖”的人，都会是“美好新世界”的热情粉丝。 这个新世界如此美好，它只有一个小小的缺陷——在那里，幸福的人们全都是“被幸福”的。 就是说，在那里，人们的幸福是政治家和科学家呕心沥血的科研成果，与每个个体自己的创造力、情感体验能力、审美能力都毫无关系。 民众只需像儿童那样，系上围兜，张口吞下哲学王或者先锋队一勺一勺送过来的食物，就乘坐直升电梯抵达了极乐世界。而精英们为了民众，制作食物既考虑营养，又考虑消化，可以说是殚精竭虑。有如此鞠躬尽瘁的统治者，民众的个体自由意志完全是多此一举。 如果说奥威尔的《1984》里，人们为失去自由而痛苦，那么Huxley的《勇敢新世界》里，人们则为摆脱了自由的重负而狂喜。真的，如果政治家科学家给民众带来如此丰盛的快乐，民众何必要自己去斗争？就像如果你可以从父亲那里继承一大笔遗产，何必要自己去辛苦挣钱？除非—— 你认为得到的过程比得到本身更有意义。除非你不识抬举地认为，通过个体努力去争取幸福比“被幸福”更体现生命的价值。 也正是在这个意义上，我在一切精英治国观里读到的是对生命的藐视。当统治者的恩赐被视为民众幸福的源泉时，统治者越高大，民众就越渺小。对有些人来说，幸福如此简单，无非是对着送过来的汤勺不断张嘴，而对另一些人来说，它如此复杂，需要汗滴禾下土粒粒皆辛苦。由于运气和能力，也许耕耘未必能带来收获，但是恩赐来的幸福和捕猎来的痛苦之间，你选什么呢？在幸福药丸soma和跌宕起伏的莎士比亚之间，野人John选择了莎士比亚。但是当然，对于美好新世界里的绝大多数人，这根本不是一个问题。他们从来没有选择的权利，无处不在的幸福不由分说，一把把他们给罩住，他们只能躺在幸福的牙缝里，被咀嚼，然后变成一堆残渣，被气势磅礴地给吐出来。\n","permalink":"https://csqread.top/posts/read/%E7%BE%8E%E4%B8%BD%E6%96%B0%E4%B8%96%E7%95%8C%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","summary":"“可是为什么要禁掉呢？”野人问道。遇见一位读过莎士比亚的人，使他一时忘了形。 元首耸耸肩膀：“”因为这本书太旧了，这是主要原因。旧东西在我们这","title":"《美丽新世界》读书笔记"},{"content":"一直以来，我在想做一个qq机器人，能够实现qq消息的转发，因为对于失物招领工作，其他的都挺满意的，但每天发那么多消息确实很浪费时间，所以就想做一个能够实现自动转发消息的机器人。对于这个需求如何实现，我也构思了很久。这个想法是大二下学期就有的，但一直没有去实现。暑假的时候在家中自学了前端，准备做好一个失物招领网站，但那又是一个半吊子项目，网站并没有完全达到我想要的那种结果。现在来做一个博客，来详细说明我的机器人是如何实现的：\n在gooogle的时候，发现了一个很好的框架，一位github大佬写的qqbot框架。于是乎我们只需要调用他的api，以及他的框架来实现我们所想要的需求就可以了。当然，这是需要用python来实现的，所以需要会一些python的基础语法。\n下面介绍github这位大佬的qqbot框架：\n一、介绍 qqbot 是一个用 python 实现的、基于腾讯 SmartQQ 协议的 QQ 机器人，可运行在 Linux, Windows 和 Mac OSX 平台下。 本项目 github 地址： https://github.com/pandolia/qqbot\n你可以通过扩展 qqbot 来实现：\n监控、收集 QQ 消息 自动消息推送 聊天机器人 通过 QQ 远程控制你的设备\n二、安装方法 在 Python 2.7/3.4+ 下使用，用 pip 安装： pip install qqbot 或者下载 源码 解压后 cd 到该目录并运行： pip install.\n三、使用方法 1. 启动 QQBot 在命令行输入： qqbot ，即可启动一个 QQBot 。 启动过程中会自动弹出二维码图片，需要用手机 QQ 客户端扫码并授权登录。启动成功后，会将本次登录信息保存到本地文件中，下次启动时，可以输入： qqbot -q qq号码 ，先尝试从本地文件中恢复登录信息（不需要手动扫码），只有恢复不成功或登录信息已过期时才会需要手动扫码登录。一般来说，保存的登录信息将在 2 天之后过期。\n注意： Linux 下，需要系统中有 gvfs-open 或者 shotwell 命令才能自动弹出二维码图片（一般安装有 GNOME 虚拟文件系统 gvfs 的系统中都会含这两个命令之一）。 Windows10 下，需要系统中已设置了 png 图片文件的默认打开程序才能自动弹出二维码图片。\n若系统无法自动弹出二维码图片，可以手动打开图片文件进行扫码，也可以将二维码显示模式设置为 邮箱模式 、 服务器模式 或 文本模式 进行扫码，详见本文档的第七节。\n操作 QQBot QQBot 启动后，在另一个控制台窗口使用 qq 命令操作 QQBot ，目前提供以下命令： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 1） 帮助、停机和重启命令 qq help|stop|restart|fresh-restart 2） 联系人查询、搜索命令 qq list buddy|group|discuss [$cinfo|$clike] ( $cinfo --\u0026gt; $qq|$name|$key=$val ) ( $clike --\u0026gt; :like:$qq|:like:$name|$key:like:$name ) qq list group-member|discuss-member $oinfo|$olike [$cinfo|$clike] ( $oinfo --\u0026gt; $oqq|$oname|$okey=$oval ) ( $cinfo --\u0026gt; $qq|$name|$key=$val ) ( $olike --\u0026gt; :like:$oqq|:like:$oname|$okey:like:$oname ) ( $clike --\u0026gt; :like:$qq|:like:$name|$key:like:$name ) 3） 联系人更新命令 qq update buddy|group|discuss qq update group-member|discuss-member $ginfo 4） 消息发送命令 qq send buddy|group|discuss $rinfo $message 5） 加载/卸载/显示插件 qq plug/unplug myplugin qq plugins list 命令提供强大的联系人查询和搜索功能，用法示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 列出所有好友 qq list buddy # 列出 名称 为 xxx 的群 qq list group xxx # 列出备注名为 jack 的好友 qq list buddy mark=jack # 列出 群“456班” 的所有成员 qq list group-member 456班 # 列出 群“456班” 中名片为 “mike” 的成员 qq list group-member 456班 card=mike # 列出 讨论组“XX小组” 中名为 jack 的好友 qq list discuss-member XX小组 jack 其中第三、四个参数如果是 key=val 的格式，则应为 name=xx|nick=xx|mark=xx|card=xx|qq=xx 的格式，如果不是 key=val 的格式，则按以下原则进行处理：若是一串数字，则按 QQ 号进行查询，否则，按名称进行查询。\n如果存在重名现象，会列出所有重名的联系人。如：\n1 qq list group 机器人测试 将列出所有名为 “机器人测试” 的群。\n如果在 list 命令的第三、四个参数中加入 “:like:” ，则会按部分匹配的模式进行搜索，用法示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 列出名称中含有 “李” 的好友 qq list buddy :like:李 # 列出 QQ 中含有 “234” 的群 qq list group :like:234 # 列出备注名中含有 jack 的好友 qq list buddy mark:like:jack # 列出 群“456班” 的中名称中含有 “李” 的成员 qq list group-member 456班 :like:李 # 列出 群“456班” 中名片中含有 “mike” 的成员 qq list group-member 456班 card:like:mike # 列出的 讨论组“xx小组” 中名为 jack 的好友 qq list discuss-member :like:小组 jack 从 v2.2.5 版开始， list 命令采用表格的形式输出联系人列表，其输出样式示例如下：\n为保证表格在终端中的显示效果，建议将终端的输出字体设置为 consolas 、且每行可打印的最大字符数大于 120 。另外需要注意：为保证表格的显示效果，当联系人的名称、名片等属性的长度太长或含有特殊字符时，将对这些属性进行截断或过滤后再输出至终端。\nupdate 命令更新指定的联系人列表，其参数含义和 list 命令相同，如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 更新好友列表 qq update buddy # 更新群列表 qq update group # 更新 群“456班” 的成员列表 qq update group-member 456班 send 命令中第三个参数和 list 命令中的第三个参数格式一致。要注意，如果有重名现象，会给所有重名的联系人发信息。 另外要注意，第二个参数只能是 buddy/group/discuss ，不能是 group-member/discuss-member 。示例： # 给 好友“jack” 发消息 “你好” qq send buddy jack 你好 # 给 群“198班” 发消息 “大家好” qq send group 198班 大家好 # 给 QQ 为 12345 的好友发消息 qq send buddy 12345 xxx # 给讨论组发消息 qq send discuss MyDiscuss hello 可以在消息内容中嵌入“/可爱”等表情关键词来向对方发送表情，详见 facemap.py。还可以在消息内容中使用 \\n,\\t 这两个转义字符（如： send buddy jack 第一行\\n第二行）。\n以上所有命令都提供对应的 HTTP API 接口，供 web 前端开发者调用，接口的 url 地址为 http://127.0.0.1:8188/{command} ，只需要将 qq 后面的命令各参数用 \u0026ldquo;/\u0026rdquo; 分隔开替换 url 中的 command 就可以了，如: http://127.0.0.1:8188/send/buddy/jack/hello ，其他示例详见 urltestbot.md 。注意：如果命令中含有中文或特殊字符，需要先进行 url 编码（ utf8 ），例如，调用 http://127.0.0.1:8188/send/buddy/jack/nihao%20%E4%BD%A0%E5%A5%BD%20wohao 将发送消息 ”nihao 你好 wohao“ 。（提示：在 JavaScript 中，可以使用 encodeURIComponent 函数进行编码）。\n另外， QQBot 启动后，用本 QQ 号在其他客户端（如：手机 QQ ）上向某个 群/讨论组 发消息 “\u0026ndash;version” ，则 QQBot 会自动在该 群/讨论组 回复： “QQBot-v2.x.x” 。\n四、实现你自己的 QQ 机器人 实现自己的 QQ 机器人非常简单，只需要定义一个自己的消息响应函数并按插件加载。示例代码： ``` # -*- coding: utf-8 -*- def onQQMessage(bot, contact, member, content): if content == \u0026lsquo;-hello\u0026rsquo;: bot.SendTo(contact, \u0026lsquo;你好，我是QQ机器人\u0026rsquo;) elif content == \u0026lsquo;-stop\u0026rsquo;: bot.SendTo(contact, \u0026lsquo;QQ机器人已关闭\u0026rsquo;) bot.Stop()\n1 2 3 4 5 6 7 8 9 注意，上面注册的响应函数的函数名必须为 “onQQMessage” ，函数参数也必须和上面的一致。 将以上代码另存为 sample.py （注意保存为 utf8 编码的文件）。放到 ~/.qqbot-tmp/plugins/ 目录下（ ~ 代表用户主目录， win7 下为 C:\\Users\\xxx ），或系统中可以 import 到的目录下（如 python 的安装目录下的 Lib/site-packages 目录）。 之后，保持前面的 qqbot 进程运行，在另一个控制台输入 qq plug sample ，则可将此文件中的 onQQMessage 函数注册到 QQBot 的相应事件上去。此时，用另外一个 QQ 向本 QQ 发送消息 “-hello”，则会自动回复 “你好，我是 QQ 机器人”，发送消息 “-stop” 则会关闭 QQ 机器人。 在控制台输入 qq unplug sample 可以卸载此插件及相应的回调函数。可以同时加载多个插件，此时各插件中的相应函数会依次被调用（但调用顺序和加载次序无关）。 QQBot 开始运行后，每收到一条 QQ 消息，会将消息来源、消息内容以及一个 QQBot 对象传递给已注册的消息响应函数。其中： bot : QQBot 对象，提供 List/SendTo/Stop/Restart 等接口，详见本文档第五节 contact : QContact 对象，消息的发送者，具有 ctype/qq/uin/nick/mark/card/name 等属性 member : QContact 对象，仅当本消息为 群消息或讨论组消息 时有效，代表实际发消息的成员 content : str 对象，消息内容 contact 代表消息发送者，其 ctype 属性可以为 buddy/group/discuss ，代表 好友/群/讨论组 对象，表示本消息是 好友消息/群消息/讨论组消息 。\n1 2 3 4 5 6 7 8 9 10 11 12 member 仅当本消息为 群消息或讨论组消息 时有效，代表实际发消息的成员，它的 ctype 属性可以为 group-member/discuss-member ，代表 群成员/讨论组成员 对象。当本消息为 好友消息 时， member 等于 None 。 contact 和 member 都是 QContact 对象，不同类型的 QContact 对象所具有的属性含义见： qcontact-attr 。注意所有 QContact 对象都是 只读对象 ，只能读取它的属性，不能设置它的属性，也不能向它添加额外的属性。 可以调用 QQBot 对象的 SendTo 接口向 QContact 对象发送消息，但要注意：只可以向 好友/群/讨论组 发消息， 不可以向 群成员/讨论组成员 发送消息 。也就是说，只可以调用 bot.SendTo(contact, \u0026#39;xxx\u0026#39;) ， 不可以调用 bot.SendTo(member, \u0026#39;xxx\u0026#39;) 。 \u0026lt;h1\u0026gt;五、 QQBot 对象的公开接口和属性\u0026lt;/h1\u0026gt; QQBot 对象提供 List/Update/SendTo/Plug/Unplug/Login/Stop/Restart/FreshRestart 共计 9 个公开接口，这些接口的第一个字母都是大写的。另外，提供一个公开属性 conf 保存全局的配置信息。 一般情况下，请勿 调用/存取 此对象的其他 方法/属性 。特别的， 请勿在子线程中调用这些接口 。 以下介绍前 7 个接口和 conf 属性。 如果需要在 IDE 或 python-shell 中运行或测试以上接口，需要先关闭 qqbot 进程，并在 IDE 或 python-shell 中运行以下代码进行登录： from qqbot import _bot as bot bot.Login([\u0026rsquo;-q\u0026rsquo;, \u0026lsquo;1234\u0026rsquo;])\n1 2 3 4 5 6 （1） bot.List(tinfo, [cinfo]) --\u0026gt; [contact0, contact1, ..., ]/[]/None 对应本文档第三节的 list 命令。返回联系人对象（ QContact 对象）列表或者 None 。第一个参数 tinfo 是联系人列表的代号，第二个参数是可选的（和 list 命令的第三个参数格式一致）。 参数 tinfo 用来代表某个联系人列表，该参数在联系人的查询中非常重要，请务必理解以下两种情况 ： tinfo 的含义（情况1）： tinfo 可以为 buddy/group/discuss ，分别代表 好友列表/群列表/讨论组列表 。示例： 返回 好友列表： bot.List(\u0026lsquo;buddy\u0026rsquo;)\n返回名为 \u0026lsquo;jack\u0026rsquo; 的好友的列表： bot.List(\u0026lsquo;buddy\u0026rsquo;, \u0026lsquo;jack\u0026rsquo;)\n返回 群列表： bot.List(\u0026lsquo;group\u0026rsquo;)\n返回名为 “机器人测试” 的群的列表： bot.List(\u0026lsquo;group\u0026rsquo;, \u0026lsquo;机器人测试\u0026rsquo;)\n1 tinfo 的含义（情况2）： tinfo 也可以是一个 ctype 等于 group/discuss 的 QContact 对象，代表该 群/讨论组 的成员列表。如以下第二句和第三句分别返回 群“456班” 的成员列表和该群中名片为 “jack” 的成员列表： g = bot.List(\u0026lsquo;group\u0026rsquo;, \u0026ldquo;456班\u0026rdquo;)[0] # g 是一个 Group 对象（群“456班”） bot.List(g) # 返回 群“456班” 的成员列表 bot.List(g, \u0026lsquo;card=jack\u0026rsquo;) # 返回 群“456班” 中名片为 “jack” 的成员列表 注意上面第三句不允许是 bot.List(g, card=\u0026lsquo;jack\u0026rsquo;) 的格式。\n1 2 3 4 5 6 7 List 接口的内部执行顺序： 首先在 QQBot 的联系人数据库内查找 tinfo 所代表的联系人列表；若数据库内已有此列表，则在此列表内进行搜索，并返回一个包含 “此列表中所有和 cinfo 匹配的联系人” 的列表；若数据库内没有此列表，则向 QQ 服务器请求数据获取联系人列表，获取成功后将联系人列表保存到数据库内，然后再进行搜索并返回一个包含 “此列表中所有和 cinfo 匹配的联系人” 的列表；如果在向 QQ 服务器请求数据的过程中出错了，则打印相关的失败信息，并返回 None 。 List 接口返回值的含义： 返回一个非空列表表示 tinfo 所指定的联系人列表内所有和 cinfo 匹配的联系人；返回一个空列表表示该联系人列表内没有和 cinfo 匹配的联系人；返回 None 表示向 QQ 服务器请求联系人列表和资料失败，不知道是否有相匹配的联系人。 调用 List 接口后， 务必 先根据以上三种情况对返回值进行判断，然后再执行后续代码。 注意： 当 List 接口返回非空列表时，列表内的元素是 QContact 对象，而不是 str 对象： g = bot.List(\u0026lsquo;group\u0026rsquo;)[0] # g 是一个 Group 对象 print([g, type(g), g.qq, g.name, g.uin, g.mark])\t# 打印 g 的各项属性\n1 2 3 4 5 6 7 8 不同类型的 QContact 对象所具有的属性含义见： qcontact-attr 。 （2） bot.Update(tinfo) --\u0026gt; True/False Update 接口的参数 tinfo 和 List 接口中的参数含义相同，调用此接口会立即向 QQ 服务器请求相应的联系人列表并更新联系人数据库，并一直阻塞至更新成功。更新最慢的是好友列表，若好友较多可能会阻塞 5 ~ 10 秒。成员列表更新的较快，即便是 2000 人的大群，更新时间仅 1 ~ 2 秒。 若更新成功，返回 True ，否则，返回 False 。 示例： 更新 好友列表 ： bot.Update(\u0026lsquo;buddy\u0026rsquo;)\n更新 群列表 ： bot.Update(\u0026lsquo;group\u0026rsquo;)\n更新 某个群的成员列表 ： gl = bot.List(\u0026lsquo;group\u0026rsquo;, \u0026ldquo;456班\u0026rdquo;) if gl: g = gl[0] bot.Update(g)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 （3） bot.SendTo(contact, content, resendOn1202=True) --\u0026gt; \u0026#39;向 xx 发消息成功\u0026#39;/\u0026#39;错误：...\u0026#39; 向联系人发送消息。第一个参数为 QContact 对象，第二个参数为消息内容。再次提醒： 只可以向 好友/群/讨论组 发消息， 不允许向 群成员/讨论组成员 发消息 。 可以在消息内容中嵌入“/微笑”等表情关键词来向对方发送表情，详见 facemap.py 。 若发送成功，返回字符串（向 xx 发消息成功）。否则，返回含错误原因的字符串（错误：...）。 发消息时可能会重复发消息，这是因为 QQ 服务器返回代码 1202 的原因。v2.1.17版已针对此问题在 bot.SendTo 接口中增加了一个参数： resendOn1202 ，若此参数为 True （默认值），则发消息时如果 QQ 服务器返回代码 1202 （表明发消息可能失败），还会继续发送 3 次，直至返回代码 0 ， 若此参数为 False ，则不会尝试重发。 设为 True 在绝大部分情况下能保证消息一定能发出去，但缺点是有时一条消息会重复发送。设为 False 则相反，消息不会重复发送，但有时消息发送不出去。 总之因为这个 1202 代码的不确定性，没有完美的解决办法。请根据各自的实际情况选择 resendOn1202 的值。 第一个参数 contact 必须是通过 bot.List 返回的 QContact 对象、或回调函数 onQQMessage 传递进来的第一个参数。示例： 向 昵称 为 jack 的好友发消息 bl = bot.List(\u0026lsquo;buddy\u0026rsquo;, \u0026lsquo;jack\u0026rsquo;) if bl: b = bl[0] bot.SendTo(b, \u0026lsquo;hello\u0026rsquo;)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 （4） bot.conf bot.conf 中保存全局的配置信息，各项配置详见本文档第七节。如 bot.conf.termServerPort 保存 QQBot 命令行服务器的端口号， bot.conf.qq 保存本次登录的 QQ 号码。 注意： bot.conf 中保存的配置信息是只读的，请勿修改这些配置信息。 \u0026lt;h1\u0026gt;六、 注册回调函数、被他人 @ 的通知、判断是否是自己发的消息、定制定时任务\u0026lt;/h1\u0026gt; 注册回调函数 除了上面提到的 onQQMessage 响应函数，还可以注册 onInit/onQrcode/onStartupComplete/onInterval/onUpdate/onPlug/onUnplug/onExit 共计九种事件的回调函数，所有事件的回调函数参数格式、含义及示例详见 sampleslots.py 。 程序的运行流程以及各回调函数的调用时机如下： \u0026lt;img src=\u0026#34;define-my-qqbot/main.png\u0026#34;\u0026gt; 再次提醒：注册的回调函数的函数名以及函数参数（数量和名称）都不得更改 。 被群内其他成员 @ 的通知 QQBot 收到群消息时，会先根据消息内容判断是否有人 @ 自己。如果是，则在消息内容的开头加一个 [@ME] 的标记，再传递给 onQQMessage 函数；否则，将消息内容中的所有 @ME 替换成 @Me 再传给 onQQMessage 。因此，在 onQQMessage 函数内，只需要判断 content 内是否含有 @ME 就知道自己是否被消息发送者 @ 了。例如： def onQQMessage(bot, contact, member, content): if \u0026lsquo;@ME\u0026rsquo; in content: bot.SendTo(contact, member.name+\u0026rsquo;，艾特我干嘛呢？\u0026rsquo;)\n1 2 3 4 请注意，若群内有另一个成员的名字和自己的名字的开头部分相同（如：自己的名字是 ab ，另一个成员的名字是 abc ），那么当有人 @abc 时，也会误报成 @ME ，在这种情况下，需要修改自己的群名片，以免误报。 判断是否是自己发的消息 当本 QQ 发消息时， QQBot 也会收到一条同样的消息， bot 对象提供一个 isMe 方法来判断是否是自己发的消息： def onQQMessage(bot, contact, member, content): if bot.isMe(contact, member): print(\u0026lsquo;This is me\u0026rsquo;)\n1 2 定制定时任务 从 2.1.13 起， qqbot 提供一个功能强大的函数装饰器 -- qqbotsched 来定制定时任务，示例代码： from qqbot import qqbotsched\n@qqbotsched(hour=\u0026lsquo;11,17\u0026rsquo;, minute=\u0026lsquo;55\u0026rsquo;) def mytask(bot): gl = bot.List(\u0026lsquo;group\u0026rsquo;, \u0026lsquo;456班\u0026rsquo;) if gl is not None: for group in gl: bot.SendTo(group, \u0026lsquo;同志们：开饭啦啦啦啦啦啦！！！\u0026rsquo;)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 以上代码以插件形式加载后，每到 11:55 和 17:55 ，都会自动向 群“456班” 发送消息：“同志们：开饭啦啦啦啦啦啦！！！” 。 qqbotsched 装饰器接受 year, month, day, week, day_of_week, hour, minute, second, start_date, end_date, timezone 共计 11 个关键字参数，每个参数表示任务的定制时间的分量所应匹配的值。例如： hour=\u0026#39;11,17\u0026#39; 表示应在 11:xx 或 17:xx 执行任务， minute=\u0026#39;55\u0026#39; 表示应在 xx:55 执行任务， minute=\u0026#39;0-55/5\u0026#39; 表示应在 xx:00, xx:05, xx:10, ..., xx:55 执行任务， day_of_week=\u0026#39;mon-fri\u0026#39; （或 \u0026#39;0-4\u0026#39; ） 表示应在 星期一 ~ 星期五 执行任务。 qqbotsched 是对 Python 的定时任务框架 apscheduler 的简单封装，其各项参数应采用 Unix 系统中的 crontab 格式输入。有关 crontab 以及 Python 的定时任务框架 apscheduler 的内容可参见以下参考资料： https://code.tutsplus.com/tutorials/scheduling-tasks-with-cron-jobs--net-8800/ http://apscheduler.readthedocs.io/en/latest/userguide.html https://lz5z.com/Python定时任务的实现方式/ http://debugo.com/apscheduler/ crontab 各项参数格式说明详见： http://apscheduler.readthedocs.io/en/latest/modules/triggers/cron.html 注册回调函数和定制定时任务的注意事项 注册回调函数和定制定时任务是对 QQBot 进行扩展的唯一方式，在编写这些函数时，请注意以下事项： 回调函数的函数名、参数名、参数数量、参数顺序都不得更改 定时任务的函数名可以自己定义，但参数有且只有一个，参数名必须为 bot ，为一个 QQBot 对象。 所有回调函数和定时任务都将在主线程中被依次调用，因此不必担心全局变量的线程安全问题。 回调函数和定时任务的运行时间应尽量短，尽量不要再这些函数中进行阻塞式的操作，否则会阻塞整个程序的运行。一般来说，每个函数的运行时间在 5 秒以内是可以接受的。 绝对不要 在回调函数、定时任务或 qqbot 主线程的内部调用 os.system 执行 本 QQ 号对应的 qq 命令 （ 如 os.system(\u0026#39;qq send buddy jack hello\u0026#39;) ）或请求 本 QQ 号对应的 HTTP-API 接口 ，否则整个程序会形成死锁（因为 os.system 要等 qq 命令执行完成后才返回、而 qq 命令要等 os.system 返回后才会被执行）。请直接使用 bot 的 SendTo/List 等接口。 \u0026lt;h1\u0026gt;七、二维码管理器、QQBot 配置、命令行参数以及工作目录\u0026lt;/h1\u0026gt; 二维码的显示模式 WebQQ 登录时需要用手机 QQ 扫描二维码图片，在 QQBot 中，二维码图片可以通过以下四种模式显示： GUI模式： 在 GUI 界面中自动弹出二维码图片 邮箱模式： 将二维码图片发送到指定的邮箱 服务器模式： 在一个 HTTP 服务器中显示二维码图片 文本模式： 在 Term 中以文本形式展示二维码(需要自行安装 pillow 和 wcwidth 库) GUI 模式是默认的模式，只适用于个人电脑。邮箱模式可以适用于个人电脑和远程服务器。服务器模式一般只在有公网 ip 的系统中使用。如果使用 QQ 邮箱来接收二维码，则发送二维码图片之后，手机 QQ 客户端会立即收到通知，在手机 QQ 客户端上打开邮件，再长按二维码就可以扫描了。文本模式方便在开发过程或者服务器部署时使用，为开发者提供快捷方式登陆 QQ 。 注意：当开启了 邮箱模式/服务器模式/文本模式 时， GUI 模式是关闭的，登陆时不会自动弹出二维码图片。 每次登录时会创建一个二维码管理器 （ QrcodeManager 对象） ，二维码管理器会根据配置文件及命令行参数来选择二维码图片的显示方式。 配置文件的使用方法 配置文件为 ~/.qqbot-tmp/v2.x.conf （ ~ 代表用户主目录， win7 下为 C:\\Users\\xxx ， linux 下为 /home/xxx ），第一次运行 QQBot 后就会自动创建这个配置文件，其中内容如下： {\n# QQBot 的配置文件 # 使用 qqbot -u somebody 启动程序时，依次加载： # 根配置 -\u0026gt; 默认配置 -\u0026gt; 用户 somebody 的配置 -\u0026gt; 命令行参数配置 # 使用 qqbot 启动程序时，依次加载： # 根配置 -\u0026gt; 默认配置 -\u0026gt; 命令行参数配置 # 用户 somebody 的配置 \u0026quot;somebody\u0026quot; : { # QQBot-term （HTTP-API） 服务器端口号（该服务器监听 IP 为 127.0.0.1 ） # 设置为 0 则不会开启本服务器（此时 qq 命令和 HTTP-API 接口都无法使用）。 \u0026quot;termServerPort\u0026quot; : 8188, # 二维码 http 服务器 ip，请设置为公网 ip 或空字符串 \u0026quot;httpServerIP\u0026quot; : \u0026quot;\u0026quot;, # 二维码 http 服务器端口号 \u0026quot;httpServerPort\u0026quot; : 8189, # 自动登录的 QQ 号 \u0026quot;qq\u0026quot; : \u0026quot;3497303033\u0026quot;, # 接收二维码图片的邮箱账号 \u0026quot;mailAccount\u0026quot; : \u0026quot;3497303033@qq.com\u0026quot;, # 该邮箱的 IMAP/SMTP 服务授权码 \u0026quot;mailAuthCode\u0026quot; : \u0026quot;feregfgftrasdsew\u0026quot;, # 是否以文本模式显示二维码 \u0026quot;cmdQrcode\u0026quot; : False, # 显示/关闭调试信息 \u0026quot;debug\u0026quot; : False, # QQBot 掉线后自动重启 \u0026quot;restartOnOffline\u0026quot; : False, # 在后台运行 qqbot ( daemon 模式) \u0026quot;daemon\u0026quot;: False, # 完成全部联系人列表获取之后才启动 QQBot \u0026quot;startAfterFetch\u0026quot; : False, # 插件目录 \u0026quot;pluginPath\u0026quot; : \u0026quot;.\u0026quot;, # 启动时需加载的插件 \u0026quot;plugins\u0026quot; : [], # 插件的配置（由用户自定义） \u0026quot;pluginsConf\u0026quot; : {}, }, # 可以在 默认配置 中配置所有用户都通用的设置 \u0026quot;默认配置\u0026quot; : { \u0026quot;qq\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;pluginPath\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;plugins\u0026quot; : [ 'qqbot.plugins.sampleslots', 'qqbot.plugins.schedrestart', ], \u0026quot;pluginsConf\u0026quot; : { 'qqbot.plugins.schedrestart': '8:00', } }, # # 注意：根配置是固定的，用户无法修改（在本文件中修改根配置不会生效） # \u0026quot;根配置\u0026quot; : { # \u0026quot;termServerPort\u0026quot; : 8188, # \u0026quot;httpServerIP\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;httpServerPort\u0026quot; : 8189, # \u0026quot;qq\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;mailAccount\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;mailAuthCode\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;cmdQrcode\u0026quot; : False, # \u0026quot;debug\u0026quot; : False, # \u0026quot;restartOnOffline\u0026quot; : False, # \u0026quot;daemon\u0026quot; : False, # \u0026quot;startAfterFetch\u0026quot; : False, # \u0026quot;pluginPath\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;plugins\u0026quot; : [], # \u0026quot;pluginsConf\u0026quot; : {} # }, }\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 可以在配置文件中添加自己的用户配置（即在该文件的字典中新增一个 item ，此 item 的 key 就代表一个用户），例如，该文件中已有的 somebody 项目就代表名为 somebody 的用户，运行 QQBot 时，输入 qqbot -u somebody ，则会加载 somebody 项目下的各项配置。 下面介绍配置文件中各项配置的功能，以下内容均假定已修改了 somebody 下的配置，且以 qqbot -u somebody 的方式运行。 邮箱模式的配置（ mailAccount 和 mailAuthCode ） 如果需要使用邮箱模式显示二维码，可以将 mailAccount 和 mailAuthCode 项中分别设置为邮箱帐号和授权码，运行后，二维码管理器会将二维码图片发送至该邮箱。 注意：授权码不是邮箱的登录密码，而是邮箱服务商提供的开通 IMAP/SMTP 服务的授权码（提醒：不是 POP3/SMTP 服务）， QQ/网易 邮箱可以在网页版的邮箱设置里面开通此项服务，并得到授权码。如果只定义了 mailAccount 而没定义 mailAuthCode ，则程序运行的开始时会要求手工输入此授权码。 邮箱模式已在 QQ 、 网易 和 Google 邮箱中测试过。 服务器模式的配置（ httpServerIP 和 httpServerPort ） 如果需要使用服务器模式，可以配置 httpServerIP 和 httpServerPort 项，一般来说应该设置为公网 ip 。服务器模式开启后，可以通过 http://{httpServerIP}:{httpServerPort}/{any} 来访问二维码图片。其中 {any} 可以是任何非空的数字或字母串。 当邮箱模式和服务器模式同时开启时，发邮件时不会发送真正的图片，只会将图片地址发到邮箱中去，而且只发送一次，二维码过期时刷新一下邮件就可以了。如果只开启邮箱模式，则发邮件时会发送真正的图片，当二维码过期时，需要将邮件设置为已读（用手机 QQ 点开邮件后该邮件就是已读了），之后才会发送最新的二维码图片。 文本模式显示二维码（cmdQrcode） 若 cmdQrcode 项设置为 True ，则会在 term 中以文本模式显示二维码。注意：要使用文本模式，需要自行安装 pillow 和 wcwidth 库，可使用 pip 安装。 自动登录的 QQ 号码（ qq ） 配置文件中每个用户都有 qq 这一项，若此项已设置为某 QQ 号码，则 QQBot 在启动时会先使用此 QQ 号上次登录保存的登录信息来自动登录。 掉线后自动重启（ restartOnOffline ） 如果配置文件中将 restartOnOffline 项设置为 True ，则当 QQBot 掉线或出错终止时，会自动重新启动 QQBot 。 在后台运行 qqbot （ daemon ） 此选项仅在 UNIX 类系统中有效，将配置中的 daemon 选项设置为 True 则会以 daemon 模式运行程序。此时，标准输出和标准错误会重定向到 daemon-$qq.log 文件（其中 $qq 是配置中 qq 选项的值）。 联系人列表获取完成后再启动（ startAfterFetch ） 一般情况下，扫码登录完成就立即启动 QQBot，只有在需要的时候才会去获取联系人列表并更新联系人数据库。如果将配置文件中的 startAfterFetch 设置为 True ，则 QQBot 会等待所有联系人列表获取完成后才启动 ，注意，如果联系人较多，会耗费较长的时间。 QQBot-term 服务器端口号（ termServerPort ） QQBot 启动后，会开启一个 QQBot-term 服务器监听用户通过 qq 命令行工具发过来的操作命令以及通过 HTTP API 接口发过来的操作命令，此服务器的监听 IP 永远为 127.0.0.1 ，监听端口号默认为 8188 ，可以通过修改 termServerPort 的值来修改此端口号。 如果配置的 QQBot-term 服务器端口号不是默认的 8188 ，那么在运行 qq 命令时，需要在第一个参数中指定端口号，如： $ qq 8100 send buddy jack hello $ qq 8100 list group-member chatbot 同样，HTTP API 接口的端口号也需要改变，如： http://127.0.0.1:8100/send/buddy/jack/hello 。 如果不需要使用 qq 命令和 HTTP-API 接口，可以将此端口号设置为 0 ，此时 QQBot-term 服务器不会开启。 如果需要在同一台机器上登录多个 QQ 号码，可以直接在不同的终端中开启多个 qqbot 进程进行登录，但是，每个 qqbot 进程必须设置专有的 termServerPort 和 httpServerPort （或者全部设置为 0 或 空值 ），否则会造成端口号冲突。 调试模式（ debug ） 若 debug 项设置为 True ，则运行过程中会打印调试信息。 插件的配置（ pluginPath 和 plugins ） 一般情况下，插件需要存放在系统的 import 目录下或 ~/.qqbot-tmp/plugins 目录下，可以在 pluginPath 选项中配置其他的存放目录。另外，在 plugins 选项中可以指定 QQBot 启动时需要加载的插件。 命令行参数及配置的优先级 配置文件中的所有选项都有对应的命令行参数，在命令行参数中输入的选项优先级比配置文件高。输入 qqbot -h 可查看所有命令行参数格式。 程序一共有四个级别的配置，其优先级如下： 使用 qqbot -u somebody 启动程序时，依次加载： 根配置 -\u0026gt; 默认配置 -\u0026gt; 用户 somebody 的配置 -\u0026gt; 命令行参数配置 使用 qqbot 启动程序时，依次加载： 根配置 -\u0026gt; 默认配置 -\u0026gt; 命令行参数配置 其中：根配置 是固定的，用户无法修改； 默认配置 和 用户配置 可由用户在 v2.x.conf 文件中进行修改；最后，还可以在 命令行参数 中输入配置。 工作目录 qqbot 运行时，会在 工作目录 下 搜索/创建 以下 文件/目录 ： 配置文件： v2.x.conf 插件目录： plugins/ 登录文件： v2.x-pyx-xxxx.pickle 联系人数据库文件： 2017-05-06-20-03-12-xxxx-contact.db 临时二维码图片： xxxx.png 保存QQ的文件： qq(pid9816) 以 daemon 模式运行时的 log 文件： daemon-xxx.log 默认的工作目录为 ~/.qqbot-tmp/ ，可以在启动 qqbot 时通过命令行参数 -b|--bench 指定其他工作目录，例如： qqbot -b bench 。 \u0026lt;h1\u0026gt;八、 插件\u0026lt;/h1\u0026gt; 插件的存放位置 插件实际上是一个 python 模块，因此可以是一个 python 文件，也可以是一个 python package。 qqbot 会根据插件名在以下目录中搜索插件： 配置中的 pluginPath 选项（命令行参数 -pp|--pluginPath ）指定的目录 工作目录下的 plugins 目录 python 的导入目录 插件的加载/卸载 hot-plug 方式 可以在 qqbot 的运行过程中动态的加载/卸载插件，有以下三种方法： 利用 qq 命令行工具： qq plug pluginname 或 qq unplug pluginname 利用 http-api 接口： http://127.0.0.1:8188/plug/pluginname 或 http://127.0.0.1:8188/unplug/pluginname 利用 bot 对象的接口： bot.Plug(\u0026#39;pluginname\u0026#39;) 或 bot.Unplug(\u0026#39;pluginname\u0026#39;) 前面两种方法是供 qqbot 进程的外部进程调用的，第三种方法是在 qqbot 进程内部使用的。请勿在 qqbot 进程的内部使用前面两种方法。 注意：采用 hot-plug 方式加载的插件在 qqbot 重启后会丢失。 auto-plug-at-start 方式 也可以在 qqbot 的启动时自动加载插件，在配置中的 plugins 选项（命令行参数 -pl|--plugins ）中指定需要加载的插件名就可以了。这些插件将在启动时、登录之前被加载。 另外，如果系统中（或插件目录中）存在名为 qqbotdefault 的 package ，那么该 package 下面的所有子模块都会被当成插件在启动时自动加载（注意：qqbotdefault 本身不会作为插件加载）。 插件内的 onPlug 和 onUnplug 回调函数 插件被加载时，会执行 reload(pluginName) ，因此插件内的所有代码都会被执行一次 当采用 hot-plug 的方式加载时，插件内的 onPlug 函数会紧接在 reload 成功后被执行 当采用 auto-plug-at-start 方式加载时，插件在启动时、登录之前被加载，但插件内的 onPlug 函数会延迟到登录成功后才被执行 插件被卸载时，插件内的 onUnplug 被执行 插件的编写 编写插件主要就是编写回调函数或定时任务函数，详见 第四~六节 。 插件列表 名称\tgithub作者\t功能说明\t是否默认加载 qqbot.plugins.sampleslots\tpandolia\t回调函数示例\t是 qqbot.plugins.schedrestart\tpandolia\t定时重启\t是 qqbot.plugins.miniirc\tpandolia\tIRC服务器\t否 passwordlogin\tpandolia\t使用用户名-密码登录\t否 adblock\tfeisuweb\t群广告拦截\t否 chatlog\tfeisuweb\t聊天内容记录\t否 如果您有好用的插件分享，欢迎发邮件给我。 \u0026lt;h1\u0026gt;九、 命令行模式下使用 IRC 聊天\u0026lt;/h1\u0026gt; linux 系统下，由于无法使用 QQ 客户端，可以使用插件 qqbot.plugins.miniirc 来实现用 IRC 聊天的功能。加载方式： qq plug qqbot.plugins.miniirc ，或启动时加载： qqbot -pl qqbot.plugins.miniirc ，或者在配置文件中的 plugins 选项中加入 qqbot.plugins.miniirc 。 插件加载后将在 6667 端口开启一个微型的 IRC 服务器，用户可以使用 IRC 客户端（如 weechat, irssi 等）连接此服务器来实现命令行模式下的聊天。以下以 weechat 为例介绍使用方法： 启动 weechat ： weechat\n连接本服务器： /connect localhost\n进入 群聊天 会话： /join group-name\n进入 讨论组聊天 会话： /join !discuss-name\n进入 好友聊天 会话： /query buddy-name\n进入 聊天会话 后，直接敲入文本并回车就可以向对方发送消息了。所有接收到的 QQ 消息也会被转发给相应的 聊天会话 。\n在聊天会话之间切换： ctrl+P 或 ctrl+N\n显示所有 群和讨论组 的名称： /list\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 以上几乎就是此微型 IRC 服务器所提供的所有功能了，但已经足够用来和 QQ 好友/群/讨论组 聊天了。 \u0026lt;h1\u0026gt;十、 smartqq 协议支持及限制\u0026lt;/h1\u0026gt; 本项目已实现绝大部分 smartqq 协议支持的功能，如下： 消息收/发 联系人（包括 好友/群/讨论组/群成员/讨论组成员）资料获取和查询（包括 昵称/名称/备注名） 联系人资料根据需要动态更新 被群内其他成员 @ 的通知 发送、接收表情（详见 facemap.py） 可以获取到自身发送的 好友/群/讨论组 消息。但若是自身发送的 好友 消息，只能获取消息文本，无法知道该消息发送给谁。 其他功能： 调用系统默认图片浏览器显示登录二维码、将登录二维码发送至邮箱、开启一个 http 服务器用来显示登录二维码、在命令行窗口使用文本模式显示二维码 用 qq 命令行工具发消息、查询|更新联系人 提供 HTTP-API 接口发消息、查询|更新联系人 提供 miniirc 插件，可以在命令行模式下使用 IRC 客户端聊天 掉线后自动重启功能（有时需要手工扫码） 定时执行任务（通过 qqbotsched 实现） 因 smartqq 协议的限制，以下问题尚无完美的解决方法： 无法长时间保持在线状态，每次登录成功后的 cookie 会每在 1 ~ 2 天后失效，将被腾讯服务器强制下线，此时 必须 重新登录。可以打开邮箱模式和自动重启模式，并配合 qqbot.plugins.schedrestart 插件使用，每天在固定的时间 手工扫码 登录一次，基本上可以稳定的保持在线状态。 无法发送图片、文件、音频、 xml 卡片消息 无法获取到联系人的实际 QQ 无法在群内 @ 其他成员，即便用本程序在群里发送了 “@jack xxx” 这样的消息， jack 也只能收到这个纯文本，收不到“有人@我”的提醒。 无法向 群/讨论组 内的其他非好友成员发消息，也无法收到非好友成员发过来的临时会话消息 在非常少的情况下，发消息时会重复发送多次，也可能对方已收到消息但返回发送失败的结果 \u0026lt;h1\u0026gt;十一、其他\u0026lt;/h1\u0026gt; 常见问题 更新日志 \u0026lt;h1\u0026gt;十二、参考资料\u0026lt;/h1\u0026gt; QQBot 参考了以下开源项目： ScienJus/qqbot (ruby) floatinghotpot/qqbot (node.js) sjdy521/Mojo-Webqq (perl) 在此感谢以上三位作者的无私分享，特别是感谢 ScienJus 对 SmartQQ 协议所做出的深入细致的分析。 \u0026lt;h1\u0026gt;十三、反馈\u0026lt;/h1\u0026gt; 有任何问题或建议可以发邮件给我 pandolia@yeah.net ，或者直接提 issue ，也可以加 QQ 群： 577126408 。但还是希望您在提问之前通读一下本文档，很有可能您想要的答案已经在文档中了。 \u0026lt;h1\u0026gt;定制我想要的机器人\u0026lt;/h1\u0026gt; 根据这位大佬的框架，如何写一个自己的机器人呢？ 我开始是这样构思的，如何将机器人实现，如果他接收到一条失物消息，那么就将这条消息转发到我们失物招领三个群当中去，这是我们的用户需求。 我们现在来把它转换成系统需求： （1）机器人需要监控它所接收到的消息，判断是否是失物招领消息。 （2）如果是失物招领消息，那么把消息提取出来作为一个字符串发送到我所指定的三个群当中去。 嗯，大体上就是这个样子的，应该定义一个函数就能解决了。 一开始我写了这样的一个函数： from qqbot import qqbotsched def onQQMessage(bot,cntact,member,content): qian=bot.list(\u0026lsquo;buddy\u0026rsquo;,\u0026lsquo;不一\u0026rsquo;)[0] //将我qq号昵称检测到qian中 linux=bot.list(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;驾校\u0026rsquo;)[0] //将我们寝室群放到linux中 windows=bot.list(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;Ingress\u0026rsquo;)[0] //将另一个群放到windows中 if content from qian.name: //如果监测到消息中有我qq号的消息 bot.SendTo(linux,content) //那么将这条消息转发到指定的群\n1 2 但经过测试，这几行代码并没有什么用，然后我有做了一些修改： form qqbot import qqbotsched def onQQMessage(bot,cntact,member,content): linux=bot.List(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;安工程失物招领群1\u0026rsquo;) windows=bot.List(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;安工程失物招领群2\u0026rsquo;) deepin=bot.List(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;安工程失物招领群3\u0026rsquo;) if \u0026lsquo;@ME\u0026rsquo; in content: bot.SendTo()\n1 2 3 就酱紫，就这么简单的几行代码。但是我却想了很久，看了很长时间的文档。 现在是深刻的理解到了。编程并不是写代码，思考和选择算法的过程才是最重要的！ ","permalink":"https://csqread.top/posts/tech/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%88%91%E7%9A%84qq%E6%9C%BA%E5%99%A8%E4%BA%BA/","summary":"一直以来，我在想做一个qq机器人，能够实现qq消息的转发，因为对于失物招领工作，其他的都挺满意的，但每天发那么多消息确实很浪费时间，所以就想","title":"自定义我的QQ机器人"}]