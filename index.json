[{"content":"这篇文章主要记录我搭建hugo博客的过程，包括搭建过程中遇到的问题及解决方法\n1、新建github仓库 新建respository, Respository的名称为 \u0026ldquo;你的github名称.github.io\u0026rdquo;.\n2、下载并安装对应版本的hugo 下载链接: 点击选择hugo版本下载\n3、创建hugo网站 进入想要存放网站的文件夹，输入以下命令:\n1 hugo new site demo-blog 4、选择主题 输入命令下载主题:\n1 git clone https://github.com/adityatelange/hugo-PaperMod.git 使用该主题的方法就是在站点文件夹下的配置文件里输入主题的名字:\n然后把主题文件夹里面的一些静态文件和配置文件复制到站点目录下，目的是为了可以自定义博客的样式，而不会改动主题文件夹里的样式，这样主题要更新的时候，直接在主题目录下git pull就可以，站点目录的修改会优先覆盖主题里的配置，所以可以实现平滑更新\n","permalink":"https://csqread.top/posts/blog/hugo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","summary":"这篇文章主要记录我搭建hugo博客的过程，包括搭建过程中遇到的问题及解决方法 1、新建github仓库 新建respository, Respos","title":"Hugo博客搭建"},{"content":"第二章：数据模型与查询语言 数据模型可能是软件开发中最重要的部分了，因为它们的影响如此深远：不仅仅影响着软件的编写方式，而且影响着我们的解题思路。 一个复杂的应用程序可能会有更多的中间层次，比如基于API的API，不过基本思想仍然是一样的：每个层都通过提供一个明确的数据模型来隐藏更低层次中的复杂性。这些抽象允许不同的人群有效地协作。\n关系模型与文档模型 现在最著名的数据模型可能是SQL。它基于Edgar Codd在1970年提出的关系模型【1】：数据被组织成关系（SQL中称作表），其中每个关系是元组（SQL中称作行)的无序集合。\nNoSQL的诞生 NoSQL：不仅是SQL（Not Only SQL） 采用NoSQL数据库的背后有几个驱动因素，其中包括： ○ 需要比关系数据库更好的可扩展性，包括非常大的数据集或非常高的写入吞吐量 ○ 相比商业数据库产品，免费和开源软件更受偏爱。 ○ 关系模型不能很好地支持一些特殊的查询操作 ○ 受挫于关系模型的限制性，渴望一种更具多动态性与表现力的数据模型 在可预见的未来，关系数据库似乎可能会继续与各种非关系数据库一起使用 - 这种想法有时也被称为混合持久化（polyglot persistence） 对象关系不匹配 应用程序使用面向对象的语言，需要一个转换层，才能转成 SQL 数据模型：被称为阻抗不匹配。 Hibernate这样的 对象关系映射（ORM object-relational mapping） 框架可以减少这个转换层所需的样板代码的数量，但是它们不能完全隐藏这两个模型之间的差异。 对于一份简历而言，关系型模型需要一对多（比如工作经历）。 而表述这样的简历，使用 JSON 是非常合适的。JSON 比多表模式有更好的局部性，可以一次查询出一个用户的所有信息。JSON 其实是一棵树。 多对一和多对多的关系 为什么在 SQL 中，地域和公司都以 ID，而不是字符串进行标识呢？ ○ ID 对人类没有任何意义，所以永远不需要改变，可以规范化人类的信息。那么就会存在多对一的关系（多个人对应了同一个 ID）。 ○ 在关系数据库 SQL 中，所有使用它的地方可以用 ID 来引用其他表中的行； ○ 但是文档数据库（比如 JSON），对连接支持很弱。 如果数据库不支持链接，那么就需要在应用代码中，对数据库执行多个查询进行模拟。执行连接的工作从数据库被转移到应用程序代码上。 哪怕最开始的应用适合无连接的文档模型，但是随着功能添加，数据会变得更加互联，比如对简历修改：\n组织和学校作为实体：假如组织和学校有主页 推荐：给别人做推荐，当别人的信息更改的时候，所有地方要同步更新。 文档数据库是否在重蹈覆辙？ 20 世纪 70 年代，最受欢迎的是层次模型（hierarchical model），它与文档数据库使用的JSON模型有一些惊人的相似之处。它将所有数据表示为嵌套在记录中的记录树。虽然能处理一对多的关系，但是很难应对多对多的关系，并且不支持链接。\n提出的解决方案：\n关系模型（relational model）（它变成了SQL，统治了世界） 网络模型（network model）（最初很受关注，但最终变得冷门） 网络模型 支持多对多，每条记录可能有多个父节点。 网络模型中记录之间的链接不是外键，而更像编程语言中的指针（同时仍然存储在磁盘上）。访问记录的唯一方法是跟随从根记录起沿这些链路所形成的路径。这被称为访问路径（access path）。 最简单的情况下，访问路径类似遍历链表：从列表头开始，每次查看一条记录，直到找到所需的记录。但在多对多关系的情况中，数条不同的路径可以到达相同的记录，网络模型的程序员必须跟踪这些不同的访问路径。 缺点：查询和更新数据库很麻烦。 关系模型 数据：一个 关系（表） 只是一个 元组（行） 的集合，很简单。 在关系数据库中，查询优化器自动决定查询的哪些部分以哪个顺序执行，以及使用哪些索引。这些选择实际上是“访问路径”，但最大的区别在于它们是由查询优化器自动生成的，不需要程序猿考虑。 与文档数据库相比 但是，在表示多对一和多对多的关系时，关系数据库和文档数据库并没有根本的不同：在这两种情况下，相关项目都被一个唯一的标识符引用，这个标识符在关系模型中被称为外键，在文档模型中称为文档引用。\n关系型数据库与文档数据库在今日的对比 ● 支持文档数据模型的主要论据是架构灵活性，因局部性而拥有更好的性能，以及对于某些应用程序而言更接近于应用程序使用的数据结构。 ● 关系模型通过为连接提供更好的支持以及支持多对一和多对多的关系来反击。\n哪个数据模型更方便写代码？ 文档模型： ● 优点： ○ 如果应用程序中的数据具有类似文档的结构（即，一对多关系树，通常一次性加载整个树），那么使用文档模型可能是一个好主意。 ● 缺点： ○ 不能直接引用文档中的嵌套的项目，而是需要说“用户251的位置列表中的第二项”（很像分层模型中的访问路径）。但是，只要文件嵌套不太深，这通常不是问题。 ○ 文档数据库对连接的糟糕支持也许或也许不是一个问题，这取决于应用程序。 ○ 如果应用程序使用多对多关系，那么文档模型就没有那么吸引人了。 ○ 对于高度相联的数据，选用文档模型是糟糕的，选用关系模型是可接受的，而选用图形模型是最自然的。\n文档模型中的架构灵活性 文档模型是「读时模式」 ○ 文档数据库有时称为无模式（schemaless），但这具有误导性，因为读取数据的代码通常假定某种结构——即存在隐式模式，但不由数据库强制执行 ○ 一个更精确的术语是读时模式（schema-on-read）（数据的结构是隐含的，只有在数据被读取时才被解释），相应的是写时模式（schema-on-write）（传统的关系数据库方法中，模式明确，且数据库确保所有的数据都符合其模式） ○ 读时模式类似于编程语言中的动态（运行时）类型检查，而写时模式类似于静态（编译时）类型检查。 模式变更 ○ 读时模式变更字段很容易，只用改应用代码 ○ 写时模式变更字段速度很慢，而且要求停运。它的这种坏名誉并不是完全应得的：大多数关系数据库系统可在几毫秒内执行ALTER TABLE语句。MySQL是一个值得注意的例外，它执行ALTER TABLE时会复制整个表，这可能意味着在更改一个大型表时会花费几分钟甚至几个小时的停机时间，尽管存在各种工具来解决这个限制。 查询的数据局部性 ● 文档通常以单个连续字符串形式进行存储，编码为JSON，XML或其二进制变体 ● 读文档： ○ 如果应用程序经常需要访问整个文档（例如，将其渲染至网页），那么存储局部性会带来性能优势。 ○ 局部性仅仅适用于同时需要文档绝大部分内容的情况。 ● 写文档： ○ 更新文档时，通常需要整个重写。只有不改变文档大小的修改才可以容易地原地执行。 ○ 通常建议保持相对小的文档，并避免增加文档大小的写入\n文档和关系数据库的融合 ● MySQL 等逐步增加了对 JSON 和 XML 的支持 ● 关系模型和文档模型的混合是未来数据库一条很好的路线。\n数据查询语言 ● 关系模型包含了一种查询数据的新方法：SQL是一种 声明式 查询语言，而IMS和CODASYL使用 命令式 代码来查询数据库。 ● 命令式语言：告诉计算机以特定顺序执行某些操作，比如常见的编程语言。 ● 声明式查询语言（如SQL或关系代数）：你只需指定所需数据的模式 - 结果必须符合哪些条件，以及如何将数据转换（例如，排序，分组和集合） - 但不是如何实现这一目标。数据库系统的查询优化器决定使用哪些索引和哪些连接方法，以及以何种顺序执行查询的各个部分。 ○ SQL相当有限的功能性为数据库提供了更多自动优化的空间。 ○ 声明式语言往往适合并行执行。\nWeb上的声明式查询 ● 声明式语言更加泛化，不用关心底层的数据存储变化 ● 在Web浏览器中，使用声明式CSS样式比使用JavaScript命令式地操作样式要好得多。 ● 类似地，在数据库中，使用像SQL这样的声明式查询语言比使用命令式查询API要好得多。\nMapReduce查询 ● 一些NoSQL数据存储（包括MongoDB和CouchDB）支持有限形式的MapReduce，作为在多个文档中执行只读查询的机制。 ● MapReduce既不是一个声明式的查询语言，也不是一个完全命令式的查询API，而是处于两者之间 ○ 查询的逻辑用代码片断来表示，这些代码片段会被处理框架重复性调用。 ○ 它基于map（也称为collect）和reduce（也称为fold或inject）函数，两个函数存在于许多函数式编程语言中。 ● map和reduce函数在功能上有所限制： ○ 它们必须是纯函数，这意味着它们只使用传递给它们的数据作为输入，它们不能执行额外的数据库查询，也不能有任何副作用。 ○ 这些限制允许数据库以任何顺序运行任何功能，并在失败时重新运行它们。 ○ MapReduce是一个相当底层的编程模型，用于计算机集群上的分布式执行。像SQL这样的更高级的查询语言可以用一系列的MapReduce操作来实现，但是也有很多不使用MapReduce的分布式SQL实现。 ○ MapReduce的一个可用性问题：必须编写两个密切合作的JavaScript函数，这通常比编写单个查询更困难。此外，声明式查询语言为查询优化器提供了更多机会来提高查询的性能。基于这些原因，MongoDB 2.2添加了一种叫做聚合管道的声明式查询语言的支持\n图数据模型 多对多关系是不同数据模型之间具有区别性的重要特征。\n文档模型：适合数据有一对多关系、不存在关系\n图数据模型：适合多对多关系\n一个图由两种对象组成： a. 顶点（vertices）（也称为节点（nodes） 或实体（entities）） b. 边（edges）（ 也称为关系（relationships）或弧 （arcs） ）。\n举例：社交图谱，网络图谱，公路或铁路网络\n图数据结构示例（以社交网络为例）\n存储方式：属性图，三元组\n属性图 在属性图模型中，每个顶点（vertex）包括： ● 唯一的标识符 ● 一组 出边（outgoing edges） ● 一组 入边（ingoing edges） ● 一组属性（键值对） 每条 边（edge） 包括： ● 唯一标识符 ● 边的起点/尾部顶点（tail vertex） ● 边的终点/头部顶点（head vertex） ● 描述两个顶点之间关系类型的标签 ● 一组属性（键值对） 使用关系模式来表示属性图\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TABLE vertices ( vertex_id INTEGER PRIMARY KEY, properties JSON ); CREATE TABLE edges ( edge_id INTEGER PRIMARY KEY, tail_vertex INTEGER REFERENCES vertices (vertex_id), head_vertex INTEGER REFERENCES vertices (vertex_id), label TEXT, properties JSON ); CREATE INDEX edges_tails ON edges (tail_vertex); CREATE INDEX edges_heads ON edges (head_vertex); 关于这个模型的一些重要方面是：\n任何顶点都可以有一条边连接到任何其他顶点。没有模式限制哪种事物可不可以关联。 给定任何顶点，可以高效地找到它的入边和出边，从而遍历图，即沿着一系列顶点的路径前后移动。（这就是为什么例2-2在tail_vertex和head_vertex列上都有索引的原因。） 通过对不同类型的关系使用不同的标签，可以在一个图中存储几种不同的信息，同时仍然保持一个清晰的数据模型。 Cypher查询语言 Cypher是属性图的声明式查询语言，为Neo4j图形数据库而发明 通常对于声明式查询语言来说，在编写查询语句时，不需要指定执行细节：查询优化程序会自动选择预测效率最高的策略，因此你可以继续编写应用程序的其他部分。 查找所有从美国移民到欧洲的人的Cypher查询 1 2 3 4 MATCH (person) -[:BORN_IN]-\u0026gt; () -[:WITHIN*0..]-\u0026gt; (us:Location {name:\u0026#39;United States\u0026#39;}), (person) -[:LIVES_IN]-\u0026gt; () -[:WITHIN*0..]-\u0026gt; (eu:Location {name:\u0026#39;Europe\u0026#39;}) RETURN person.name SQL中的图查询 用关系数据库表示图数据，那么也可以用SQL，但有些困难。 在关系数据库中，你通常会事先知道在查询中需要哪些连接。在图查询中，你可能需要在找到待查找的顶点之前，遍历可变数量的边。也就是说，连接的数量事先并不确定。 语法很复杂， 三元组存储和SPARQL 在三元组存储中，所有信息都以非常简单的三部分表示形式存储（主语，谓语，宾语）。例如，三元组 (吉姆, 喜欢 ,香蕉) 中，吉姆 是主语，喜欢 是谓语（动词），香蕉 是对象。 三元组的主语相当于图中的一个顶点。而宾语是下面两者之一： a. 原始数据类型中的值，例如字符串或数字。在这种情况下，三元组的谓语和宾语相当于主语顶点上的属性的键和值。例如，(lucy, age, 33)就像属性{“age”：33}的顶点lucy。 b. 图中的另一个顶点。在这种情况下，谓语是图中的一条边，主语是其尾部顶点，而宾语是其头部顶点。例如，在(lucy, marriedTo, alain)中主语和宾语lucy和alain都是顶点，并且谓语marriedTo是连接他们的边的标签。 当主语一样的时候，可以进行省略写法 1 2 3 4 5 @prefix : \u0026lt;urn:example:\u0026gt;. _:lucy a :Person; :name \u0026#34;Lucy\u0026#34;; :bornIn _:idaho. _:idaho a :Location; :name \u0026#34;Idaho\u0026#34;; :type \u0026#34;state\u0026#34;; :within _:usa _:usa a :Loaction; :name \u0026#34;United States\u0026#34;; :type \u0026#34;country\u0026#34;; :within _:namerica. _:namerica a :Location; :name \u0026#34;North America\u0026#34;; :type \u0026#34;continent\u0026#34;. 语义网络 语义网是一个简单且合理的想法：网站已经将信息发布为文字和图片供人类阅读，为什么不将信息作为机器可读的数据也发布给计算机呢？ 资源描述框架（RDF）的目的是作为不同网站以一致的格式发布数据的一种机制，允许来自不同网站的数据自动合并成一个数据网络 - 一种互联网范围内的“关于一切的数据库“。 现在已经凉了。 SPARQL查询语言 SPARQL是一种用于三元组存储的面向RDF数据模型的查询语言 查找从美国转移到欧洲的人 1 2 3 4 5 6 PREFIX : \u0026lt;urn:example:\u0026gt; SELECT ?personName WHERE { ?person :name ?personName. ?person :bornIn / :within* / :name \u0026#34;United States\u0026#34;. ?person :livesIn / :within* / :name \u0026#34;Europe\u0026#34;. } 基础：Datalog Datalog是比SPARQL或Cypher更古老的语言，在20世纪80年代被学者广泛研究 本章小结 在历史上，数据最开始被表示为一棵大树（层次数据模型），但是这不利于表示多对多的关系，所以发明了关系模型来解决这个问题。 最近，开发人员发现一些应用程序也不适合采用关系模型。新的非关系型“NoSQL”数据存储在两个主要方向上存在分歧：\n文档数据库的应用场景是：数据通常是自我包含的，而且文档之间的关系非常稀少。 图形数据库用于相反的场景：任意事物都可能与任何事物相关联。 文档数据库和图数据库有一个共同点，那就是它们通常不会为存储的数据强制一个模式，这可以使应用程序更容易适应不断变化的需求。但是应用程序很可能仍会假定数据具有一定的结构；这只是模式是明确的（写入时强制）还是隐含的（读取时处理）的问题。 每个数据模型都具有各自的查询语言或框架，我们讨论了几个例子：SQL，MapReduce，MongoDB的聚合管道，Cypher，SPARQL和Datalog。我们也谈到了CSS和XSL/XPath，它们不是数据库查询语言，而包含有趣的相似之处。 ","permalink":"https://csqread.top/posts/tech/ddia%E7%AC%AC%E4%BA%8C%E7%AB%A0/","summary":"第二章：数据模型与查询语言 数据模型可能是软件开发中最重要的部分了，因为它们的影响如此深远：不仅仅影响着软件的编写方式，而且影响着我们的解题思","title":"DDIA第二章"},{"content":"第一章：可靠性，可扩展性，可维护性 (\r)\n关于数据系统的思考 单个工具已经不能满足应用系统的需求，总体工作被拆分成一系列能被单个工具高效完成的任务，并通过应用代码将它们缝合起来。比如一个缓存、索引、数据库协作的例子：\n一个应用被称为数据密集型的，如果数据是其主要挑战（数据量，数据复杂度、数据变化速度）——与之相对的是计算密集型，即处理器速度是其瓶颈。 软件系统中很重要的三个问题：\n可靠性（Reliability）：系统在困境（硬件故障、软件故障、人为错误）中仍可正常工作 可扩展性（Scalability）：有合理的办法应对系统的增长（数据量、流量、复杂性） 可维护性（Maintainability）：许多不同的人在不同的生命周期，都能高效地在系统上工作。 可靠性 定义 造成错误的原因叫做故障（fault），能预料并应对故障的系统特性可称为容错（fault-tolerant）或者韧性（resilient）。讨论容错时，只有讨论特定类型的错误 故障（fault）不同于失效（failure）：故障指的是一部分状态偏离标准，而失效则是系统作为一个整体停止向用户提供服务。 通常倾向于容忍错误（而不是阻止错误），但也有预防胜于治疗的情况（比如安全问题） 硬件故障 一般都是增加单个硬件的冗余度 云平台的设计是优先考虑灵活性和弹性，而不是单机可靠性。 软件错误 这类软件故障的bug 通常潜伏很长时间，直到被异常情况触发为止。往往是某个假设出于某种原因最后不在成立了。 解决办法：仔细考虑假设和交互；彻底的测试；重启；监控。 人为错误 人是不可靠的，运维配置错误是导致服务中断的首要原因。 解决办法：最小化犯错机会的方式设计系统；容易犯错的地方解耦；测试；监控；培训。 可扩展性 定义 可扩展性（Scalability）是用来描述系统应对负载增长能力的术语。 描述负载 负载可以用负载参数的数字来描述，取决于系统架构\n推特的发推设计：\na. 推文放在全局推文集合中，查询的时候做 join\nb.推文插入到每个关注者的时间线中，「扇出」比较大，当有千万粉丝的大 V 发推压力大\nc.推特从方案一变成了方案二，然后变成了两者结合的方式\n描述性能 当描述好负载以后，问题变成了：\na. 增加负载参数并保持系统资源不变时，系统性能将受到什么影响？\nb. 增加负载参数并希望性能不变时，需要增加多少系统资源？ 批处理系统，通常关心吞吐量（throughput）；在线系统，通常更关心响应时间（response time） 对于系统响应时间而言，最好用百分位点，比如中位数、p99 等标识。 测量客户端的响应时间非常重要（而不是服务端），比如会出现头部阻塞、网络延迟等。 实践中的百分位点，可以用一个滑动的时间窗口（比如 10 分钟）进行统计。可以对列表进行排序，效率低的话，考虑一下前向衰减，t-digest 等方法近似计算。 应对负载的方法 纵向扩展：转向更强大的机器 横向扩展：将负载分布到多台小机器上 弹性系统：检测到负载增加时自动增加计算资源 跨多台机器部署无状态服务比较简单，但是把带状态的数据系统从单节点变成分布式配置则可能引入许多额外复杂度。因此，应该尽量将数据库放在单个节点上。 可维护性 在设计之初就尽量考虑尽可能减少维护期间的痛苦，从而避免自己的软件系统变成遗留系统。 三个设计原则：可操作性（Operability）便于运维团队保持系统平稳运行。简单性（Simplicity）从系统中消除尽可能多的复杂度（complexity），使新工程师也能轻松理解系统。（注意这和用户接口的简单性不一样。）可演化性（evolability）使工程师在未来能轻松地对系统进行更改，当需求变化时为新应用场景做适配。也称为可扩展性（extensibility），可修改性（modifiability）或可塑性（plasticity）。 可操作性：人生苦短，关爱运维 ● 尽量自动化\n简单性：管理复杂度 ● 消除额外的（accidental）的复杂度 ● 消除额外复杂度的最好工具之一是抽象（abstraction）\n可演化性：拥抱变化 ● 敏捷（agile） 工作模式为适应变化提供了一个框架\n● 简单易懂的系统通常比复杂系统更容易修改，即可演化性（evolvability）\n参考文章: https://www.yuque.com/fuxuemingzhu/ddia/hqu56x\n","permalink":"https://csqread.top/posts/tech/ddia%E7%AC%AC%E4%B8%80%E7%AB%A0/","summary":"第一章：可靠性，可扩展性，可维护性 ( ) 关于数据系统的思考 单个工具已经不能满足应用系统的需求，总体工作被拆分成一系列能被单个工具高效完成的任务","title":"DDIA第一章"},{"content":"推荐序 这本书的适合所有后台开发工程师、大数据工程师，也很适合面试前复习系统设计的同学。\n什么是「数据密集型应用系统」？\n当数据（数据量、数据复杂度、数据变化速度）是一个应用的主要挑战，那么可以把这个应用称为数据密集型的。与之相对的是计算密集型——处理器速度是主要瓶颈。\n其实我们平时遇到的大部分系统都是数据密集型的——应用代码访问内存、硬盘、数据库、消息队列中的数据，经过业务逻辑处理，再返回给用户。\n很多应用都是在解决不同场景下的数据存储和检索问题——MySQL，Redis，HBase，Kafka，ElasticSearch…… 还有很多技术是围绕着数据展开——索引，编码（JSON, XML, Thrift, ProtoBuffer），行列存储…… 当数据在分布式处理时，要考虑——数据复制，分区，事务…… 大数据场景下，我们会使用——MapReduce，Spark，Flink 等批处理、流处理框架。 《数据密集型应用系统设计》这本书，把所有跟「数据」有关的知识点做了剖析、整理、总结，从一个很高的层次把各项技术的共性和区别讲得透彻。 当我们懂了底层原理之后，就明白了每项技术产生的背景是什么，解决了什么问题，有什么适用场景。\n这本书既有理论也有实践，基本没有公式，图很多，阅读起来很流畅，比较容易理解。\n这本书分为了三部分： ● 第一部分：数据系统的基石，包括数据模型与查询语言、存储与检索、数据编码与演化； ● 第二部分：分布式数据，包括复制、分片、事务、一致性与共识； ● 第三部分：衍生数据，包括批处理、流处理、数据系统的未来。\n阅读资源 这是一些阅读资源：\n《数据密集型应用系统设计》开源翻译仓库（9.3K star）： https://github.com/Vonng/ddia 开源版本在线阅读： https://vonng.gitbooks.io/ddia-cn/content/ 负雪明烛的读书笔记：数据密集型应用系统设计 《数据密集型应用系统设计》纸质书（翻译水平比开源在线阅读版好很多，强烈建议买书）：京东购买链接 辅助资料 ddia-references 这个仓库包含了《数据密集型应用系统设计》每章后面的所有参考文献对应的 pdf。 地址是：https://github.com/ept/ddia-references\nBook Review 这里有个很不错的 Book Review，是一个小哥讲了《DDIA》每一章的概述，作者很用心。 全英文的，在油管可以看到。地址是： https://www.youtube.com/watch?v=PdtlXdse7pw\u0026amp;list=PL4KdJM8LzAMecwInbBK5GJ3Anz-ts75RQ\n连载 后续我会把我读书笔记以及读书感想连载更新，这里是抄的一位大佬的序 https://www.yuque.com/fuxuemingzhu/ddia/kpqcs3\n","permalink":"https://csqread.top/posts/tech/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","summary":"推荐序 这本书的适合所有后台开发工程师、大数据工程师，也很适合面试前复习系统设计的同学。 什么是「数据密集型应用系统」？ 当数据（数据量、数据复杂","title":"数据密集型应用系统设计"},{"content":"分布式系统秒杀交易性能优化 1、背景 首先介绍一下这个交易的功能： 该交易是一个定期存款交易，利率非常高，存期有三个月，六个月、一年、两年、三年、五年，年化利率基本上每天发布的产品中都有4%以上的。其中六个月和一年的年化利率最受欢迎。这个定期产品只能每天9点开始购买，并且支持的渠道只有手机银行，因此在每天8点59-9点这个区间会收到非常的请求。根据指标控制中心查询的数据大概在9点到9点01秒这一秒中大概需要处理1000个请求。不过老核心那边怕给系统搞宕机做了一个并发控制，也就是达到性能瓶颈时会限制购买，打印“系统繁忙”反馈给前端.大概在不久前，老核心反馈给我们分布式核心说该交易也还需要持续进行优化，因为目前该产品实在是太火爆了，导致老核心也一直无法处理短时间内的大量请求，只能反馈系统繁忙。\n2、分析 我对老核心的这个交易逻辑进行了分析，其实他们已经做了不少优化了，不过目前看起来也还是很慢，从9点到9点1分我统计了这一分钟成功交易的平均耗时，大概是370ms左右。这个标准在老核心的交易中其实已经是比较慢的了。而分布式这边目前观察并行生产跑的性能更是非常慢，成功交易的平均耗时大概900ms左右。。。而这个交易又是我负责的产品，因此对它进行性能优化是当务之急。该交易的大致逻辑如下：\n前端请求进来，先做一些必输项校验，包括客户信息，密码校验等。接着是查询该产品的产品信息表，获取该产品信息，与客户信息做一些校验。后面会做一些额度的扣减。因为该款产品每次发布是有限额的。比如1000万美元的额度，抢完就没了，因此每次购买完了需要做一个额度的扣减。完了之后就需要记一笔账务。即从该客户的备用金账户扣除一笔钱到该客户的定期账户。最后再落一张表存储该客户购买的信息。\n其实老核心之前做的优化是把额度扣减从交易的前面移到了交易的最后。这样做的目的是，在并发请求打进来时，在做额度扣减的时候，会锁表，把这块逻辑放在最后做，能把锁表的时间减少的最短，如果放在前面做额度扣减，那么锁表的时间就是整个交易做完的时间，但是放在最后，就是最后几十ms。其他他们也没有做什么优化，\n但是分布式这边就很麻烦了。我这边也是把额度扣减放在最后的，按照他们的想法是把锁表的时间降低到最少。但是这样的话，因为我们是分布式，我得调两次额度服务，两次RPC所耗费得的时间也是很长的。而且分布式按照之前卡服务提供的接口，需要查询三次卡服务的表才能满足业务逻辑。所以我这边所做的优化大概如下：\n3、优化 首先，向卡组提需求，将三次RPC卡服务减少为RPC一次，一次查询，把所有需要的字段全部返回，减少两次RPC的时间 关于额度这块，我的想法是得把两次RPC减少为1次，而且也要减少锁表时间。。。一种是加缓存，把查询产品信息这块加缓存，这样就不用RPC额度了，但是这块有隐患，产品信息这张表是不断更新的，每天都有新的产品发布，按照redis缓存，是一些参数数据不太变化的数据加缓存，如果加缓存，生产上也得不断地去更新缓存。到时候查不到产品信息还是得去RPC额度服务。因此考虑了第二种方法。就是把更新额度和查询额度还是放到一次RPC中去做，并且放到交易得最前面。然后这块和整个交易不放在一个全局事务里面去做，分为两个事务去做。即更新额度与记账不在一起。这样的话。我们只需要try catch整个记账得逻辑，如果记账失败了，那我们就触发一个事件，去回滚前面那个事务做的额度扣减，把这个额度给加上。这样就不会造成长时间的锁表，也不会两次RPC。 最后，其实我对分布式每秒的高并发量是保持乐观态度的，因此在单元化之后已经将客户哈希到十个不同的服务上。因此就算1s钟500个并发进来那么也是随机平均的打到10个服务上面，而且进行容器化后，每个服务也是有很多机器来处理这些请求的，大家分担之后是完全可以处理这种秒杀交易的，虽然分布式这边的交易耗时没办法降到单核心交易耗时以下，但是系统处理能力也不是单核系统能比拟的。 4、总结 该交易涉及账务和额度增减，也就是说，如果交易失败，还需要进行事务回滚。在老核心单核系统处理起来是非常简单粗暴的。但是分布式由于各种RPC的原因，处理的手法就得麻烦点，不过也非常考验对分布式事务和消息队列的理解。其实整个系统达到1000tps在之前的性能测试中已经压到这个指标了，不过还没有涉及到这个秒杀的情况。虽然做了上面的这几个优化，但是肯定还是无法从900ms的平均耗时降下来很多。根据投产后的观察，大概能到500ms，和单核系统相比仍然差距很大。后续要需要持续的进行优化。\n这里涉及到的分布式事务知识点还是很重要的。而且我们在框架设计中使用的是乐观锁，也就是已经使得系统性能不会受到很大限制了。但是我觉得在这个秒杀交易这不太适合使用乐观锁。。。。。因为可能会导致很多请求冲突无功而返浪费时间\n大致的一个工作总结吧，其实这个产品我也买了。。，但是在查询的时候，明显的在9点的时候刷不到产品信息，提示系统繁忙，其实这个产品的查询也是需要优化的，因为大家在购买产品的时候刷新这个页面看产品信息的时候可能那一秒中的并发量就破万了，这可能就是单核的局限性吧，后面还分布式了应该不会出现这种尴尬的情况。不过仅仅是查询的化从后端的角度来说，优化其实也只是在缓存和SQL上面进行优化。我也是第一次在工作中实际遇到秒杀交易的优化，也让我更加深入的理解了一下这块所涉及的分布式系统设计和后端优化上的知识点。\n","permalink":"https://csqread.top/posts/tech/%E5%88%86%E5%B8%83%E5%BC%8F%E7%A7%92%E6%9D%80%E4%BA%A4%E6%98%93%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E8%AE%B0%E5%BD%95/","summary":"分布式系统秒杀交易性能优化 1、背景 首先介绍一下这个交易的功能： 该交易是一个定期存款交易，利率非常高，存期有三个月，六个月、一年、两年、三年、","title":"分布式秒杀交易性能优化记录"},{"content":"最近在看《重构》第二版，看到替换循环这里，想到我在工作中也是无脑的使用循环和if-else，最近也有空没事就重构之前自己写的代码以及别人留下来的代码。因此就自己尝试了一下管道。发现确实非常的简单明了。大致我举个例子说明下：\n大致需求是这样，从汽车List中找到2000年以后生产的汽车:\nCar类： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package test; public class Car { private String make; private String model; private String year; public Car(String make, String model, String year) { this.make = make; this.model = model; this.year = year; } public String getMake() { return make; } public void setMake(String make) { this.make = make; } public String getModel() { return model; } public void setModel(String model) { this.model = model; } public String getYear() { return year; } public void setYear(String year) { this.year = year; } } new 对象 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package test; import java.util.Arrays; import java.util.List; public class Iterating { public static List\u0026lt;Car\u0026gt; createCars(){ return Arrays.asList( new Car(\u0026#34;Jeep\u0026#34;, \u0026#34;Wrangler\u0026#34;, \u0026#34;2011\u0026#34;), new Car(\u0026#34;Jeep\u0026#34;, \u0026#34;Comanche\u0026#34;, \u0026#34;1990\u0026#34;), new Car(\u0026#34;Dodge\u0026#34;, \u0026#34;Avenget\u0026#34;, \u0026#34;2010\u0026#34;), new Car(\u0026#34;Buick\u0026#34;, \u0026#34;Cascada\u0026#34;, \u0026#34;2016\u0026#34;), new Car(\u0026#34;Ford\u0026#34;, \u0026#34;Focus\u0026#34;, \u0026#34;2012\u0026#34;), new Car(\u0026#34;Chevrolet\u0026#34;, \u0026#34;Geo Metro\u0026#34;, \u0026#34;1992\u0026#34;) ); } } 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 package test; import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; import java.util.List; import java.util.stream.Collectors; public class GetCarsModel { public static void main(String[] args) { List\u0026lt;Car\u0026gt; cars = Iterating.createCars(); System.out.println(getAfter2000years(cars)); System.out.println(getModelsAfter2000UsingPipeline(cars)); } /** * 使用for循环 * @param cars * @return */ private static List\u0026lt;String\u0026gt; getAfter2000years (List\u0026lt;Car\u0026gt; cars) { List\u0026lt;Car\u0026gt; carsDortedByYear = new ArrayList\u0026lt;\u0026gt;(); for (Car car : cars) { if (car.getYear().compareTo(\u0026#34;2000\u0026#34;) \u0026gt; 0) { carsDortedByYear.add(car); } } Collections.sort(carsDortedByYear, new Comparator\u0026lt;Car\u0026gt;() { @Override public int compare(Car o1, Car o2) { return (o1.getYear().compareTo(o2.getYear())); } }); List\u0026lt;String\u0026gt; models = new ArrayList\u0026lt;\u0026gt;(); for (Car car : carsDortedByYear) { models.add(car.getModel()); } return models; } /** * @deacription 使用管道 * @param cars * @return */ private static List\u0026lt;String\u0026gt; getModelsAfter2000UsingPipeline(List\u0026lt;Car\u0026gt; cars) { return cars.stream().filter(car -\u0026gt; car.getYear().compareTo(\u0026#34;2000\u0026#34;) \u0026gt; 0).sorted(Comparator.comparing(Car::getYear)).map(Car::getModel).collect(Collectors.toList()); } } 最后这俩结果是一样的，但是哪个更加简单明了一眼可知。\n只用了短短几行代码，代码的意图就很明显 — 给定一个汽车集合，过滤或提取仅在 2000 年或以后制造的汽车；然后按年份进行排序，将这些对象映射或转换为它们的型号，最后将结果收集到一个列表中。\n","permalink":"https://csqread.top/posts/tech/%E9%87%8D%E6%9E%84%E4%B9%8B%E7%AE%A1%E9%81%93%E6%9B%BF%E6%8D%A2%E5%BE%AA%E7%8E%AF/","summary":"最近在看《重构》第二版，看到替换循环这里，想到我在工作中也是无脑的使用循环和if-else，最近也有空没事就重构之前自己写的代码以及别人留下","title":"重构之管道替换循环"},{"content":"这几天一直在修并行环境的bug，也踩了不少坑， 记录一下\n1、关于对集合的排序循环问题。集合被修改之后不能再用于循环 原本的逻辑是这样的，在对C代码使用Java进行翻写的时候，因为C使用的是临时表，我这边用的是List集合，然后那边直接order By，而我用的是封装的一个对集合进行排序的方法。因此该集合在某种程度上是被修改了的，但是我仍然对让它进入了下一次循环，而没有break掉，或者使用一个新的集合来进行修改，导致并行生产环境报错。在不确定集合是否被修改的情况下，一定要New一个新的List来进行修改，保持原来的List进行循环。。。。\n2、spring boot redis 序列化报错 as a subtype of [simple type, class java.lang.Object]: no such class found 问题 大致场景是这样的， 有两个服务 A B, A服务用于授权， 授权成功会存储对象到redis中, B服务通过token去redis中拿到Object对象转换成业务对象。\n大致原因是： A服务存储对象到redis中时候会有一个全路径类名限定，在通过token进行取对象值并强制转换的时候，如果接受对象的全路径名与redis中保存的不一致的话就会转换失败报错。\n可能是因为之前项目类路径改造的时候，把这张表对应的类路径漏掉了，而redis那边不是实时从现有的数据库中获取的，而是根据我们各个业务组之前手动登记的路径进行修改的，因此并行生产出现这种问题。。。\n解决办法：\n1、把路径名称改成一致（使用了这个办法） 2、将保存对象的方式改成其他方式（这个存疑， 在网上看的）\n","permalink":"https://csqread.top/posts/tech/%E5%B7%A5%E4%BD%9C%E4%B8%8A%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91%E8%AE%B0%E5%BD%95/","summary":"这几天一直在修并行环境的bug，也踩了不少坑， 记录一下 1、关于对集合的排序循环问题。集合被修改之后不能再用于循环 原本的逻辑是这样的，在对C代","title":"工作上遇到的坑记录"},{"content":"进程的概念 在多道程序环境下，允许多个程序并发执行，此时他们将失去封闭性，并具有间断性和不可再现性的特征。为此引入了进程的概念，以便更好地描述和控制程序的并发执行，实现操作系统的并发行和共享性。为此引入了进程的概念，以便更好地描述和控制程序的并发执行，实现操作系统的并发性和共享性。\n为了是参与并发执行的程序能独立的运行，必须为之配置一个专门的数据结构，称之为进程控制块（process control block），系统利用PCB来描述进程的基本情况和运行状态，进而控制和管理进程。\n相应的，有程序段、相关数据段和PCB三部分构成了进程映像（进程实体）。所谓创建进程，实质上是创建进程映像中的PCB；而撤销进程，实质上是撤销进程的PCB。指的注意的是，进程影响是静态的，晋城市动态的。\n从不同的角度，进程可以有不同的定义，比较经典的定义有：\n1） 进程是程序的一次执行过程\n2） 进程是一个程序及其数据在处理器上顺序执行时所发生的活动。\n3） 进程是具有独立功能的程序在一个数据集合上运行的过程，他是系统进行资源分配和调度的一个独立单位。\n在引入了进程实体的概念后，我们可以吧传统的操作系统中的进程定义为：“进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位”。\n进程的特征 进程是由多程序的并发执行而引出的，他和程序是两个截然不同的概念。进程的基本特征是对比单个程序的顺序执行提出的，也是对进程管理提出的基本要求。\n1） 动态性：进程是程序的一次执行，他有着创建、活动、暂停、终止等过程，具有一定的生命周奇奇，是动态的产生、变化和消亡的。动态性是进程最基本的特征。\n2） 并发性：至多个进程实体，同存于内存中，能在一段时间内同时运行，并发性是进程的重要特征，同时也是操作系统的重要特征，引入进程的目的就是为了是程序能与去其他进程的程序并发执行，以提高资源利用率。\n3） 独立性：指进程实体是一个能独立运行、独立获得资源和独立接收调度的基本单位。范围建立PCB的程序都不能作为一个独立的单位参与运行。\n4） 异步性：由于进程的相互制约，是进程具有执行的间断性。也即进程按各自独立的、不可预知的速度向前推进。异步性会导致执行结果不可再现性，为此，在操作系统中必须配置相应的进程同步机制。\n5） 结构性：每个进程都配置一个PCB对其进行描述。从结构上来看，进程实体是由程序段、数据段和进程控制端三部分组成的。\n进程的状态与转换 进程在其生命周期内，由于系统中个进程之间的相互制约关系以及系统的运行环境的变化，使的进程的状态也在不断地发生着变化。通常进程有以下五种状态。前三种是进程的基本状态。\n1） 运行状态：进程正在处理器上运行。在单处理器的环境下，每一时刻最多只有一个进程处于运行状态。\n2） 就绪状态：进程已处于准备运行的状态，即进程获得了除CPU之外的一切所需资源，一旦得到处理器即可运行。\n3） 阻塞状态：又称为等待状态：进程正在等待某一事件而暂停运行，如等待某资源为可用（不包括处理器），或等待输入输出的完成。及时处理器空闲，该进程也不能运行。\n4） 创建状态：进程正在被创建，尚未转到就绪状态。创建进程通常需要多个步骤：首先申请一个空白的PCB，并向PCB中填写一些控制和管理进程的信息；然后由系统为该进程分配运行时所必须的资源；最后把该进程转入到就绪状态。\n5） 结束状态：进程正在从系统中消失，这可能是进程正常结束或其他原因中断退出运行。当进程需要结束运行时，系统首先必须置该进程为结束状态，然后再进一步处理资源释放和回收工作。\n注意区别就绪状态和等待状态：就绪状态是指进程仅缺少处理器，只要活得处理器资源就立即执行；而等待状态是指进程需要其他资源或等待某一事件，及时处理器空闲也不能运行。\n进程控制 进程控制的主要功能是对系统中所有进程实施有效地管理，她具有创建新进程、撤销已有进程、实现进程状态转换等功能。在操作系统中，一般把进程控制用的程序段成为原语，原语的特点是执行期间不允许中断，他是一个不可分割的基本单位。\n允许一个进程创建另一个进程。\n操作系统创建一个新进程的过程如下（创建原语）：\n1） 为新进程分配一个为我一个进程标示号，并申请一个空白的PCB。\n2） 为进程分配资源，为新进程的程序和数据，以及用户占分配必要的空间。\n3） 初始化PCB，主要包括初始化标识信息、初始化处理器状态信息和初始化处理器控制信息，以及设置进程的空闲及。\n4） 如果进程就绪队列能够接纳新进程，就将新进程插入到就绪队列，等待被调度运行。\n引起进程终止的时间主要有：正常结束、表示进程的任务已经完成和准备退出运行。异常结束是指进程在运行时，发生了某种异常事件，是程序无法继续运行，如：存储区越界、保护措、非法指令、特权指令错、IO故障等。外界干预是指进程外界的请求而终止，如操作员或操作系统干预、父进程请求和父进程终止。\n操作系统终止进程的过程如下：（撤消原语）\n1） 根据被终止进程的标示符，检索PCB，从中读出该进程的状态。\n2） 若被终止进程处于执行状态，立即终止该进程的执行，将处理器资源分配给其他进程。\n3） 若该进程还有子进程，则应将其所有子进程终止。\n4） 将该进程所拥有的资源、或归还给父进程或归还给操作系统。\n5） 将该PCB从所在队列（链表）中删除。\n进程的阻塞与唤醒 正在执行的进程，犹豫期待的某些时间为发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无心工作可做等，则由系统自动执行阻塞原语，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为。\n阻塞原语的执行过程为：找到将要被阻射进城的标识号对应的PCB，如果该进程为运行状态，则保护其现场，将其状态改为阻塞状态，停止运行，并把该PCB插入响应时间的等待队列中去；若为就绪状态，则将其状态改为阻塞状态，把它溢出就绪队列，插入到等待队列中去。\n当阻塞进程所期待的时间出现时，如它所启动的IO操作已完成或其所期待的数据已到达，则有关进程（比如，提供数据的进程），调用唤醒原语，将等待该事件的进程唤醒，唤醒原语的执行过程是：在该事件的等待队列中找到相应进程的PCB，然后把该PCB插入到就绪队列中，等待调度程序调度。\n需要注意的是，Block原语和Wakeup原语是一对作用刚好相反的原语，必须成对使用。Block原语是由被阻塞进程自我调用实现的，而Wakeup原语则是由一个与被唤醒进程相合作或被其他相关进程调用实现的。\n无论什么样的进程操作，都是在内核执行的。\n进程切换是指当前正在运行的进程被转换到其他状态后，再回到运行继续执行的过程，这个过程中，进程的运行环境产生了实质性的变化。进程切换的过程如下：\n1） 保存处理器上下文，包括程序计数器和其他寄存器。\n2） 更新PCB信息。\n3） 把进程的PCB移入相应的队列，如就绪、在某时间阻塞等队列。\n4） 选择另一个进程执行，并更新其PCB。更新内存管理的数据结构。\n5） 恢复处理器的上下文。\n进程控制块 进程创建时，操作系统就新建一个PCB结构，它之后就常驻内存，任意时刻可以存取。在进程结束时删除。PCB是进程实体的一部分，是进程存在的唯一标识。\nPCB主要包括：进程描述信息、进程控制和管理信息、资源分配清单和处理器相关信息等。\n在一个系统中，通常存在这许多进程，有的处于就绪状态，有的处于阻塞状态，而且阻塞的原因各不相同。为了方便进程的调度和管理，需要将各进程的PCB用适当的方法组织起来。目前，常用的组织方式有连接方式和索引方式两种。连接方式将同一状态的PCB连接成一个队列，不同状态对应不同的队列，也可以把处于阻塞状态的进程的PCB，根据其阻塞原因的不同，排成多个阻塞队列。索引方式是将同一状态的进程组织在一个索引表中，索引表的表项只想相应的PCB，不同状态对应不同的索引表，如就绪索引表和阻塞索引表等。\n程序段就是能北京城调度程度调度到CPU执行的程序代码段。注意，程序可以被多个进程共享，就是说多个进程可以运行同一个程序。\n一个进程的数据段，可以是进程对应的程序加工处理的原始数据，也可以是程序执行时产生的中间或最终结果。\n进程的通信 进程通信就是进程之间的数据交换。PV操作时低级通信方式2，高级通信方式是指以较高的效率传输大量数据的通信方式。高级通信方法可分为共享存储、消息传递和管道通信三大类。\n共享存储 在通信的进程之间存在着一款可以直接访问的共享空见，通过对这块共享空间的读写操作时间进程之间的信息交换。在共享存储方法中，需要使用同步互斥工具。\n需要注意的是：用户进程空间一般都是相互独立的，要想让两个用户进程共享空间，必须通过特殊系统调用实现，而进程内的线程是自然共享进程空间的。\n消息传递 在消息传递系统中，进程间的数据交换，是以格式化的小心Message为单位的。\n管道通信 管道通信是消息传递的一种特殊方式。。所谓管道，就是用于连接一个读进程和一个写进程以实现他们之间通信的一个共享文件，又名为pipe文件。向管道或共享文件提供输入的发送进程，以字符流的形势将大量的数据送入写管道；而接收管道输出的接收进城，则从管道中接受数据。为了协调双方的通信，关到极致必须他提供以下撒按方面的协调能力：互斥、同步和确定对方存在。\n线程概念和多线程模型 引入进程的目的，是为了是多道程序能并发执行，以提高资源利用率和系统吞吐量；而引入线程，则是为了减小程序在并发执行时所付出的时空开销，提高操作系统的并发性能。\n线程最直接的理解就是“轻量级进程”，它是一个基本的CPU执行单元，也是程序执行流的最小单元，由线程ID、程序计数器、寄存器集合和堆栈组成。线程是进程中的一个实体，是被系统独立调度和分派的基本单位。进程只作为除CPU以外的系统资源的分配单元，线程则作为处理器的分配单元。线程也有就绪、阻塞和运行三种基本状态。\n线程和进程的比较 1） 调度：在引入线程的操作系统中，线程是独立调度的基本单位，进程是资源拥有的基本单位。\n2） 拥有资源：进程是拥有资源的基本单位，而线程不拥有系统资源，单线程可以防伪其隶属进程的系统资源。\n3） 并发性：在引入线程的操作系统中，不仅进程之间可以并发执行，线程之间也可以并发执行，从而是操作系统具有更好的并发性，大大提高了系统的吞吐量。\n4） 系统开销：线程开销极小。\n5） 地址空间和其他资源：进程的地址空间之间相互独立，同一进程的各线程间共享进程的资源，进程内的线程对进程外的其他进程不可见。\n6） 通信方面：进程间通信需要进程同步和互斥手段的辅助，以保证数据的一致性，而线程间可以直接读写进程数据段来进行通信。\n线程的属性 在多线程操作系统中，八仙城作为独立运行的基本单位。此时的进程已不是一个基本可执行的实体。线程的主要属性如下：\n1） 线程是一个轻型实体，它不拥有系统资源，但每个线程都应有一个唯一的标识符和一个线程控制块，线程控制块记录了线程执行的寄存器和栈等现场情况。\n2） 不同的线程可以执行相同的程序，即同一个服务程序被不同的用户调用时，操作系统为他们创建不同的线程。\n3） 统一进程中的各个线程共享该进程所拥有的系统资源。\n4） 线程是处理器的独立调度单位，多个线程是可以并发执行的。\n5） 一个线程被创建后便开始了它的生命周期，直至终止，线程在生命周期内会经历等待态、就绪态和运行态等各种状态变化。\n线程的实现方法 线程的实现可以分为两类：用户级线程和内核级线程。\n多线程模型 有些系统同时支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程和内核级线程的连接方式。\n1） 多对一模型。多对一模型将多个用户级线程映射到一个内核级线程。线程管理在用户空间完成。\n2） 一对一模型。\n3） 多对多模型。\n特点：克服了多对一模型的并发度不高的缺点，又克服了一对一模型中一个用户进程占用太多内核级线程，开销太大的缺点。\n线程的调度 在多道程序系统中，进程的数量往往多于处理器的个数，进程争用处理器的情况在所难免。处理器调度是对处理器进行分配，就是从就绪队列中，按照一定的算法，选择一个进程并将处理器分配给他运行，以实现进程的并发执行。\n处理器调度是多道程序操作系统的基础，它是操作系统设计的核心问题。\n一个作业从提交开始知道完成，往往要经历一下三级调度：\n1）作业调度。作业调度又称高级调度：其主要任务是按一定的原则从外存上处于后备状态的作业中挑选一个或多个作业，给他们分配内存、输入输出设备等必要的资源。并建立相应的进程，以使他们获得竞争处理器的权利。\n多道批处理系统中大多配有作业调度，而其它系统中通常不需要配置作业调度。作业调度的执行频率较低，通常为几分钟一次。\n2）中级调度。中级调度又称内存调度。引入中级调度视为了提高内存利用率和系统吞吐率，为此，应使那些暂时不能运行的进程调至外存等待，把此时的进程状态称为挂起状态。当他们已具备运行条件且内存有稍有空闲时，由中级调度来决定，吧外存上那些已具备运行条件的就绪进程，在重新调入内存，并修改其状态为就绪状态，挂在就绪队列上等待。\n3）进程调度。进程调度又称为低级调度，其主要任务是按照某种方法和策略从就绪队列中选取一个进程，将处理器分配给它。进程调度是操作系统中最基本的一中调度，在一般操作系统中都不需配置进程调度。进程调度的频率很高，一般几十毫秒一次。\n作业调度从外存的后备队列中选择一批作业进入内存，为他们建立进程。这些进程被送入就绪队列。进程调度从就绪队列中选出一个进程，并把其状态改为运行状态，把CPU分配给它。中级调度是位于高级调度和低级调度之间的一种调度。为了提高内存的利用率，系统将那些暂时不能运行的进程挂起来。当内存空间宽松式，通过中级调度选择具备运行条件的进程，将其唤醒。\n调度的时机、切换与过程 进程调度和切换程序是操作系统内核程序。当请求调度的事件发生后，才可能会运行进程调度程序，当调度了新的就绪进程后，才会去进行进程间的切换。\n现在操作系统中，不能进行进程的调度与切换的情况有以下几种：\n1） 在处理中断的过程中：中断处理过程复杂，在实现上很难做到，而且中断处理时系统工作的一部分，逻辑上不属于某一进城，不应被剥夺处理器资源。\n2） 进程在操作系统内核程序临界区中：进入临界区后，需要独占式的访问共享数据，理论上必须加锁，以防止其他并行程序的进入，在解锁前不应该切换到其他进程，以加快该共享数据的释放。\n3） 其他需要完全屏蔽中断的原子操作过程中：如加锁、解锁、中断现场保护、恢复等等源自操作。在原子过程中，连中断都要屏蔽，更不应该进行进程的切换。\n如果在上述过程中发生了引起调度的条件，并不能马上进行调度和切换，应置系统请求调度标志，知道上述过程结束后才能进行相应的调度和切换。\n应该进行进程的调度与切换的情况有：\n1） 当发生引起调度条件且当前进程无法继续运行下去时，可以马上进行调度与切换。如果操作系统只在这种情况下进行进程调度，就是非剥夺调度。\n2） 当中断处理结束后或自陷处理结束后，返回被中断进程的用户态程序执行现场前，若置上请求调度标志，即可马上进行进程调度与切换。如果操作系统支持这种情况下的运行调度程序，就实现了剥夺方式的调度。\n进程切换往往在调度完成后立刻发生，它要求保存源进程当前切换点的县城信息，恢复被调度进程的现场信息。现场切换时，操作系统内核将远近程的现场信息推入到当前进程的内核对战来保存他们，并更新堆栈指针。内核完成从新进程的内核栈中装入新进程的县城信息、更新当前运行进程空间指针、重设PC寄存器等相关工作之后，开始运行新的进程。\n进程调度方式 所谓进程调度方式是指当某一个进程正在处理器上执行时，若有某个更为重要或紧迫的进程需要处理，既有优先权更高的进程进入就绪队列，此时应如何分配处理器。通常有一下两种进程调度方式：\n（1） 非剥夺调度方式 非剥夺调度方式又称为非抢占调度方式，是指当一个进程正在处理器上执行时，即使有某个更为重要或紧迫的进程进入就绪状态，仍然让正在执行的进程继续执行，知道该进程完成或发生某种时间而进入阻塞状态时，才把处理器分配给更为重要或紧迫的进程。\n（2） 剥夺调度方式 剥夺调度方式又称为抢占方式，是指当一个进程正在处理器上执行时，若有某个更为重要或紧迫的进程需要使用处理器，则立即暂停正在执行的进程，将处理器分配给这个更为重要或紧迫的进程。\n“剥夺”不是一种任意性行为，必须遵循一定的原则：优先权原则，短进程优先原则和时间片原则。\n调度的基本准则 不同的调度算法具有不同的特性，在选择调度算法时，必须考虑算法所具有的特性。为了比较处理器调度算法的性能，人们提出很多评价准则，下面介绍主要的几种准则：\n（1） CPU利用率 CPU是计算机系统中最重要的资源之一，所以应尽可能使CPU保持在忙状态，是这一资源利用率最高。\n（2） 系统吞吐量 系统吞吐量表示单位时间内CPU完成作业的数量。长作业需要消耗较长的处理器时间，因此会降低系统的吞吐量。而对于短作业，他们所需要消耗的处理器时间端，因此能提高系统的吞吐量。调度算法和方式的不同，也会对系统的吞吐量产生较大的影响。\n（3） 周转时间 周转时间是指从作业提交到作业完成所经历的时间，包括作业等待、在就绪队列中排队、在处理器上运行以及进行输入输出操作所花费的时间的总和。\n作业的周转时间=作业完成时间-作业提交时间\n（4） 等待时间 等待时间是指进程处于等处理器状态时间之和，等待时间越长，用户满意度越低。处理器调度算法实际上并不影响作业执行或输入输出操作时间，只影响作业在就绪队列中等待所花的时间。因此，衡量一个调度算法优劣常常只需简单地考察等待时间。\n（5） 响应时间 响应时间是指从用户提交请求到系统首次产生响应所有的时间。在交互式系统中，周转时间不可能是最好的评测准则，一般采用响应时间作为衡量调度算法的重要准则之一。从用户的角度来看，调度策略应尽量降低响应时间，使响应时间处在用户能够接受的范围之内。\n典型的调度算法 通常系统的设计目标不同，所采用的调度算法也不同。在操作系统中存在多种调度算法，其中有的调度算法适用于作业调度，有的调度算法适用于进程调度，有的调度算法两者都适用。下面介绍几种常用的调度算法：\n（1） FIFS先来先服务调度算法 特点：算法简单，但是效率低；有利于长作业，不利于短作业；有利于CPU繁忙型作业而不利于IO繁忙型作业。\n（2） SJF短作业优先调度算法 短作业（进程）优先调度算法是指对短作业祸端进程优先调度的算法。短作业优先调度算法是从后备队列中选择一个或若干个估计运算时间最短的作业，将他们呢掉入内存运行。\nSJF调度算法的缺点：\n1） 该算法对长作业不理。\n2） 该算法完全未考虑作业的紧迫程度\n3） 由于作业的长短只根据用户所提供的估计执行时间而定的，而用户又可能会有意或无意的缩短其作业的估计运行时间，致使该算法不一定能真正做到算作业优先调度。\n4） 注意：SJF调度算法的平均等待时间、平均周转时间最少。\n（3） 优先级调度算法 （4） 高响应比优先调度算法 高响应比优先调度算法主要用于作业调度。同时考虑从每个作业的等待时间和估计需要运行的时间。\n（5） 时间片轮转调度算法 时间片轮转调度算法主要适用于分时系统。\n（6） 多级反馈队列调度算法 多级反馈队列调度算法主要是时间片轮转调度算法和优先级调度算法的综合和发展。通过动态调整进程优先级和时间片大小，多级反馈队列调度算法可以兼顾多方面的系统目标。\n","permalink":"https://csqread.top/posts/tech/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/","summary":"进程的概念 在多道程序环境下，允许多个程序并发执行，此时他们将失去封闭性，并具有间断性和不可再现性的特征。为此引入了进程的概念，以便更好地描述","title":"进程与线程"},{"content":"有效数据生成以及插入数据库方案 先产生insert数据并存到备份文件中 因为有效数据生成的数量不大， 按照压测那边给我的需求大概每个交易4千笔数据左右，需要并发量比较高的交易也不过4万笔数据，涉及到转账的交易数据量比较高一点。并且需要做一个备份为了以后压测可以备用，因此我选择先生成到不同的表对应的表名文件中，然后再写一个批量执行SQL的程序执行这些文件中的insert语句\n下面的代码做了脱敏处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 import java.io.BufferedReader; import java.io.BufferedWriter; import java.io.FileReader; import java.io.FileWriter; import java.io.IOException; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.TimeUnit; import java.util.regex.Matcher; import java.util.regex.Pattern; public class MultiThreadScript { private static final int THREAD_POOL_SIZE = 4; // 线程池大小 private static final String INPUT_FILE_PATH = \u0026#34;input.sql\u0026#34;; // 输入文件路径 private static final String OUTPUT_FILE_PATH = \u0026#34;output.sql\u0026#34;; // 输出文件路径 private static final String INSERT_REGEX = \u0026#34;(?i)^insert into .* values\\\\s*\\\\((.*)\\\\);?$\u0026#34;; // insert语句的正则表达式 private static final String PK_REGEX = \u0026#34;\u0026#39;[0-9A-Za-z]+\u0026#39;\u0026#34;; // 主键的正则表达式 private static final int PK_INDEX = 0; // 主键在值列表中的索引 public static void main(String[] args) throws Exception { // 创建线程池 ExecutorService executor = Executors.newFixedThreadPool(THREAD_POOL_SIZE); try (BufferedReader reader = new BufferedReader(new FileReader(INPUT_FILE_PATH))) { String line; while ((line = reader.readLine()) != null) { if (isInsertStatement(line)) { executor.execute(new InsertTask(line)); } } } // 关闭线程池并等待所有任务完成 executor.shutdown(); executor.awaitTermination(1, TimeUnit.HOURS); } // 判断一行文本是否为insert语句 private static boolean isInsertStatement(String line) { return line.matches(INSERT_REGEX); } // 插入任务 private static class InsertTask implements Runnable { private final String originalSql; public InsertTask(String originalSql) { this.originalSql = originalSql; } @Override public void run() { try { // 提取主键 Pattern pkPattern = Pattern.compile(PK_REGEX); Matcher pkMatcher = pkPattern.matcher(originalSql); pkMatcher.find(); String originalPk = pkMatcher.group(); // 提取值列表 String valueList = originalSql.replaceAll(INSERT_REGEX, \u0026#34;$1\u0026#34;); String[] values = valueList.split(\u0026#34;,\u0026#34;); // 递增主键并生成新的SQL语句 StringBuilder newSqlBuilder = new StringBuilder(); for (int i = 0; i \u0026lt; 40000; i++) { String newPk = getNextPk(originalPk); String newValueList = valueList.replace(originalPk, newPk); String newSql = originalSql.replaceAll(valueList, newValueList); newSqlBuilder.append(newSql).append(\u0026#34;\\n\u0026#34;); } // 写入输出文件 synchronized (MultiThreadScript.class) { try (BufferedWriter writer = new BufferedWriter(new FileWriter(OUTPUT_FILE_PATH, true))) { writer.write(newSqlBuilder.toString()); } } } catch (IOException e) { e.printStackTrace(); } } // 获取下一个主键 private String getNextPk (String originalPk) { String prefix = originalPk.substring(0, originalPk.length() - 1); String suffix = originalPk.substring(originalPk.length() - 1); String newSuffix = getNextSuffix(suffix); return prefix + newSuffix; } // 获取下一个主键后缀 private String getNextSuffix(String suffix) { StringBuilder sb = new StringBuilder(); for (int i = 0; i \u0026lt; suffix.length(); i++) { char c = suffix.charAt(i); if (Character.isDigit(c)) { int digit = Character.getNumericValue(c); if (digit == 9) { sb.append(\u0026#39;A\u0026#39;); } else if (digit == 35) { sb.append(\u0026#39;a\u0026#39;); } else { sb.append(Character.forDigit(digit + 1, 36)); } } else if (Character.isLetter(c)) { if (c == \u0026#39;Z\u0026#39;) { sb.append(\u0026#39;0\u0026#39;); } else if (c == \u0026#39;z\u0026#39;) { sb.append(\u0026#39;0\u0026#39;); } else { sb.append((char) (c + 1)); } } else { sb.append(c); } } return sb.toString(); } } 上面的代码中，MultiThreadScript类是脚本的主类，它负责读取输入文件并创建线程池来处理每条insert语句。InsertTask类是插入任务类，它实现了Runnable接口，用于递增主键并生成新的SQL语句。为了避免多个线程同时写入输出文件，InsertTask类中使用了synchronized关键字来进行同步。\n在getNextPk()方法中，我使用了类似于Excel中列名的递增方式来递增主键。首先，将原始主键分为前缀和后缀两部分，其中前缀是主键的前面部分，后缀是主键的最后一位字符。然后，对后缀进行递增，并根据递增后的后缀重新生成新的主键。\n最后，需要注意的是，由于主键可能包含字母和数字，因此使用36进制来对主键进行递增。例如，对于主键值为\u0026quot;001\u0026quot;，它的下一个值为\u0026quot;002\u0026quot;；对于主键值为\u0026quot;AZ9\u0026quot;，它的下一个值为\u0026quot;BA0\u0026quot;。\n进行插入操作（脱敏处理后的代码）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 import java.io.BufferedReader; import java.io.FileReader; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; import java.sql.SQLException; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class InsertExecutor { private static final String URL = \u0026#34;jdbc:mysql://localhost:3306/mydatabase\u0026#34;; private static final String USER = \u0026#34;myuser\u0026#34;; private static final String PASSWORD = \u0026#34;mypassword\u0026#34;; private static final int THREAD_POOL_SIZE = 10; public static void main(String[] args) { try { // 读取insert语句文件 BufferedReader reader = new BufferedReader(new FileReader(\u0026#34;inserts.sql\u0026#34;)); String line; Queue\u0026lt;String\u0026gt; inserts = new LinkedList\u0026lt;\u0026gt;(); while ((line = reader.readLine()) != null) { inserts.add(line); } reader.close(); // 创建线程池 ExecutorService executorService = Executors.newFixedThreadPool(THREAD_POOL_SIZE); // 执行insert语句 while (!inserts.isEmpty()) { String insert = inserts.poll(); executorService.execute(new InsertWorker(insert)); } // 关闭线程池 executorService.shutdown(); } catch (IOException e) { e.printStackTrace(); } } static class InsertWorker implements Runnable { private String insert; public InsertWorker(String insert) { this.insert = insert; } @Override public void run() { try (Connection conn = DriverManager.getConnection(URL, USER, PASSWORD); PreparedStatement statement = conn.prepareStatement(insert)) { // 执行insert语句 statement.executeUpdate(); } catch (SQLException e) { // 主键冲突，跳过该语句 if (e.getErrorCode() == 1062) { System.out.println(\u0026#34;Skip duplicate insert: \u0026#34; + insert); } else { e.printStackTrace(); } } } } } 上面代码中我们把insert.sql取代为我们想要进行批量insert的sql文件即可，线程数量可根据CPU的情况来看，在不进行其他工作任务的情况下，可尽量压榨CPU的使用率以达到最高的效率。\n亿级别的无效数据生成并插入数据库方案 这里因为涉及到的数据量特别大， 一般是模拟生产环境，因此一张表可能有千万级别以及亿级别的数据量，因此我选择一边生成一边做insert操作。也就是一个生产者一个消费者，当然，这里都是多线程来操作的。一开始我是每次达到20个事务一次提交的，后来换了OceanBase后，只能一次提交一个事务了，效率也变满了一点点. 对于多线程插入数据库，将生成的SQL语句分配给多个线程，每个线程使用单独的数据库连接插入数据库，可以使用线程池来管理多个线程\n下面是脱敏后的代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 import java.io.BufferedReader; import java.io.FileReader; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; import java.sql.SQLException; import java.util.concurrent.BlockingQueue; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.LinkedBlockingQueue; import java.util.concurrent.TimeUnit; public class MultiThreadedSqlInsert { // 数据库连接信息 private static final String DB_URL = \u0026#34;jdbc:mysql://localhost:3306/mydb\u0026#34;; private static final String DB_USER = \u0026#34;root\u0026#34;; private static final String DB_PASSWORD = \u0026#34;mypassword\u0026#34;; // 主键列名和初始值 private static final String PK_COLUMN_NAME = \u0026#34;id\u0026#34;; private static final String PK_INITIAL_VALUE = \u0026#34;1000\u0026#34;; // 线程数和每个线程处理的主键值个数 private static final int THREAD_COUNT = 10; private static final int KEYS_PER_THREAD = 10000000; // 文件名和队列大小 private static final String FILE_NAME = \u0026#34;data.sql\u0026#34;; private static final int QUEUE_SIZE = 10000; public static void main(String[] args) throws Exception { // 读取文件中的 SQL 语句 String sql = readSqlFromFile(FILE_NAME); // 创建线程池和队列 ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT); BlockingQueue\u0026lt;String\u0026gt; queue = new LinkedBlockingQueue\u0026lt;\u0026gt;(QUEUE_SIZE); // 创建多个线程，为每个线程分配一段主键值的区间 for (int i = 0; i \u0026lt; THREAD_COUNT; i++) { int start = i * KEYS_PER_THREAD; int end = (i + 1) * KEYS_PER_THREAD - 1; executor.submit(new SqlGenerator(sql, start, end, queue)); } // 创建多个数据库连接，为每个连接分配一个线程 for (int i = 0; i \u0026lt; THREAD_COUNT; i++) { Connection conn = DriverManager.getConnection(DB_URL, DB_USER, DB_PASSWORD); executor.submit(new SqlExecutor(conn, queue)); } // 等待所有线程执行完毕 executor.shutdown(); executor.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS); } // 从文件中读取 SQL 语句 private static String readSqlFromFile(String fileName) throws Exception { try (BufferedReader reader = new BufferedReader(new FileReader(fileName))) { StringBuilder sb = new StringBuilder(); String line; while ((line = reader.readLine()) != null) { sb.append(line).append(\u0026#34;\\n\u0026#34;); } return sb.toString(); } } // 生成新的 SQL 语句 private static String generateSql(String sql, int key) { String pkValue = PK_INITIAL_VALUE + key; return sql.replaceFirst(PK_COLUMN_NAME, pkValue); } // 生成新的 SQL 语句的线程 private static class SqlGenerator implements Runnable { private final String sql; private final int start; private final int end; private final BlockingQueue\u0026lt;String\u0026gt; queue; public SqlGenerator(String sql, int start, int end, BlockingQueue\u0026lt;String\u0026gt; queue) { this.sql = sql; this.start = start; this.end = end; this.queue = queue; } @Override public void run() { for (int i = start; i \u0026lt;= end; i++) { String newSql = generateSql(sql, i); try { queue.put(newSql); } catch (InterruptedException e) { Thread.currentThread().interrupt(); return; } } } } // 执行 SQL 语句的线程 private static class SqlExecutor implements Runnable { private final Connection conn; private final BlockingQueue\u0026lt;String\u0026gt; queue; public SqlExecutor(Connection conn, BlockingQueue\u0026lt;String\u0026gt; queue) { this.conn = conn; this.queue = queue; } @Override public void run() { try (PreparedStatement stmt = conn.prepareStatement(\u0026#34;\u0026#34;)) { while (true) { String sql = queue.take(); if (sql == null) { break; } stmt.addBatch(sql); if (stmt.getBatchSize() \u0026gt;= 1000) { stmt.executeBatch(); } } stmt.executeBatch(); } catch (SQLException | InterruptedException e) { e.printStackTrace(); } } } 这里使用了两个线程池，一个用于生成新的 SQL 语句，一个用于执行 SQL 语句。生成 SQL 语句的线程将生成的 SQL 语句存储到一个线程安全的队列中，执行 SQL 语句的线程从队列中取出 SQL 语句并执行插入操作。程序使用了 JDBC 连接 MySQL 数据库，并使用了 PreparedStatement 批量执行 SQL 语句，以提高插入效率。\n需要注意的是，为了避免多个线程同时操作数据库导致数据不一致的问题，每个线程使用了自己的数据库连接。此外，程序还使用了线程安全的队列和加锁机制来保证线程安全。\n总结 以上就是我在工作中遇到的问题之一，做一个小结，用到了很多线程和线程池的地方，以及操作数据库相关的知识。\n","permalink":"https://csqread.top/posts/tech/%E5%8E%8B%E6%B5%8B%E6%95%B0%E6%8D%AE%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90%E5%B9%B6%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%80%BB%E7%BB%93/","summary":"有效数据生成以及插入数据库方案 先产生insert数据并存到备份文件中 因为有效数据生成的数量不大， 按照压测那边给我的需求大概每个交易4千笔数据","title":"压测数据快速生成并插入数据库总结"},{"content":"JVM 内存结构 Java 虚拟机的内存空间分为 5 个部分：\n程序计数器 Java虚拟机栈 本地方法栈 堆 方法区 JDK 1.8 同 JDK 1.7 比，最大的差别就是：元数据区取代了永久代。元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元数据空间并不在虚拟机中，而是使用本地内存。\n程序计数器(PC寄存器) 程序计数器的定义 程序计数器是一块较小的内存空间，是当前线程正在执行的那条字节码指令的地址。若当前线程正在执行的是一个本地方法，那么此时程序计数器为Undefined。在Java虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制的指示器，分支，循环，跳转、异常处理、线程恢复等基础功能都需要这个计数器来完成。\n程序计数器的作用 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制。 在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次线程执行到哪了 ","permalink":"https://csqread.top/posts/tech/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/","summary":"JVM 内存结构 Java 虚拟机的内存空间分为 5 个部分： 程序计数器 Java虚拟机栈 本地方法栈 堆 方法区 JDK 1.8 同 JDK 1.7 比，最大的差别就是：元数据区取代了永久代。元","title":"Java虚拟机知识总结"},{"content":"什么是Netty 1、Netty 是一个 基于 NIO 的 client-server(客户端服务器)框架，使用它可以快速简单地开发网络应用程序。 2、它极大地简化并优化了 TCP 和 UDP 套接字服务器等网络编程,并且性能以及安全性等很多方面甚至都要更好。 3、支持多种协议 如 FTP，SMTP，HTTP 以及各种二进制和基于文本的传统协议。 用官方的总结就是：Netty 成功地找到了一种在不妥协可维护性和性能的情况下实现易于开发，性能，稳定性和灵活性的方法。\n除了上面之外，很多开源项目比如我们常用的 Dubbo、RocketMQ、Elasticsearch、gRPC 等等都用到了 Netty\n相比于直接使用 JDK 自带的 NIO 相关的 API 来说更加易用。 统一的 API，支持多种传输类型，阻塞和非阻塞的。 简单而强大的线程模型。 自带编解码器解决 TCP 粘包/拆包问题。 自带各种协议栈。 真正的无连接数据包套接字支持。 比直接使用 Java 核心 API 有更高的吞吐量、更低的延迟、更低的资源消耗和更少的内存复制。 安全性不错，有完整的 SSL/TLS 以及 StartTLS 支持。 社区活跃、成熟稳定，经历了大型项目的使用和考验，而且很多开源项目都使用到了 Netty， 比如我们经常接触的 Dubbo、RocketMQ 等等。 应用场景 NIO 可以做的事情 ，使用 Netty 都可以做并且更好。Netty 主要用来做网络通信 : 作为 RPC 框架的网络通信工具 ：我们在分布式系统中，不同服务节点之间经常需要相互调用，这个时候就需要 RPC 框架了。不同服务节点之间的通信是如何做的呢？可以使用 Netty 来做。比如我调用另外一个节点的方法的话，至少是要让对方知道我调用的是哪个类中的哪个方法以及相关参数吧！ 实现一个自己的 HTTP 服务器 ：通过 Netty 我们可以自己实现一个简单的 HTTP 服务器，这个大家应该不陌生。说到 HTTP 服务器的话，作为 Java 后端开发，我们一般使用 Tomcat 比较多。一个最基本的 HTTP 服务器可要以处理常见的 HTTP Method 的请求，比如 POST 请求、GET 请求等等。 实现一个即时通讯系统 ：使用 Netty 我们可以实现一个可以聊天类似微信的即时通讯系统， 实现消息推送系统 ：市面上有很多消息推送系统都是基于 Netty 来做的。 Netty 的高性能表现 心跳，对服务端：会定时清除闲置会话 inactive(netty5)，对客户端:用来检测会话是否断开，是否重来，检测网络延迟，其中 idleStateHandler 类 用来检测会话状态 串行无锁化设计，即消息的处理尽可能在同一个线程内完成，期间不进行线程切换，这样就避免了多线程竞争和同步锁。表面上看，串行化设计似乎 CPU 利用率不高，并发程度不够。但是，通过调整 NIO 线程池的线程参数，可以同时启动多个串行化的线程并行运行，这种局部无锁化的串行线程设计相比一个队列-多个工作线程模型性能更优。 可靠性，链路有效性检测：链路空闲检测机制，读/写空闲超时机制；内存保护机制：通过内存池重用 ByteBuf;ByteBuf 的解码保护；优雅停机：不再接收新消息、退出前的预处理操作、资源的释放操作。 Netty 安全性：支持的安全协议：SSL V2 和 V3，TLS，SSL 单向认证、双向认证和第三方 CA认证。 高效并发编程的体现：volatile 的大量、正确使用；CAS 和原子类的广泛使用；线程安全容器的使用；通过读写锁提升并发性能。IO 通信性能三原则：传输（AIO）、协议（Http）、线程（主从多线程） 流量整型的作用（变压器）：防止由于上下游网元性能不均衡导致下游网元被压垮，业务流中断；防止由于通信模块接受消息过快，后端业务线程处理不及时导致撑死问题 Netty核心组件 Bootstrap和ServerBootstrap 当需要连接客户端或者服务器绑定指定端口是需要使用Bootstrap，ServerBootstrap有两种类型，一种是用于客户端的Bootstrap，一种是用于服务端 的ServerBootstrap。不管程序使用哪种协议，无论是创建一个客户端还是服务器都需要使 用“引导”。\nBootstrap 是客户端的启动引导类/辅助类\n1 2 3 4 5 6 7 8 9 10 11 12 13 EventLoopGroup group = new NioEventLoopGroup(); try { //创建客户端启动引导/辅助类： Bootstrap Bootstrap b = new Bootstrap(); //指定线程模型 b.group(group). ...... // 尝试建立连接 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); } finally { // 优雅关闭相关线程组资源 group.shutdownGracefully(); } ServerBootstrap 客户端的启动引导类/辅助类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 1.bossGroup 用于接收连接，workerGroup 用于具体的处理 EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //2.创建服务端启动引导/辅助类： ServerBootstrap ServerBootstrap b = new ServerBootstrap(); //3.给引导类配置两大线程组,确定了线程模型 b.group(bossGroup, workerGroup). ...... // 6.绑定端口 ChannelFuture f = b.bind(port).sync(); // 等待连接关闭 f.channel().closeFuture().sync(); } finally { //7.优雅关闭相关线程组资源 bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } Bootstrap 通常使用 connet() 方法连接到远程的主机和端口，作为一个 Netty TCP 协议通信中的客户端。另外，Bootstrap 也可以通过 bind() 方法绑定本地的一个端口，作为 UDP 协议通信中的一端。 ServerBootstrap通常使用 bind() 方法绑定本地的端口上，然后等待客户端的连接。\nBootstrap 只需要配置一个线程组— EventLoopGroup，而 ServerBootstrap需要配置两个线程组— EventLoopGroup ，一个用于接收连接，一个用于具体的处理。\n一个 ServerBootstrap 可以认为有2个 Channel 集合，\n第一个集合包含一个单例 ServerChannel，代表持有一个绑定了本地端口的 socket;\n第二集合包含所有创建的 Channel，处理服务器所接收到的客户端进来的连接。\nEventLoop和EventLoopGroup EventLoop 定义了 Netty 的核心抽象，用于处理连接的生命周期中所发生的事件。\nEventLoop 的主要作用实际就是负责监听网络事件并调用事件处理器进行相关 I/O 操作的处理。\nChannel 和 EventLoop 直接有啥联系呢？\nChannel 为 Netty 网络操作(读写等操作)抽象类，EventLoop 负责处理注册到其上的Channel 处理 I/O 操作，两者配合参与 I/O 操作。\nEventLoopGroup包含多个EventLoop，每个EventLoop通常内部包含一个线程。EventLoop在处理IO事件时在自己的Thread线程上进行，从而保证线程安全\nNioEventLoopGroup在未指定线程数时，默认时当前cpu线程数*2\nEventLoopGroup 是一组 EventLoop 的抽象，Netty 为了更好的利用多核 CPU 资源，一般会有多个 EventLoop 同时工作，每个 EventLoop 维护着一个 Selector 实例。 EventLoopGroup 提供 next 接口，可以从组里面按照一定规则获取其中一个EventLoop来处理任务。在 Netty 服务器端编程中，我们一般都需要提供两个EventLoopGroup，例如:BossEventLoopGroup 和 WorkerEventLoopGroup。 通常一个服务端口即一个ServerSocketChannel对应一个Selector和一个EventLoop 线程。BossEventLoop 负责接收客户端的连接并将 SocketChannel 交给 WorkerEventLoopGroup 来进行 IO 处理\nBossEventLoopGroup 通常是一个单线程的 EventLoop，EventLoop 维护着一个注册了ServerSocketChannel 的Selector 实例BossEventLoop 不断轮询Selector 将连接事件分离出来 通常是 OP_ACCEPT 事件，然后将接收到的 SocketChannel 交给WorkerEventLoopGroup WorkerEventLoopGroup 会由 next 选择其中一个 EventLoop来将这个SocketChannel 注册到其维护的Selector 并对其后续的 IO 事件进行处理 EventLoop继承图\nChannel通道 Channel 接口是 Netty 对网络操作抽象类，它除了包括基本的 I/O 操作，如 bind()、connect()、read()、write() 等。\n比较常用的Channel接口实现类是NioServerSocketChannel（服务端）和NioSocketChannel（客户端），这两个 Channel 可以和 BIO 编程模型中的ServerSocket以及Socket两个概念对应上。Netty 的 Channel 接口所提供的 API，大大地降低了直接使用 Socket 类的复杂性。\n1 2 3 4 5 6 7 Channel channel = ...; // 获取channel的引用 ByteBuf buf = Unpooled.copiedBuffer(\u0026#34;your data\u0026#34;, CharsetUtil.UTF_8); //1 ChannelFuture cf = channel.writeAndFlush(buf); //2 cf.addListener(new ChannelFutureListener() { //3 @Override public void operationComplete(ChannelFuture future) { if (future.isSuccess()) { //4 } }); 创建 ByteBuf 保存写的数据 写数据，并刷新 添加 ChannelFutureListener 即可写操作完成后收到通知 写操作没有错误完成 写操作完成时出现错误 channel声明周期 | 状态 | 描述 | | —- | —- | | ChannelUnregistered | Channel 已经被创建，但还未注册到EventLoop | | ChannelRegistered | Channel 已经被注册到了EventLoop | | ChannelActive | Channel 处于活动状态（已经连接到它的远程节点）。它现在可以接收和发送数据了 | | ChannelInactive | Channel 没有连接到远程节点 |\nselector 作用：\nI/O 的就绪与选择 是 NIO 网络编程的基础 SelectonKey 状态 OP_ACCEPT 操作集位用于插座接受操作。 OP_CONNECT 用于套接字连接操作的操作集位。 OP_READ 读操作的操作位。 OP_WRITE 写操作的操作位。 1 2 3 4 5 Selector selector = new Selector.open() SelectorKey selectorKey = channel.register(selector, SelectionKey.OP_READ); int selectNum = selector.select(); Set\u0026lt;Selection\u0026gt; selectionkeys = selector.selectdkeys(); ","permalink":"https://csqread.top/posts/tech/netty%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/","summary":"什么是Netty 1、Netty 是一个 基于 NIO 的 client-server(客户端服务器)框架，使用它可以快速简单地开发网络应用程序。 2、它极大","title":"Netty相关总结"},{"content":"面试题目 记录下社招面试拼多多的总结与心得，以及失败的原因吧。\n1、服务注册是如何发现的，eureka的基本原理， 容器ip是动态的还是静态的\n答：eureka, 基本原理大概说个一些：包括服务注册，服务发现，心跳机制，服务下线， 自我保护机制等等。\n2、转账，支付如何保证数据一致性的， 说一下分布式事务的实现， 消息的生产和消费机制。\n这个基本上说出个一二三来。其实就是分布式事务，保证这个数据的一致性就是要保证事务的原子性。即，事务要么全部成功，要么全部失败。我就提了下XA协议和TCC模式，具体如何实现的我也不太清楚。\n3、mysql 索引优化，子查询优化\n这里基本上都讲出来了。之前做过很多压测，包括让sql走上索引，参数表添加缓存等等。\n4、线上有排查过什么问题.\n5、MQ的实现原理\n5、图算法题\n还有些问题已经忘了。\n总结与回顾 其实关于这次面试，我还是没有做好完全的准备，而且是近三年以来的第一次面试，心里难免还是有点紧张。导致我有些东西知道的知识可能一时半会想不起来。后面把这些问到的知识点再复习一下。基本上只是浅浅的了解了一下，细说一下底层原理我就懵了。大概知道我们有这么个流程，知道哪里出了问题该找谁来看。因为现在吧，大公司基本上就是这么个情况， 包括中间件团队，数据库团队，DTF团队，DCF团队等等。基本上我们只用知道这些东西有，然后找相应团队的负责人帮忙看下问题就能解决。我们都是在脚手架上做着CURD。\n但是还是要把面试问到的东西基本原理做一个小小的总结和记录:\nEureka Eureka是Netflix开源的一款提供服务注册和发现的产品， 开源地址为 Eureka, 注册中心是分布式开发的核心组件之一\n而eureka是spring cloud推荐的注册中心实现, Eureka是一个REST (Representational State Transfer)服务 它主要用于AWS云，用于定位服务，以实现中间层服务器的负载平衡和故障转移，我们称此服务为Eureka服务器\nEureka也有一个基于java的客户端组件，Eureka客户端，这使得与服务的交互更加容易，同时客户端也有一个内置的负载平衡器，它执行基本的循环负载均衡。\n自我保护机制 自我保护机制主要在Eureka Client和Eureka Server之间存在网络分区的情况下发挥保护作用，在服务器端和客户端都有对应实现.\n假设在某种特定的情况下（如网络故障）, Eureka Client和Eureka Server无法进行通信，此时Eureka Client无法向Eureka Server发起注册和续约请求，Eureka Server中就可能因注册表中的服务实例租约出现大量过期而面临被剔除的危险，然而此时的Eureka Client可能是处于健康状态的（可接受服务访问），如果直接将注册表中大量过期的服务实例租约剔除显然是不合理的，自我保护机制提高了eureka的服务可用性。\n当自我保护机制触发时，Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务，仍能查询服务信息并且接受新服务注册请求，也就是其他功能是正常的。\n这里思考下，如果eureka节点A触发自我保护机制过程中，有新服务注册了然后网络回复后，其他peer节点能收到A节点的新服务信息，数据同步到peer过程中是有网络异常重试的，也就是说，是能保证最终一致性的。\n服务发现原理 eureka server可以集群部署，多个节点之间会进行（异步方式）数据同步，保证数据最终一致性，Eureka Server作为一个开箱即用的服务注册中心，提供的功能包括：服务注册、接收服务心跳、服务剔除、服务下线等。\n需要注意的是，Eureka Server同时也是一个Eureka Client，在不禁止Eureka Server的客户端行为时，它会向它配置文件中的其他Eureka Server进行拉取注册表、服务注册和发送心跳等操作。\neureka server端通过appName和instanceInfoId来唯一区分一个服务实例，服务实例信息是保存在哪里呢？其实就是一个Map中：\n1 2 // 第一层的key是appName，第二层的key是instanceInfoIdprivate final ConcurrentHashMap\u0026lt;String, Map\u0026lt;String, Lease\u0026lt;InstanceInfo\u0026gt;\u0026gt;\u0026gt; registry = new ConcurrentHashMap\u0026lt;String, Map\u0026lt;String, Lease\u0026lt;InstanceInfo\u0026gt;\u0026gt;\u0026gt;(); 服务注册 Service Provider启动时会将服务信息（InstanceInfo）发送给eureka server，eureka server接收到之后会写入registry中，服务注册默认过期时间DEFAULT_DURATION_IN_SECS = 90秒。InstanceInfo写入到本地registry之后，然后同步给其他peer节点，对应方法com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#replicateToPeers。\n写入本地redistry 服务信息（InstanceInfo）保存在Lease中，写入本地registry对应方法com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#register，Lease统一保存在内存的ConcurrentHashMap中，在服务注册过程中，首先加个读锁，然后从registry中判断该Lease是否已存在，如果已存在则比较lastDirtyTimestamp时间戳，取二者最大的服务信息，避免发生数据覆盖。使用InstanceInfo创建一个新的InstanceInfo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 if (existingLastDirtyTimestamp \u0026gt; registrationLastDirtyTimestamp) { // 已存在Lease则比较时间戳，取二者最大值 registrant = existingLease.getHolder(); } Lease\u0026lt;InstanceInfo\u0026gt; lease = new Lease\u0026lt;InstanceInfo\u0026gt;(registrant, leaseDuration); if (existingLease != null) { // 已存在Lease则取上次up时间戳 lease.setServiceUpTimestamp(existingLease.getServiceUpTimestamp()); } public Lease(T r, int durationInSecs) { holder = r; registrationTimestamp = System.currentTimeMillis(); // 当前时间 lastUpdateTimestamp = registrationTimestamp; duration = (durationInSecs * 1000); } 同步给其他peer InstanceInfo写入到本地registry之后，然后同步给其他peer节点，对应方法com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#replicateToPeers。如果当前节点接收到的InstanceInfo本身就是另一个节点同步来的，则不会继续同步给其他节点，避免形成“广播效应”；InstanceInfo同步时会排除当前节点。\nInstanceInfo的状态有依以下几种：Heartbeat, Register, Cancel, StatusUpdate, DeleteStatusOverride，默认情况下同步操作时批量异步执行的，同步请求首先缓存到Map中，key为requestType+appName+id，然后由发送线程将请求发送到peer节点。\nPeer之间的状态是采用异步的方式同步的，所以不保证节点间的状态一定是一致的，不过基本能保证最终状态是一致的。结合服务发现的场景，实际上也并不需要节点间的状态强一致。在一段时间内（比如30秒），节点A比节点B多一个服务实例或少一个服务实例，在业务上也是完全可以接受的（Service Consumer侧一般也会实现错误重试和负载均衡机制）。所以按照CAP理论，Eureka的选择就是放弃C，选择AP。 如果同步过程中，出现了异常怎么办呢，这时会根据异常信息做对应的处理，如果是读取超时或者网络连接异常，则稍后重试；如果其他异常则打印错误日志不再后续处理。\n服务续约 Renew（服务续约）操作由Service Provider定期调用，类似于heartbeat。主要是用来告诉Eureka Server Service Provider还活着，避免服务被剔除掉。renew接口实现方式和register基本一致：首先更新自身状态，再同步到其它Peer，服务续约也就是把过期时间设置为当前时间加上duration的值。\n注意：服务注册如果InstanceInfo不存在则加入，存在则更新；而服务预约只是进行更新，如果InstanceInfo不存在直接返回false。\n服务失效剔除 Eureka Server中有一个EvictionTask，用于检查服务是否失效。Eviction（失效服务剔除）用来定期（默认为每60秒）在Eureka Server检测失效的服务，检测标准就是超过一定时间没有Renew的服务。默认失效时间为90秒，也就是如果有服务超过90秒没有向Eureka Server发起Renew请求的话，就会被当做失效服务剔除掉。失效时间可以通过eureka.instance.leaseExpirationDurationInSeconds进行配置，定期扫描时间可以通过eureka.server.evictionIntervalTimerInMs进行配置。\n服务剔除#evict方法中有很多限制，都是为了保证Eureka Server的可用性：比如自我保护时期不能进行服务剔除操作、过期操作是分批进行、服务剔除是随机逐个剔除，剔除均匀分布在所有应用中，防止在同一时间内同一服务集群中的服务全部过期被剔除，以致大量剔除发生时，在未进行自我保护前促使了程序的崩溃。\n服务信息拉取 Eureka consumer服务信息的拉取分为全量式拉取和增量式拉取，eureka consumer启动时进行全量拉取，运行过程中由定时任务进行增量式拉取，如果网络出现异常，可能导致先拉取的数据被旧数据覆盖（比如上一次拉取线程获取结果较慢，数据已更新情况下使用返回结果再次更新，导致数据版本落后），产生脏数据。对此，eureka通过类型AtomicLong的fetchRegistryGeneration对数据版本进行跟踪，版本不一致则表示此次拉取到的数据已过期。\nfetchRegistryGeneration过程是在拉取数据之前，执行fetchRegistryGeneration.get获取当前版本号，获取到数据之后，通过fetchRegistryGeneration.compareAndSet来判断当前版本号是否已更新。 注意：如果增量式更新出现意外，会再次进行一次全量拉取更新。\nEureka server的伸缩容 Eureka Server是怎么知道有多少Peer的呢？Eureka Server在启动后会调用EurekaClientConfig.getEurekaServerServiceUrls来获取所有的Peer节点，并且会定期更新。定期更新频率可以通过eureka.server.peerEurekaNodesUpdateIntervalMs配置。\n这个方法的默认实现是从配置文件读取，所以如果Eureka Server节点相对固定的话，可以通过在配置文件中配置来实现。如果希望能更灵活的控制Eureka Server节点，比如动态扩容/缩容，那么可以override getEurekaServerServiceUrls方法，提供自己的实现，比如我们的项目中会通过数据库读取Eureka Server列表。\neureka server启动时把自己当做是Service Consumer从其它Peer Eureka获取所有服务的注册信息。然后对每个服务信息，在自己这里执行Register，isReplication=true，从而完成初始化。\nService Provider Service Provider启动时首先时注册到Eureka Service上，这样其他消费者才能进行服务调用，除了在启动时之外，只要实例状态信息有变化，也会注册到Eureka Service。需要注意的是，需要确保配置eureka.client.registerWithEureka=true。register逻辑在方法AbstractJerseyEurekaHttpClient.register中，Service Provider会依次注册到配置的Eureka Server Url上，如果注册出现异常，则会继续注册其他的url。\nRenew操作会在Service Provider端定期发起，用来通知Eureka Server自己还活着。 这里instance.leaseRenewalIntervalInSeconds属性表示Renew频率。默认是30秒，也就是每30秒会向Eureka Server发起Renew操作。这部分逻辑在HeartbeatThread类中。在Service Provider服务shutdown的时候，需要及时通知Eureka Server把自己剔除，从而避免客户端调用已经下线的服务，逻辑本身比较简单，通过对方法标记@PreDestroy，从而在服务shutdown的时候会被触发。\nService Consumer Service Consumer这块的实现相对就简单一些，因为它只涉及到从Eureka Server获取服务列表和更新服务列表。Service Consumer在启动时会从Eureka Server获取所有服务列表，并在本地缓存。需要注意的是，需要确保配置eureka.client.shouldFetchRegistry=true。由于在本地有一份Service Registries缓存，所以需要定期更新，定期更新频率可以通过eureka.client.registryFetchIntervalSeconds配置。\n总结 我们为什么要使用Eureka呢，在分布式开发架构中， 任何单点的服务都不能保证不会中断，因此需要服务发现机制，某个节点中断后，服务消费者能及时感知到保证服务高可用。注册中心除了Eureka之外，还有Zookeeper、consul、nacos等解决方案，实现原理不同， 各自适用于不同业务场景。\n数据一致性问题 事务 严格意义上的事务实现应该是具备原子性、一致性、隔离性和持久性，简称ACID。\n原子性(Atomicity) ， 可以理解为一个事务内的所有操作要么都执行，要么都不执行。 一致性(Consistency)， 数据是满足完整性约束的，也就是不会存在中间状态的数据，比如说你账户上有400， 我账户上有100， 你给我打200块，此时你账户上的钱应该是200， 我账户上的钱应该是300， 不会存在我账户上的钱加了，你账户上的钱没扣的中间状态 隔离性(Lsolation) ，指的是多个事务并发执行的时候不会互相干扰，即事务内部的数据对于其他事务来说是隔离的 持久性(Durability), 指的是一个事务完成了之后数据就被永远保存下来，之后的其他操作或故障都不会对事务的结果产生影响 而通俗意义上事务就是为了使得一些更新操作要么都成功，要么都失败。\n分布式事务 分布式事务顾名思义就是要在分布式系统中实现事务，它其实是由多个本地事务组合而成。\n对于分布式事务而言几乎满足不了ACID，其实对于单机事务而言大部分情况下也没有满足ACID，不然怎么会有四种隔离级别呢？所以更不用说分布在不用数据库或者不同应用上的分布式事务了。\n2PC 2PC（Two-phase commit protocol），中文叫二阶段提交。 二阶段提交是一种强一致性设计，2PC 引入一个事务协调者的角色来协调管理各参与者（也可称之为各本地资源）的提交和回滚，二阶段分别指的是准备（投票）和提交两个阶段。\n注意这只是协议或者说是理论指导，只阐述了大方向，具体落地还是有会有差异的。\n让我们来看下两个阶段的具体流程。\n准备阶段协调者会给各参与者发送准备命令，你可以把准备命令理解成除了提交事务之外啥事都做完了。\n同步等待所有资源的响应之后就进入第二阶段即提交阶段（注意提交阶段不一定是提交事务，也可能是回滚事务）。\n假如在第一阶段所有参与者都返回准备成功，那么协调者则向所有参与者发送提交事务命令，然后等待所有事务都提交成功之后，返回事务执行成功。\n假如在第一阶段有一个参与者返回失败，那么协调者就会向所有参与者发送回滚事务的请求，即分布式事务执行失败。\n那第二阶段提交失败的话呢？\n这里有两种情况。\n第一种是第二阶段执行的是回滚事务操作，那么答案是不断重试，直到所有参与者都回滚了，不然那些在第一阶段准备成功的参与者会一直阻塞着。\n第二种是第二阶段执行的是提交事务操作，那么答案也是不断重试，因为有可能一些参与者的事务已经提交成功了，这个时候只有一条路，就是头铁往前冲，不断的重试，直到提交成功，到最后真的不行只能人工介入处理。\n大体上二阶段提交的流程就是这样，我们再来看看细节。\n首先 2PC 是一个同步阻塞协议，像第一阶段协调者会等待所有参与者响应才会进行下一步操作，当然第一阶段的协调者有超时机制，假设因为网络原因没有收到某参与者的响应或某参与者挂了，那么超时后就会判断事务失败，向所有参与者发送回滚命令。\n在第二阶段协调者的没法超时，因为按照我们上面分析只能不断重试！\n协调者故障分析 协调者是一个单点，存在单点故障问题\n假设协调者在发送准备命令之前挂了， 还行，等于事务没开始。\n假设协调者在发送准备命令之后挂了，这就不太行了，有些参与者等于都执行了处于事务资源锁定的状态。不仅事务执行不下去，还会因为锁定了一些公共资源而阻塞系统其他操作。\n假设协调者在发送事务回滚命令之前挂了，那么事务也是执行不下去，且在第一阶段那些准备成功参与者都阻塞着。\n假设协调者在发送回滚事务命令之后挂了，这个还行，至少命令发出去了，很大概率都会回滚成功，资源都会释放。但是如果出现网络分区问题，某些参与者将因为收不到命令而阻塞着。\n假设协调者在发送提交事务命令之前挂了，这个不行，这下所有资源都阻塞着。\n假设协调者在发送提交事务命令之后挂了，很大概率都会提交成功，然后释放资源。但是如果出现网络分区问题某些参与者因为收不到命令而阻塞着。\n协调者故障，通过选举得到新的协调者 因为协调者单点问题，因此我们可以通过选举等操作选出一个新协调者来顶替。\n如果处于第一阶段，其实影响不大都回滚好了，在第一阶段事务肯定还没提交。\n如果处于第二阶段，假设参与者都没挂，此时新协调者可以向所有参与者确认它们自身情况来推断下一步的操作。\n假设有个别参与者挂了！这就有点僵硬了，比如协调者发送了回滚命令，此时第一个参与者收到了并执行，然后协调者和第一个参与者都挂了。\n此时其他参与者都没收到请求，然后新协调者来了，它询问其他参与者都说OK，但它不知道挂了的那个参与者到底O不OK，所以它傻了。\n问题其实就出在每个参与者自身的状态只有自己和协调者知道，因此新协调者无法通过在场的参与者的状态推断出挂了的参与者是什么情况。\n虽然协议上没说，不过在实现的时候我们可以灵活的让协调者将自己发过的请求在哪个地方记一下，也就是日志记录，这样新协调者来的时候不就知道此时该不该发了？\n但是就算协调者知道自己该发提交请求，那么在参与者也一起挂了的情况下没用，因为你不知道参与者在挂之前有没有提交事务。\n如果参与者在挂之前事务提交成功，新协调者确定存活着的参与者都没问题，那肯定得向其他参与者发送提交事务命令才能保证数据一致。\n如果参与者在挂之前事务还未提交成功，参与者恢复了之后数据是回滚的，此时协调者必须是向其他参与者发送回滚事务命令才能保持事务的一致。\n所以说极端情况下还是无法避免数据不一致问题。\ntalk is cheap 让我们再来看下代码，可能更加的清晰。以下代码取自 Distributed System: Principles and Paradigms。\n这个代码就是实现了 2PC，但是相比于2PC增加了写日志的动作、参与者之间还会互相通知、参与者也实现了超时。这里要注意，一般所说的2PC，不含上述功能，这都是实现的时候添加的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 协调者: write START_2PC to local log; //开始事务 multicast VOTE_REQUEST to all participants; //广播通知参与者投票 while not all votes have been collected { wait for any incoming vote; if timeout { //协调者超时 write GLOBAL_ABORT to local log; //写日志 multicast GLOBAL_ABORT to all participants; //通知事务中断 exit; } record vote; } //如果所有参与者都ok if all participants sent VOTE_COMMIT and coordinator votes COMMIT { write GLOBAL_COMMIT to local log; multicast GLOBAL_COMMIT to all participants; } else { write GLOBAL_ABORT to local log; multicast GLOBAL_ABORT to all participants; } 参与者: write INIT to local log; //写日志 wait for VOTE_REQUEST from coordinator; if timeout { //等待超时 write VOTE_ABORT to local log; exit; } if participant votes COMMIT { write VOTE_COMMIT to local log; //记录自己的决策 send VOTE_COMMIT to coordinator; wait for DECISION from coordinator; if timeout { multicast DECISION_REQUEST to other participants; //超时通知 wait until DECISION is received; /* remain blocked*/ write DECISION to local log; } if DECISION == GLOBAL_COMMIT write GLOBAL_COMMIT to local log; else if DECISION == GLOBAL_ABORT write GLOBAL_ABORT to local log; } else { write VOTE_ABORT to local log; send VOTE_ABORT to coordinator; } 每个参与者维护一个线程处理其它参与者的DECISION_REQUEST请求： while true { wait until any incoming DECISION_REQUEST is received; read most recently recorded STATE from the local log; if STATE == GLOBAL_COMMIT send GLOBAL_COMMIT to requesting participant; else if STATE == INIT or STATE == GLOBAL_ABORT; send GLOBAL_ABORT to requesting participant; else skip; /* participant remains blocked */ } 至此已经详细分析了2PC的各种细节，总结如下：\n2PC是一种尽量保证强一致性的分布式事务，因此它是同步阻塞的，而同步阻塞就导致长久的资源锁定问题，总体而言效率低，并且存在单点故障问题，在极端条件下存在数据不一致的风险。\n当然具体的实现可以变形，比如Tree 2PC、Dynamic 2PC\n2PC适用于数据库层面的分布式事务场景，而我们业务需求有时候不仅仅关乎数据库，也有可能是上传一张图片或者发送一条短信。\n而且像Java中的JTA, 它是基于XA规范实现的事务接口，这里的XA可以简单理解为基于数据库的XA规范来实现的2PC。\n解决方案 XA方案 2PC的传统方案是在数据库层面实现的，如 Oracle、MySQL 都支持 2PC 协议，为了统一标准减少行业内不必要的对接成本，需要制定标准化的处理模型及接口标准，国际开放标准组织 Open Group 定义了分布式事务处理模型DTP（Distributed Transaction Processing Reference Model）。\n整个 2PC 的事务流程涉及到三个角色 AP、RM、TM。AP 指的是使用 2PC 分布式事务的应用程序；RM 指的是资源管理器，它控制着分支事务；TM 指的是事务管理器，它控制着整个全局事务。\n（1）在准备阶段 RM 执行实际的业务操作，但不提交事务，资源锁定\n（2）在提交阶段 TM 会接受 RM 在准备阶段的执行回复，只要有任一个RM执行失败，TM 会通知所有 RM 执行回滚操作，否则，TM 将会通知所有 RM 提交该事务。提交阶段结束资源锁释放。\nXA方案的问题\n需要本地数据库支持XA协议。 资源锁需要等到两个阶段结束才释放，性能较差。\nSeata方案 Seata 是由阿里中间件团队发起的开源项目 Fescar，后更名为 Seata，它是一个是开源的分布式事务框架。\n传统 2PC 的问题在 Seata 中得到了解决，它通过对本地关系数据库的分支事务的协调来驱动完成全局事务，是工作在应用层的中间件。主要优点是性能较好，且不长时间占用连接资源，它以高效并且对业务 0 侵入的方式解决微服务场景下面临的分布式事务问题，它目前提供 AT 模式（即 2PC）及 TCC 模式的分布式事务解决方案。\nSeata 的设计思想如下: Seata 的设计目标其一是对业务无侵入，因此从业务无侵入的 2PC 方案着手，在传统 2PC的基础上演进，并解决 2PC 方案面临的问题。\nSeata 把一个分布式事务理解成一个包含了若干分支事务的全局事务。全局事务的职责是协调其下管辖的分支事务达成一致，要么一起成功提交，要么一起失败回滚。此外，通常分支事务本身就是一个关系数据库的本地事务。\nSeata实现2PC与传统2PC的差别\n架构层次方面：传统 2PC 方案的 RM 实际上是在数据库层，RM 本质上就是数据库自身，通过 XA 协议实现，而 Seata 的 RM 是以 jar 包的形式作为中间件层部署在应用程序这一侧的。\n两阶段提交方面：传统 2PC无论第二阶段的决议是 commit 还是 rollback ，事务性资源的锁都要保持到 Phase2 完成才释放。而 Seata 的做法是在 Phase1 就将本地事务提交，这样就可以省去 Phase2 持锁的时间，整体提高效率。\n","permalink":"https://csqread.top/posts/tech/%E6%8B%BC%E5%A4%9A%E5%A4%9A%E9%9D%A2%E7%BB%8F/","summary":"面试题目 记录下社招面试拼多多的总结与心得，以及失败的原因吧。 1、服务注册是如何发现的，eureka的基本原理， 容器ip是动态的还是静态的 答：","title":"拼多多面经"},{"content":"ArrayList 1. 概览 实现了 RandomAccess 接口，因此支持随机访问。这是理所当然的，因为 ArrayList 是基于数组实现的。\n1 2 public class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。\n1 private static final int DEFAULT_CAPACITY = 10; 2. 扩容 添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1)，也就是旧容量的 1.5 倍。\n扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length \u0026gt; 0) grow(minCapacity); } private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } 3. 删除元素 需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看出 ArrayList 删除元素的代价是非常高的。\n1 2 3 4 5 6 7 8 9 10 public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue; } 4. Fail-Fast modCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。\n在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i\u0026lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); } } 5. 序列化 ArrayList 基于数组实现，并且具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。\n保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化。\n1 transient Object[] elementData; // non-private to simplify nested class access ArrayList 实现了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size \u0026gt; 0) { // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i\u0026lt;size; i++) { a[i] = s.readObject(); } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i\u0026lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); } } 序列化时需要使用 ObjectOutputStream 的 writeObject() 将对象转换为字节流并输出。而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。反序列化使用的是 ObjectInputStream 的 readObject() 方法，原理类似。\n1 2 3 ArrayList list = new ArrayList(); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file)); oos.writeObject(list); ","permalink":"https://csqread.top/posts/tech/java-arraylist%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%8E%E6%80%BB%E7%BB%93/","summary":"ArrayList 1. 概览 实现了 RandomAccess 接口，因此支持随机访问。这是理所当然的，因为 ArrayList 是基于数组实现的。 1 2 public class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。 1 private static final int DEFAULT_CAPACITY =","title":"Java ArrayList源码分析与总结"},{"content":"缓存优化 1、对应参数表数据量如果小于10M，数据要同时缓存到Redis和本地，查询遵循的缓存原则为:优先查询本地缓存，本地没有查询Redis缓存，Redis没有再查询数据库.\n2、对参数表的缓存要分类进行配置：黑名单类参数表查询，Redis缓存为空时不再查询数据库：常规类参数表查询，Redis缓存为空的时候需要再查询数据库\n3、如果参数表出现大量RPC。确定SQL都是等值的情况下，一般是缓存索引配置的不合适/\n4、如果出现大量RPC，在SQL非等值情况下，需要通过等值SQL去查询，然后在程序代码中去判断数据是否符合要求。\n5、针对查询比较多并且修改也比较多的数据，可以针对部分不变的数据配置缓存。 （比如针对内部合约相关的数据，需要频繁的调用内部账的科目存储字段，虽然内部户的一整条数据会经常变动，但是内部户的科目存储字段基本上不会改变，就可以把这些不变的数据配置缓存，提升查询效率，并且可以减少RPC的次数）。\n6、根据不同条件调用他组缓存表的接口时，可以现根据他组配置缓存索引条件进行查询，然后在程序中再根据非索引条件过滤查询结果。\n编码优化 1、当某业务构件执行缓慢时，除了要排查是否有非必要RPC时或环境影响因素外，还需要检查构件中是否有重复执行的代码和SQL，第一次执行向后传递可以提升传递效率。\n2、如果通过实现JAR包调用他组接口还RPC了的，首先检查他们是否缓存表以及缓存表索引配置是否正确，如以上没问题，则可能是JAR包中未打入实现类。\n3、因为Java接口中传递对象是通过引用方式传递的，如果接口对传入的对象进行了修改，当执行完接口在接口外部拿到的该对象中的数据是被修改的，此时在接口外部继续获取对象中被修改的原数据时容易得到意想不到的结果。\n4、调用某构件的时候，构件中又要获取一些数据进行RPC，如果调用构件之前已有相关数据，可以调用构件时传递进去，可减少RPC。\n5、根据不同条件多次使用其他组的接口进行查询时，可以提取多个条件的交集，根据交集条件查询所有数据，然后在程序中分别根据非交集的条件进行过滤。\n6、对于多次循环调用他组同一个方法时，每次输入的值不同，可将所有输入值包装成LIST一次性传入，得到一个查询列表集合作为类似本地缓存，然后对这个LSIT集合进行循环整理。\n7、RPC接口要尽量简单，输入输出接口不要继承一些不需要的东西，既能减少网络传输消耗，还能减少序列化和反序列化的时间。\n数据库及SQL优化 1、尽量避免使用SELECT * 来查询所有字段，仅查询必要数据行及字段，既能减少网络传输消耗，还能提高命中覆盖索引的概率。\n2、写SQK的时候永远记得WHERE条件要和主键或者索引匹配。\n3、对于查询量非常大修改比较少的表， 且SELECT的字段正好比索引多一个或者两个字段的时候，可以采取把多余的一个或者两个字段冗余到索引上，这种一空间换取时间的方式虽然一定程度上增加了索引空间的开销，但是对于非常高频的SQL直接命中了覆盖索引，避免了回表查询，有助于提升查询效率。\n4、对于修改量非常大的交易，要及其精简索引，能不要的索引最好不要建，在修改表的时候可以减少索引的维护，有助于提升修改效率。\n以上是工作中的性能优化总结，后续如果有新的总结再来记录\n","permalink":"https://csqread.top/posts/tech/%E5%88%86%E5%B8%83%E5%BC%8F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/","summary":"缓存优化 1、对应参数表数据量如果小于10M，数据要同时缓存到Redis和本地，查询遵循的缓存原则为:优先查询本地缓存，本地没有查询Redis","title":"分布式性能优化总结"},{"content":"字符操作 编码与解码 编码就是把字符转换为字节，而解码是把字节重新组合成字符。\n如果编码和解码过程使用不同的编码方式那么就出现了乱码。\nGBK 编码中，中文字符占 2 个字节，英文字符占 1 个字节； UTF-8 编码中，中文字符占 3 个字节，英文字符占 1 个字节； UTF-16be 编码中，中文字符和英文字符都占 2 个字节。 UTF-16be 中的 be 指的是 Big Endian，也就是大端。相应地也有 UTF-16le，le 指的是 Little Endian，也就是小端。\nJava 的内存编码使用双字节编码 UTF-16be，这不是指 Java 只支持这一种编码方式，而是说 char 这种类型使用 UTF-16be 进行编码。char 类型占 16 位，也就是两个字节，Java 使用这种双字节编码是为了让一个中文或者一个英文都能使用一个 char 来存储。\nString 的编码方式 String 可以看成一个字符序列，可以指定一个编码方式将它编码为字节序列，也可以指定一个编码方式将一个字节序列解码为 String。\n1 2 3 4 String str1 = \u0026#34;中文\u0026#34;; byte[] bytes = str1.getBytes(\u0026#34;UTF-8\u0026#34;); String str2 = new String(bytes, \u0026#34;UTF-8\u0026#34;); System.out.println(str2); 在调用无参数 getBytes() 方法时，默认的编码方式不是 UTF-16be。双字节编码的好处是可以使用一个 char 存储中文和英文，而将 String 转为 bytes[] 字节数组就不再需要这个好处，因此也就不再需要双字节编码。getBytes() 的默认编码方式与平台有关，一般为 UTF-8。\n1 byte[] bytes = str1.getBytes(); Reader 与 Writer 不管是磁盘还是网络传输，最小的存储单元都是字节，而不是字符。但是在程序中操作的通常是字符形式的数据，因此需要提供对字符进行操作的方法。\nInputStreamReader 实现从字节流解码成字符流； OutputStreamWriter 实现字符流编码成为字节流。 实现逐行输出文本文件的内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public static void readFileContent(String filePath) throws IOException { FileReader fileReader = new FileReader(filePath); BufferedReader bufferedReader = new BufferedReader(fileReader); String line; while ((line = bufferedReader.readLine()) != null) { System.out.println(line); } // 装饰者模式使得 BufferedReader 组合了一个 Reader 对象 // 在调用 BufferedReader 的 close() 方法时会去调用 Reader 的 close() 方法 // 因此只要一个 close() 调用即可 bufferedReader.close(); } ","permalink":"https://csqread.top/posts/tech/java%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E6%93%8D%E4%BD%9C/","summary":"字符操作 编码与解码 编码就是把字符转换为字节，而解码是把字节重新组合成字符。 如果编码和解码过程使用不同的编码方式那么就出现了乱码。 GBK 编码中，中","title":"Java中的字符操作"},{"content":"自旋锁 互斥同步进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求一个共享数据的锁时执行忙循环（自旋）一段时间，如果在这段时间内能获得锁，就可以避免进入阻塞状态。\n自旋锁虽然能避免进入阻塞状态从而减少开销，但是它需要进行忙循环操作占用 CPU 时间，它只适用于共享数据的锁定状态很短的场景。\n在 JDK 1.6 中引入了自适应的自旋锁。自适应意味着自旋的次数不再固定了，而是由前一次在同一个锁上的自旋次数及锁的拥有者的状态来决定。\n锁消除 锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。\n锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。\n对于一些看起来没有加锁的代码，其实隐式的加了很多锁。例如下面的字符串拼接代码就隐式加了锁：\n1 2 3 public static String concatString(String s1, String s2, String s3) { return s1 + s2 + s3; } String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作：\n1 2 3 4 5 6 7 public static String concatString(String s1, String s2, String s3) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString(); } 每个 append() 方法中都有一个同步块。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是说，sb 的所有引用永远不会逃逸到 concatString() 方法之外，其他线程无法访问到它，因此可以进行消除。\n锁粗化 如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。\n上一节的示例代码中连续的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部。对于上一节的示例代码就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。\n轻量级锁 JDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。\n以下是 HotSpot 虚拟机对象头的内存布局，这些数据被称为 Mark Word。其中 tag bits 对应了五个状态，这些状态在右侧的 state 表格中给出。除了 marked for gc 状态，其它四个状态已经在前面介绍过了。\n下图左侧是一个线程的虚拟机栈，其中有一部分称为 Lock Record 的区域，这是在轻量级锁运行过程创建的，用于存放锁对象的 Mark Word。而右侧就是一个锁对象，包含了 Mark Word 和其它信息。\n轻量级锁是相对于传统的重量级锁而言，它使用 CAS 操作来避免重量级锁使用互斥量的开销。对于绝大部分的锁，在整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步，如果 CAS 失败了再改用互斥量进行同步。\n当尝试获取一个锁对象时，如果锁对象标记为 0 01，说明锁对象的锁未锁定（unlocked）状态。此时虚拟机在当前线程的虚拟机栈中创建 Lock Record，然后使用 CAS 操作将对象的 Mark Word 更新为 Lock Record 指针。如果 CAS 操作成功了，那么线程就获取了该对象上的锁，并且对象的 Mark Word 的锁标记变为 00，表示该对象处于轻量级锁状态。\n如果 CAS 操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的虚拟机栈，如果是的话说明当前线程已经拥有了这个锁对象，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。\n偏向锁 偏向锁的思想是偏向于让第一个获取锁对象的线程，这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。\n当锁对象第一次被线程获得的时候，进入偏向状态，标记为 1 01。同时使用 CAS 操作将线程 ID 记录到 Mark Word 中，如果 CAS 操作成功，这个线程以后每次进入这个锁相关的同步块就不需要再进行任何同步操作。\n当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向（Revoke Bias）后恢复到未锁定状态或者轻量级锁状态。\n","permalink":"https://csqread.top/posts/tech/java%E9%94%81%E4%BC%98%E5%8C%96%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/","summary":"自旋锁 互斥同步进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求","title":"Java锁优化相关笔记"},{"content":"C. x的最低有效字节中的位都等于1 D. x的最高有效字节中的位都等于0 代码应该遵循位级整数编码规则，另外还有一个限制，不能使用(==)和(!=)测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 /* * 2.61.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; int A(int x) { return !~x; } int B(int x) { return !x; } int C(int x) { return A(x | ~0xff); } int D(int x) { return B((x \u0026gt;\u0026gt; ((sizeof(int)-1) \u0026lt;\u0026lt; 3)) \u0026amp; 0xff); } int main(int argc, char* argv[]) { int all_bit_one = ~0; int all_bit_zero = 0; assert(A(all_bit_one)); assert(!B(all_bit_one)); assert(C(all_bit_one)); assert(!D(all_bit_one)); assert(!A(all_bit_zero)); assert(B(all_bit_zero)); assert(!C(all_bit_zero)); assert(D(all_bit_zero)); // test magic number 0x1234ff assert(!A(0x1234ff)); assert(!B(0x1234ff)); assert(C(0x1234ff)); assert(D(0x1234ff)); // test magic number 0x1234 assert(!A(0x1234)); assert(!B(0x1234)); assert(!C(0x1234)); assert(D(0x1234)); return 0; } 2.62编写一个函数int_shifts_are_arithemtic()，在对int类型的数使用算数右移的机器上运行时这个函数生成1，而其他情况下生成0.你的代码应该可以运行在任何字长的机器上。在几种机器上测试你的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /* * int-shifts-are-arithemetic.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; int int_shifts_are_arithemetic() { int num = -1; return !(num ^ (num \u0026gt;\u0026gt; 1)); } int main(int argc, char* argv[]) { assert(int_shifts_are_arithemetic()); return 0; } 2.63 将下面的c代码补充完整。函数srl用算术右移(由值xsra给出)来完成逻辑右移，后面的其他操作不包括右移或者除法。函数sra用逻辑右移(由值xsrl给出)来完成算术右移，后面的其他操作不包括右移或者除法。可以通过计算8*sizeof(int)来确定数据类型int中的位数w。位移量k的取值范围为0~w-1.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 /* * srl-sra.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; unsigned srl(unsigned x, int k) { unsigned xsra = (int) x \u0026gt;\u0026gt; k; int w = sizeof(int) \u0026lt;\u0026lt; 3; int mask = (int) -1 \u0026lt;\u0026lt; (w - k); return xsra \u0026amp; ~mask; } int sra(int x, int k) { int xsrl = (unsigned) x \u0026gt;\u0026gt; k; int w = sizeof(int) \u0026lt;\u0026lt; 3; int mask = (int) -1 \u0026lt;\u0026lt; (w - k); //let mask remain unchanged when the first bit of x is 1, otherwise 0. int m = 1 \u0026lt;\u0026lt; (w - 1); mask \u0026amp;= ! (x \u0026amp; m) - 1; return xsrl | mask; } int main(int argc, char* argv[]) { unsigned test_unsigned = 0x12345678; int test_int = 0x12345678; assert(srl(test_unsigned, 4) == test_unsigned \u0026gt;\u0026gt; 4); assert(sra(test_int, 4) == test_int \u0026gt;\u0026gt; 4); test_unsigned = 0x87654321; test_int = 0x87654321; assert (srl (test_unsigned, 4) == test_unsigned \u0026gt;\u0026gt; 4); assert (sra (test_int, 4) == test_int \u0026gt;\u0026gt; 4); return 0; } 2.64 写出代码实现如下函数：\n1 2 /*Return 1 when any odd bit of x equals 1; 0 otherwise. Assume w=32 */ int any_odd_one(unsigned x) 函数应该遵循位级整数编码规则，不过你可以假设数据类型int有w=32位。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /* * any-odd-one.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; int any_odd_one(unsigned x) { return !!(0xAAAAAAAA \u0026amp; x); } int main(int argc, char* argv[]) { assert(any_odd_one(0x2)); assert(!any_odd_one(0x4)); return 0; } 2.65 写出代码实现如下函数：\n1 2 /* return 1 when x contains an odd number of 1s; 0 otherwise. Assume w =32 */ int odd_ones(unsigned x); 函数应该遵循位级整数编码规则，不过你可以假设数据类型int有w=32位。你的代码最多只能包含12个算数运算，位运算和逻辑运算。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /* * odd-ones.c */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;assert.h\u0026gt; int odd_ones(unsigned x) { x ^= x \u0026gt;\u0026gt; 16; x ^= x \u0026gt;\u0026gt; 8; x ^= x \u0026gt;\u0026gt; 4; x ^= x \u0026gt;\u0026gt; 2; x ^= x \u0026gt;\u0026gt; 1; x \u0026amp;= 0x1; return x; } int main(int argc, char* argv[]) { assert(odd_ones(0x10101011)); assert(!odd_ones(0x01010101)); return 0; } 代码参考自GitHub\n","permalink":"https://csqread.top/posts/tech/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F-chapter-2-practice/","summary":"C. x的最低有效字节中的位都等于1 D. x的最高有效字节中的位都等于0 代码应该遵循位级整数编码规则，另外还有一个限制，不能使用(==)和(!=)测","title":"深入理解计算机系统 Chapter 2 Practice"},{"content":" 我大概是从3月中旬发现这部剧集的，那个时候还是因为在tg的某句台词引起了我的兴趣，在这里就不说了，说了这篇公众号就发不出去了，说不定我的号也就没了。然后我就开始在各种电影网站搜罗资源，结果发现低端影视已经出了好几集了，ddys站长真的是良心大大滴好，以后有钱了我一定要donate，现在就白嫖吧。\n说到Alex Garland，我不得不想起来以前看过的他的两部科幻电影。一部是《机械姬》，另一部是《湮灭》。两部电影分别从人工智能和生物科学的主题进行探讨，从这两部以及现在的《devs》来看，他的电影的画面都有那种给虚无缥缈的一种奇幻的氛围，\n不过，这部Devs从量子力学的角度来探讨在今天还是很热门，一开始看得时候我有点懵，不懂什么叫做量子力学，看完第一集我去维基百科查看了一下量子力学的定义：\n量子力学（英语：quantum mechanics）是物理学的分支学科。它主要描写微观的事物，与相对论一起被认为是现代物理学的两大基本支柱，许多物理学理论和科学，如原子物理学、固体物理学、核物理学和粒子物理学以及其它相关的学科，都是以其为基础。\n19世纪末，人们发现旧有的经典理论无法解释微观系统，于是经由物理学家的努力，在20世纪初创立量子力学，解释了这些现象。量子力学从根本上改变人类对物质结构及其相互作用的理解。除了透过广义相对论描写的引力外，迄今所有基本相互作用均可以在量子力学的框架内描述（量子场论）。\n量子理论的重要应用包括量子化学、量子光学、量子计算、超导磁体、发光二极管、激光器、晶体管和半导体如微处理器等。\n这里面涉及到的东西太多，我看了半天也就了解了几个概念的定义，比如量子纠缠，量子退相干等等，总之一句话：不知道是什么玩意\n所以人们经常有一个说法：人类如果能够突破量子力学，也就能够确认灵魂是否存在。在众多的科幻小说里面，量子力学主要是突破意识的决定存在。我们经常所说的“薛定谔的猫”，“电子双缝实验”以及“上帝掷骰子”也有些相关性。不过在影视作品中这个概念特别吃香：自我最早看到的变形金刚系列开始的美帝影视作品中就开始：遇事不决，量子力学\n而《Devs》是一部与量子力学强相关的作品。\n何为强相关？强相关又称高度相关，即当一列变量变化时，与之相应的另一列变量增大（或减少）的可能性非常大。在坐标图上则表现为散点图较为集中在某条直线的周围。-摘自百度百科；\n剧中有专门研究量子力学的公司，并且取得了突破性的成果。不过不仅仅如此，这部剧中还涉及到了多重宇宙，自由意志以及万事万物之间都有因果关系这些概念，看起来还是挺有意思的，能够感受到美帝的电影人在这方面确实有些功底，或者说下了不少功夫。\n并且，我认为，这还是一部不可多得的悬疑剧，和之前的《误杀》一样足以调动观众的胃口，感觉不巧的是同时和《西部世界》第三季上映，让很多人没有能够了解到这部剧集。\n第一集主要介绍了一下这个量子力学的公司以及男主女主的出现：\n阿玛雅\u0026ndash;量子未来 故事开始，阿玛雅的工程师谢尔盖和团队争取到了15分钟，在CEO面前演示自己团队的成果\u0026ndash;线虫仿真映射实验。\n什么是线虫？：\n线虫动物门是动物界中最大的门之一，为假体腔动物，有超过28,000个已被记录的物种，尚有大量种尚未命名。绝大多数体小呈圆柱形，又称圆虫（roundworms）。它们在淡水、海水、陆地上随处可见，不论是个体数或物种数都往往超越其他动物，并在极端的环境如南极和海沟都可发现。此外，有许多种的线虫是寄生性的(超过16,000种)，包括许多植物及人类在内的动物的病原体。只有节肢动物比线虫更多样化-线虫也是目前世界上唯一被完整绘制出神经网络的生物，因此可以很好的进行观测，并用计算机来模拟实验\n通过这次实验，他们团队展示了他们可以通过计算机追踪线虫的神经系统培养数据，并在电脑中模拟出一条仿真线虫。不仅仅如此，它可以预测该线虫在接下来10秒中的行为。这种科技成果意味着什么，可想而知。\n接下来，谢尔盖被CEO邀请加入Devs Devs，阿玛雅公司迄今为止最机密，最隐秘的实验项目，没有人知道它是做什么的。在加入Devs之前还做了严格的审查，在这里又爆出了美剧中常黑的中俄 不过，谢尔盖还是顺利通过了审查，CEO亲自带领谢尔盖进入Devs内部。\n在这里，就可以发现Alex Garland电影的某一共同之处，就是营造神秘气氛的手法，通过灯光变化，明暗交替以及一些硬科学的概念。\n带着谢尔盖简单的观光之后，CEO神秘的对他说，坐下来，read code，你就会明白这里的一切。谢尔盖读完代码后盯着屏幕良久，一脸懵逼，跑到厕所又哭又吐。看到这里我也是一脸懵逼加好奇。这代码有什么？还能让一个人感到这么恶心？\n谢尔盖从厕所出来后问到项目负责人Katie:这只是理论上的还是真的？代码你们已经运行过了吗？Katie淡定的回答：代码我们已经运行过了，并且已经有了结果。“但并不会改变什么，这就是意义所在”。\n晚上谢尔盖神色匆匆的离开Devs实验室，被CEO Forest撞个正着。Forest直接说出了谢尔盖的真实身份：俄罗斯特工（看到这里我有点想diss，多少年了，美帝还是在疯狂在电影里搞阴谋论）。但是这个时候Forest并没有责怪谢尔盖，而是试图让他理解，你所做的并非出于自己的自由意志，你的选择只是一系列的因果，这句话在后面也好像经常被提起过，前面的剧集好久之前看的了，有些忘记了。“如果我们生活在确定的宇宙中，这些决定，只能是之前一些事情的结果：你出生在哪，你如何长大，你大脑的物理构造，这就是先天后天矩阵，，就像你仿真演示的线虫，不过更复杂，但仍然是遵循因果。”感觉有点像我们佛家的因果循环。善有善报，恶有恶报，一切因皆有果的样子。感觉有点玄学的味道了。然后，谢尔盖被Forest的贴身保镖用塑料袋窒息，谢尔盖卒，谢尔盖女友开始着急，并开始探寻事情的真相。第一集结束\n第一集做了一个很好的铺垫，让我想知道这个Devs到底是干嘛的。后面的剧情慢慢的给出了解释：\n通过无数次的量子计算，Devs投射出了2000多年前耶稣受难的影像 “这不是想象，也不是重绘，这是对过去的量子直播\u0026quot;\n\u0026ldquo;Devs\u0026quot;在追寻神的起源\n不过看到最后就会发现Devs不仅仅可以追溯神的起源，它还可以让你变成Deus，这也就是这两张图片的寓意： 最后这两个人都变成了另一个世界的神。具体怎么变成的，最后一集中是这样描述的：他们两因为某种”宿命“或者什么原因，在Devs实验室死去，这也是他们都知道会发生的结果，CEO Forest让Katie捕获了他们死亡那个时刻的数据，通过Devs项目在计算机中进行模拟，也就是说，他们在系统中又重新复活了，并且有死亡之前的记忆，可以在另一个世界，一个真实的世界仿真的存在，并且，只要他们愿意，他们就可以重新来过。\n在理论物理中，时间并不是一个切实的存在\n只是因为意识的存在或者说生命的有限才造成了这种概念\n回想之前的线虫实验，生命体的神经活动可以被预测，而大脑的构造是写进\u0026quot;DNA\u0026quot;的，这意味着从一颗胚胎开始，未来就可以被决定。这里又让我想起来《Brave New World》，哈哈哈。\n不过这部剧不适合有巨恐的人观看，因为那个巨娃娃看着挺瘆人的 诺贝尔奖得主尤金维·格纳认为，因为存在观察者的意识，宇宙的存在才有意义\n线虫的观察者是人类，那么人类的观察者的是什么？宇宙的观察者是什么？\n是否存在一个最高的宇宙意识在观察整个宇宙？\n这里我想到了三体中的宇宙观察者的概念，一维二维三维到十维的概念。更高维度的观察者是否此刻正在观察着我们的一举一动？\n今天总算是追完了这部剧，接下来可以安安心心的看《西部世界》了。Anyway，又是胡思乱想的时刻，在这里胡思乱想。 Big Cat Is Watching You ! ","permalink":"https://csqread.top/posts/read/%E5%BC%80%E5%8F%91%E8%80%85%E5%BD%B1%E8%AF%84/","summary":"我大概是从3月中旬发现这部剧集的，那个时候还是因为在tg的某句台词引起了我的兴趣，在这里就不说了，说了这篇公众号就发不出去了，说不定我的号也","title":"《开发者》影评"},{"content":"刚开始我没想到用爬虫，就直接手创了一个字典\u0026hellip; 需要安装模块pyechart and pyechart-china-provinces-pypkg\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from pyecharts.charts import Map from pyecharts import options as opts #省和直辖市 province_distribution = { \u0026#39;湖北\u0026#39;: 4586, \u0026#39;浙江\u0026#39;: 428, \u0026#39;广东\u0026#39;: 311, \u0026#39;湖南\u0026#39;: 277, \u0026#39;河南\u0026#39;: 278, \u0026#39;安徽\u0026#39;: 200, \u0026#39;重庆\u0026#39;: 165, \u0026#39;山东\u0026#39;: 145, \u0026#39;江西\u0026#39;: 162, \u0026#39;四川\u0026#39;: 142, \u0026#39;江苏\u0026#39;: 129, \u0026#39;北京\u0026#39;: 111, \u0026#39;福建\u0026#39;: 101, \u0026#39;上海\u0026#39;: 101, \u0026#39;广西\u0026#39;: 78, \u0026#39;陕西\u0026#39;: 56, \u0026#39;河北\u0026#39;: 48, \u0026#39;云南\u0026#39;: 44, \u0026#39;海南\u0026#39;: 43, \u0026#39;黑龙江\u0026#39;: 43, \u0026#39;辽宁\u0026#39;: 39, \u0026#39;山西\u0026#39;: 35, \u0026#39;天津\u0026#39;: 28, \u0026#39;甘肃\u0026#39;: 26, \u0026#39;内蒙古\u0026#39;: 16, \u0026#39;新疆\u0026#39;: 14, \u0026#39;宁夏\u0026#39;: 12, \u0026#39;贵州\u0026#39;: 12, \u0026#39;吉林\u0026#39;: 14, \u0026#39;台湾\u0026#39;: 8, \u0026#39;香港\u0026#39;: 10, \u0026#39;澳门\u0026#39;: 7, \u0026#39;青海\u0026#39;: 6, \u0026#39;西藏\u0026#39;: 1 } # maptype = \u0026#39;china\u0026#39; 只显示全国直辖市和省级 map = Map() map.set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;2020中国疫情地图\u0026#34;), visualmap_opts=opts.VisualMapOpts(max_=3600, is_piecewise=True, pieces=[ {\u0026#34;max\u0026#34;: 5000, \u0026#34;min\u0026#34;: 1001, \u0026#34;label\u0026#34;: \u0026#34;\u0026gt;1000\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#8A0808\u0026#34;}, {\u0026#34;max\u0026#34;: 1000, \u0026#34;min\u0026#34;: 500, \u0026#34;label\u0026#34;: \u0026#34;500-1000\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#B40404\u0026#34;}, {\u0026#34;max\u0026#34;: 499, \u0026#34;min\u0026#34;: 100, \u0026#34;label\u0026#34;: \u0026#34;10-99\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#78181\u0026#34;}, {\u0026#34;max\u0026#34;: 99, \u0026#34;min\u0026#34;: 1, \u0026#34;label\u0026#34;: \u0026#34;1-9\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#F5A9A9\u0026#34;}, {\u0026#34;max\u0026#34;: 0, \u0026#34;min\u0026#34;: 0, \u0026#34;label\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#FFFFFF\u0026#34;}, ] ) ) map.add(\u0026#34;20200130中国疫情地图\u0026#34;, data_pair=province_distribution.items(), maptype=\u0026#34;china\u0026#34;, is_roam=True) map.render(\u0026#39;20200130中国1疫情地图.html\u0026#39;) 然后生成了一张图，看起来还行 但是昨天学习了API调用，以及进行可视化，今天怎么能用这么粗糙的方法来做统计，直接爬下来放到字典里然后用pyechart进行可视化岂不美哉\nmain.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 import matplotlib.pyplot as plt import numpy as np import json import requests from matplotlib.font_manager import FontProperties import re import os from pyecharts.charts import Line, Pie, Map from pyecharts import options as opts import pygal from pygal.style import LightColorizedStyle as LCS, LightenStyle as LS # 获取各省市今日的数据，存入josn文件 def get_province_data(month, day, province_name=None): file = open(\u0026#39;results/%d%d.json\u0026#39; % (month, day), \u0026#39;r\u0026#39;, encoding=\u0026#39;UTF-8\u0026#39;) json_array = json.loads(file.read()) file.close() if not province_name: return json_array for json_object in json_array: if json_object[\u0026#39;provinceName\u0026#39;] == province_name: return json_object if json_object[\u0026#39;provinceShortName\u0026#39;] == province_name: return json_object return None def get_province_status(month, day, province_name=None): if province_name: print(province_name) json_object = get_province_data(month, day, province_name) data = [] for city in json_object[\u0026#39;cities\u0026#39;]: data.append((city[\u0026#39;cityName\u0026#39;], city[\u0026#39;confirmedCount\u0026#39;])) data.sort(key=lambda x: -x[1]) title = \u0026#39;%s2020年%d月%d日确诊病例\u0026#39; % (province_name, month, day) else: json_array = get_province_data(month, day, province_name) data = [] for province in json_array: data.append((province[\u0026#39;provinceShortName\u0026#39;], province[\u0026#39;confirmedCount\u0026#39;])) data.sort(key=lambda x: -x[1]) title = \u0026#39;全国2020年%d月%d日确诊病例\u0026#39; % (month, day) labels = [d[0] for d in data] counts = [d[1] for d in data] return labels, counts, title def show_province_status(month, day, province_name=None): labels, counts, title = get_province_status(month, day, province_name) # draw_pie(month, day, labels, counts, title) get_pyecharts_pie(month, day, labels, counts, title) #饼状图 def draw_pie(month, day, labels, counts, title): if len(labels) == 0: return labels = np.array(labels) counts = np.array(counts) title += \u0026#39;-%d例\u0026#39; % sum(counts) fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\u0026#34;equal\u0026#34;)) explode = np.zeros(len(labels)) explode[np.argmax(counts)] = 0.1 wedges, texts = ax.pie(counts, wedgeprops=dict(width=0.5), startangle=-40, explode=explode) font = FontProperties(fname=\u0026#39;font/ZiXinFangYunYuanTi-2.ttf\u0026#39;) bbox_props = dict(boxstyle=\u0026#34;square,pad=0.3\u0026#34;, fc=\u0026#34;w\u0026#34;, ec=\u0026#34;k\u0026#34;, lw=0.72) kw = dict(arrowprops=dict(arrowstyle=\u0026#34;-\u0026#34;), bbox=bbox_props, zorder=0, va=\u0026#34;center\u0026#34;, fontproperties=font) for i, p in enumerate(wedges[:6]): ang = (p.theta2 - p.theta1) / 2. + p.theta1 y = np.sin(np.deg2rad(ang)) x = np.cos(np.deg2rad(ang)) horizontalalignment = {-1: \u0026#34;right\u0026#34;, 1: \u0026#34;left\u0026#34;}[int(np.sign(x))] connectionstyle = \u0026#34;angle,angleA=0,angleB={}\u0026#34;.format(ang) kw[\u0026#34;arrowprops\u0026#34;].update({\u0026#34;connectionstyle\u0026#34;: connectionstyle}) ax.annotate(\u0026#39;%s-%d例\u0026#39; % (labels[i], counts[i]), xy=(x, y), xytext=(1.35 * np.sign(x), 1.4 * y), horizontalalignment=horizontalalignment, **kw) ax.set_title(title, fontproperties=font) root = \u0026#39;charts/%d%d\u0026#39; % (month, day) create_dir(root) plt.savefig(\u0026#39;%s/%s.jpg\u0026#39; % (root, title)) plt.show() def create_dir(root): if not os.path.exists(root): os.makedirs(root) def draw(month, day): provinces = get_province_data(month, day) for p in provinces: show_province_status(month, day, p[\u0026#39;provinceShortName\u0026#39;]) show_province_status(month, day) def get_html(month, day): import requests url = \u0026#39;http://3g.dxy.cn/newh5/view/pneumonia\u0026#39; response = requests.get(url) html = str(response.content, \u0026#39;UTF-8\u0026#39;) html_file = open(\u0026#39;results/%d%d.html\u0026#39; % (month, day), \u0026#39;w\u0026#39;, encoding=\u0026#39;UTF-8\u0026#39;) html_file.write(html) html_file.close() json_file = open(\u0026#39;results/%d%d.json\u0026#39; % (month, day), \u0026#39;w\u0026#39;, encoding=\u0026#39;UTF-8\u0026#39;) matches = re.findall(\u0026#39;\\[[^\u0026gt;]+\\]\u0026#39;, html) for match in matches: if \u0026#39;provinceName\u0026#39; in json.loads(match)[0]: json_file.write(match) break json_file.close() def compare(m1, d1, m2, d2): ps1 = get_province_data(m1, d1) ps2 = get_province_data(m2, d2) ps_dict1 = {} ps_dict2 = {} for p in ps1: ps_dict1[p[\u0026#39;provinceShortName\u0026#39;]] = p[\u0026#39;confirmedCount\u0026#39;] for p in ps2: ps_dict2[p[\u0026#39;provinceShortName\u0026#39;]] = p[\u0026#39;confirmedCount\u0026#39;] data = [] for key in ps_dict2: increased_count = ps_dict2[key] if key in ps_dict1: increased_count -= ps_dict1[key] data.append((key, increased_count)) data.sort(key=lambda x: -x[1]) labels = [d[0] for d in data] counts = [d[1] for d in data] title = \u0026#39;2020年%d月%d日全国新增确诊病例\u0026#39; % (m2, d2) draw_pie(m2, d2, labels, counts, title) get_pyecharts_pie(m2, d2, labels, counts, title) def get_pyecharts_pie(month, day, labels, counts, title): title += \u0026#39;-%d例\u0026#39; % (sum(counts)) c = ( Pie(init_opts=opts.InitOpts(width=\u0026#39;1200px\u0026#39;, height=\u0026#39;700px\u0026#39;)) .add( \u0026#34;\u0026#34;, [list(z) for z in zip(labels, counts)], radius=[\u0026#34;40%\u0026#34;, \u0026#34;80%\u0026#34;], center=[\u0026#39;50%\u0026#39;, \u0026#39;60%\u0026#39;], ) .set_global_opts( title_opts=opts.TitleOpts(title=title), legend_opts=opts.LegendOpts( orient=\u0026#34;vertical\u0026#34;, pos_top=\u0026#34;15%\u0026#34;, pos_left=\u0026#34;2%\u0026#34; ), ) .set_series_opts(label_opts=opts.LabelOpts(formatter=\u0026#34;{b}: {c}\u0026#34;)) ) root = \u0026#39;html-charts/%d%d\u0026#39; % (month, day) create_dir(root) c.render() c.render(\u0026#39;%s/%s.html\u0026#39; % (root, title)) return c def draw_tendency(month, day): dates = [\u0026#39;1-%d\u0026#39; % i for i in range(16, 30)] v0 = [4, 17, 59, 78, 92, 149, 131, 259, 444, 688, 769, 1771, 1459, 1576 ] v1 = [4, 17, 59, 77, 72, 105, 69, 105, 180, 323, 371, 1291, 840, 1032] c = ( Line() .add_xaxis(dates) .add_yaxis(\u0026#34;全国新增确诊病例\u0026#34;, v0, is_smooth=True, linestyle_opts=opts.LineStyleOpts(width=4, color=\u0026#39;#B44038\u0026#39;), itemstyle_opts=opts.ItemStyleOpts( color=\u0026#39;#B44038\u0026#39;, border_color=\u0026#34;#B44038\u0026#34;, border_width=5 )) .add_yaxis(\u0026#34;湖北新增确诊病例\u0026#34;, v1, is_smooth=True, linestyle_opts=opts.LineStyleOpts(width=2, color=\u0026#39;6FA0A7\u0026#39;), label_opts=opts.LabelOpts(position=\u0026#39;bottom\u0026#39;), itemstyle_opts=opts.ItemStyleOpts( color=\u0026#39;#6FA0A7\u0026#39;, border_color=\u0026#34;#6FA0A7\u0026#34;, border_width=3 )) .set_global_opts(title_opts=opts.TitleOpts(title=\u0026#34;\u0026#34;), yaxis_opts=opts.AxisOpts( type_=\u0026#34;log\u0026#34;, name=\u0026#34;y\u0026#34;, splitline_opts=opts.SplitLineOpts(is_show=True), is_scale=True, axisline_opts=opts.AxisLineOpts(is_show=False) ) ) ) c.render(\u0026#39;results/%d%d-新增病例趋势图.html\u0026#39; % (month, day)) return c def draw_map(month, day): labels, counts, title = get_province_status(month, day, None) c = ( Map() .add(\u0026#34;\u0026#34;, [list(z) for z in zip(labels, counts)], \u0026#34;china\u0026#34;) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;新型肺炎全国确诊病例\u0026#34;), visualmap_opts=opts.VisualMapOpts( pieces=[ {\u0026#39;min\u0026#39;: 1000, \u0026#39;color\u0026#39;: \u0026#39;#450704\u0026#39;}, {\u0026#39;max\u0026#39;: 999, \u0026#39;min\u0026#39;: 100, \u0026#39;color\u0026#39;: \u0026#39;#75140B\u0026#39;}, {\u0026#39;max\u0026#39;: 99, \u0026#39;min\u0026#39;: 10, \u0026#39;color\u0026#39;: \u0026#39;#AD2217\u0026#39;}, {\u0026#39;max\u0026#39;: 9, \u0026#39;min\u0026#39;: 1, \u0026#39;color\u0026#39;: \u0026#39;#DE605B\u0026#39;}, {\u0026#39;max\u0026#39;: 0, \u0026#39;color\u0026#39;: \u0026#39;#FFFEE7\u0026#39;}, ], is_piecewise=True ), ) ) c.render(\u0026#39;results/%d%d-疫情地图.html\u0026#39; % (month, day)) if __name__ == \u0026#39;__main__\u0026#39;: m, d = 1, 30 #get_html(m, d) #draw(m, d) #compare(1, 28, 1, 29) #show_province_status(m, d, \u0026#39;云南\u0026#39;) #show_province_status(m, d, None) draw_tendency(m, d) draw_map(m, d) 函数直接都放一个文件里面了，其实进行重构一下代码会更简洁一些\n","permalink":"https://csqread.top/posts/tech/2020-ncov%E7%88%AC%E8%99%AB%E7%BB%9F%E8%AE%A1/","summary":"刚开始我没想到用爬虫，就直接手创了一个字典\u0026hellip; 需要安装模块pyechart and pyechart-china-provinces-pypkg 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25","title":"2020 NCov爬虫统计"},{"content":"《拍电影时我在想的事》是戛纳电影节金棕榈奖得主是枝裕和历史8年，写下的首部自传性随笔集。是枝裕和在书中回顾三十余年的创作生涯，讲述每一部经典作品背后的传奇故事、缘起和理念，记录各个创作时期对电影的创作和思考，以及对世界和人生的看法。 其中不仅汇集了电影大师的哲思与灵光，更讲述了一位导演脚踏实地，从赊账拍片到斩获世界各大电影节奖项的励志旅程。《拍电影时我在想的事》在日本出版后，连续六次紧急加印，得到《朝日新闻》等各大媒体的高度评价，被盛赞到：哪怕再过100年，这本书也一定是作者的圣经，世界如此精彩，日常就很美丽，生命本身就是奇迹。是枝裕和打动世界的理由，都在这本书里。\n关于是枝裕和，大概是我在两年之前接触到的一位导演，那是因为室友推荐的一部电影《无人知晓》，看完之后感觉特别棒，就继续搜罗这他的电影资源，把他的电影大都看了一遍。其中不仅仅是无人知晓让我印象深刻，《如父如子》，《海街日记》，《比海更深》，《奇迹》都挺好看的。我就不一一剧透了，想看的话资源在页面最下面，是枝裕和蓝光合集。 关于他的这本书，看了之后发现其实是枝裕和开始的时候是拍纪录片起家的。那个时候和侯孝贤和杨德昌是同一个时代的导演。甚至是他俩的后来者。记得书中他说过，侯孝贤导演对他在电影上面也有过帮助呢。在开始的时候，是枝裕和使用分镜完成了他的处女作，幻之光，看了他画的分镜，挺有意思的，发现原来电影就是这样开始先是被简单的画出来的啊\n他的第一部电视纪录片的拍摄过程就他别传奇。采访自杀官员的遗孀，对方在一堆采访者中唯独接受是枝裕和的访问，其实是枝裕和也不知道为什么，事后她才道出原委：“你第一天来这里的时候，拘谨地坐在榻榻米上。当时的你，跟我丈夫和我相亲的样子特别像” 我讲述的电影语言，与以电影为母语的创作者所讲述的不同是带着电视口音的方言，也就是说，在语法上是不规范的。对电视的养育之情，我心怀感激，也坦率地承认自己“电视人”的身份。与此同时，我对目前所处的环境感到了某种责任。\n一些笔记： 关于分镜，他还曾被侯孝贤导演指出过： ‘知道被侯孝贤导演指出来，我才意识到自己“被分镜图绑住了手脚”’。 侯导来到日本参加东京电影节，见面时，他对我说：“技术很厉害，但是在拍摄之前，你早就画好了所有的分镜图吧？” “是的，画了，当时的我特别没有自信”我回答。 “不是应该看了演员的表演之后，才确定摄影机的位置吗？你以前是拍记录片的应该知道啊。”\n他对于虚构作品和纪录片的理解： “我一直认为，虚构作品要令观众‘沉醉’，而纪录片”则要让观众清醒。\n“比起有意义的死，不如去发现有意义却丰富的生”作为想法，这是正确的。但是从拍摄的电影来看，与带着这种意识拍摄面成的《花之舞者》相比，将生的实感通过细节表现出来的下一部电影《步履不停》，更明显地体现了这种价值观。\n电影并非空喊口号的东西，它是为了表达生命真实丰富的感受而1存在的。现在我正义这一点为目标而努力\n“在《无人知晓》中，我不想探讨谁对谁错的问题，也不想追究大人应该如何对待孩子，以及围绕孩子的法律应该如何修改等等。所谓的批判，教训和建议都不是我想讲的。我真正想做的是讲述孩子们的日常生活，以及在一旁观察他们，倾听他们的声音。这样一来，孩子们的话语就不再是独白，而是变成了对话。同样孩子们也通过双眼观察着我们”-（这应该是是枝裕和拍纪录片时养成的习惯）\n“我仍然要坚持这样来拍摄《无人知晓》，并非从单纯的黑与白的对立出发，而是从灰色的视角记录世界。没有纯粹的英雄或坏人只是如实的描述我们生活的这个由相对主义价值观构筑的世界”\n“明显的不同在于，在西方人看来，死亡始于生命的终结，也就是说生与死是两个对立的概念。但是在东方人（特别是日本人）看来，生与死是表里一本的，两者的关系甚至有点亲近。死亡未必始于生命的终结，死常常存在于圣的内部。这个观念一直以来都存在于我的思想中”\n“在欧洲，我反复被问到‘为什么您的作品中经常并不出现的死者，为什么不讲述死亡，而是常常讲述死后的世界’，我一直苦于如何回答，当时却不知不觉地说出了这样的答案：日本到某个时期为止，一直都有‘无颜面对祖先’地观念。日本没有绝对权威的神明，但是在日常生活中存在这一种伦理观：应该活得对得起死去的人。我也怀着这样的伦理观。因此在日本文化中的‘死者’代替了西方文化中的‘神’。死去的人并不是就这样离开了世间，而是从外部批判我们的生活，承担着伦理规范的作用1.也就是说，从故事外部批判我们的是死者，而站在故事内部承担这一角色的是孩子” -原来如父如子也是这样的观念啊\n对于死亡，是枝裕和所认为的与村上春树在《挪威的森林》中所写的几乎一模一样：“生常常存在与死的内部”。他们都认为，日本传统文化中一味看重“有意义的死”并非病态文化，没有实实在在生活过的实感，是无法去探讨死亡的意义的。这点和黑泽明在《梦》中所表现的也一样。死去的桃树化为桃树神，从外部批判着我们的生活。日本是一个泛灵的国家，万物皆有灵气，这一点在《千与千寻》，《幽灵公主》等宫崎骏的动漫中也可得以一窥。在《菊与刀》中更是系统的阐述了这一点。\n“游走在网络上的人为u什么普遍是右翼，或者是国家主义者？思考这个问题，会发现与他人缺乏紧密联系的人容易沉迷于网络世界，‘国家’这种概念轻轻松松就会将他们收编，成为他们内心唯一的价值观。在现代日本，所谓的地区共同体已经趋向崩溃，企业共同体随着终身雇佣制度一起消亡，家庭内的关系也越发越远。因此，如果没有可以代替共同体和家庭的事务、场所和价值观，将会有越来越多的人陷入虚幻的国家主义之中” 这么说来来我们人均陷入了虚幻的国家主义，难道因为天朝是家国情怀？\n一些想法： 看完这本书我知道了《无人知晓》是如何拍出来的，《小偷家族》是在表达什么。是枝裕和的电影观念可以这样表达：比起有意义的死，不如去发现无意义却丰富的生。\n电影并非高喊口号的东西，它就是为了表达生命真实丰富的感受而存在的。 有人问是枝裕和，在《无人知晓》中，作为导演你没有对电影中的人物进行道德上的审判，甚至没有指责抛弃孩子的母亲。是枝裕和是这样回答：\n电影不是用来审判人的，电影导演也不是法官。设计一个坏蛋可能会让故事（世界）更易于理解，但是不这样做，反而能让观众将电影中的问题带入日常生活中去思考。 因为观众是要回归日常生活的，每一个观众都要在电影结束后离开电影院。如果一个观众能够看完电影之后，对生活的看法会有所改变，这或许会成为他们带着批判性的视角观察日常生活的契机。\n是的，我喜欢这样的电影哲学，所谓的批判留给其他人，电影只带给观众真实生活的感受。杨德昌说：电影发明之后，人类的生命，比以前延长了三倍。而在观看好莱坞大片时，我们的生活近乎暂停了两个小时。但在观看这种电影时，我们并没有离开生活。所有的电影观众，无一例外的都要在电影散场之后，必须回到他们的日常生活。是枝裕和就这样影响了我们的生活，像他所说 “如果说我的电影中更有共通的东西，那就是无法取代的珍贵之物不在日常生活之外，而是蕴藏在日常生活的细枝末节里”\n是枝裕和合集： 链接：https://pan.baidu.com/s/1dUz8weNnzyc9s3zpKUCO_Q 提取码：janz\n","permalink":"https://csqread.top/posts/read/%E6%88%91%E5%9C%A8%E6%8B%8D%E7%94%B5%E5%BD%B1%E6%97%B6%E6%83%B3%E7%9A%84%E4%BA%8B-%E6%98%AF%E6%9E%9D%E8%A3%95%E5%92%8C/","summary":"《拍电影时我在想的事》是戛纳电影节金棕榈奖得主是枝裕和历史8年，写下的首部自传性随笔集。是枝裕和在书中回顾三十余年的创作生涯，讲述每一部经典","title":"‘我在拍电影时想的事‘ 是枝裕和"},{"content":"上午两节课睡过了，就没去了，吃了个早饭，还有一堆事情没有做，就开始先把昨晚没弄完的python词频分析的代码给写完了。做一个记录吧，防止以后用得着\n工具 工具：python3.7 Vscode wordcloud jieba 等 获取数据源 点击Tim左上角头像 选择安工程失物招领群1，导出消息记录 要用txt导出到任意盘，接下来就是要对导出的文件进行数据分析 下载对应库 到官方网站下载对应包 传送门 重要提醒：通过cmd中输入 Python -V 来查看python版本并下载相应的安装包，同时注意python是32位的还是64位的，我这里是32位的 下载完成后，进入刚刚下载该文件的路径，使用pip3 install wordcloud-1.3.3-cp-37-cp-37-win_amd32-whl命令开始安装， 这样 wordcloud就安装完成了。 接下来还要安装jieba matplotlib scipy 均使用 pip3 install xxx 即可 代码 首先过滤掉txt文件中无用的信息，比如时间，以及聊天的名片，避免词云中都是无效信息，并用jieba进行分词 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import jieba newtext = [] # 打开E盘下的聊天记录文件qq.txt for word in open(\u0026#39;E:\\\\qq.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;): tmp = word[0:4] if (tmp == \u0026#34;2017\u0026#34; or tmp == \u0026#34;====\u0026#34;or tmp == \u0026#34;2018\u0026#34;): # 过滤掉聊天记录的时间和qq名称 continue tmp = word[0:2] if (tmp[0] == \u0026#39;[\u0026#39; or tmp[0] == \u0026#39;/\u0026#39;or tmp[0] == \u0026#39;@\u0026#39;): # 过滤掉图片和表情，例如[图片]，/滑稽 continue newtext.append(word) # 将过滤掉图片和表情和时间信息和qq名称剩下的文字重新写入E盘下的q1.txt文件中去 with open(\u0026#39;E:\\\\q1.txt\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: for i in newtext: f.write(i) # 打开新生成的聊天记录文件 text = open(\u0026#39;E:\\\\q1.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() word_jieba = jieba.cut(text, cut_all=True) word_split = \u0026#34; \u0026#34;.join(word_jieba) 通过这步在E盘中得到了一个q1.txt文件，打开会发现变的整洁干净了许多.\n2.再新建一个.py文件，用到wordcloud库来绘制词云图\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator import matplotlib.pyplot as plt from scipy.misc import imread text = open(\u0026#39;E:\\\\q1.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() # 打开过滤好的txt文件 print(text) bg_pic = imread(\u0026#39;E:\\\\sjt.jpg\u0026#39;) # 导入词云背景 wordcloud = WordCloud(mask=bg_pic, background_color=\u0026#39;white\u0026#39;, scale=1.5, font_path=\u0026#39;C:/Windows/Fonts/simhei.ttf\u0026#39;, width=1000,height=600,stopwords={\u0026#39;表情\u0026#39;,\u0026#39;糊脸\u0026#39;,\u0026#39;拍桌\u0026#39;,\u0026#39;拍头\u0026#39;},min_font_size=10,max_font_size=36,font_step=4, ).generate(text) # 定义词云的各种变量，可以控制词云的形式，这里的控制变量可以去网上查找，stopwords={\u0026#39;表情\u0026#39;,\u0026#39;糊脸\u0026#39;,\u0026#39;拍桌\u0026#39;,\u0026#39;拍头\u0026#39;\u0026#39;是为了过滤掉里面的部分表情信息 image_colors = ImageColorGenerator(bg_pic) plt.imshow(wordcloud) plt.axis(\u0026#39;off\u0026#39;) plt.show() wordcloud.to_file(\u0026#39;E:\\\\text.jpg\u0026#39;) # 输出词云 做出的结果图\n挺好玩的，就多测试了几个群，就这样\n对词频出现次数进行统计，并生成统计表 直接上代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import jieba fr=open(\u0026#39;q1.txt\u0026#39;,\u0026#39;r\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) //打开上面经过分词后的txt文件 s=\u0026#34;\u0026#34; data={} for line in fr: line=line.strip() if len(line)==0: continue if line[0]==\u0026#39;2\u0026#39;: continue for x in range(0,len(line)): if line[x] in [\u0026#39; \u0026#39;,\u0026#39;\\t\u0026#39;,\u0026#39;\\n\u0026#39;,\u0026#39;。\u0026#39;,\u0026#39;，\u0026#39;,\u0026#39;[\u0026#39;, \u0026#39;]\u0026#39;, \u0026#39;（\u0026#39;, \u0026#39;）\u0026#39;, \u0026#39;:\u0026#39;, \u0026#39;-\u0026#39;, \u0026#39;？\u0026#39;, \u0026#39;！\u0026#39;, \u0026#39;《\u0026#39;, \u0026#39;》\u0026#39;, \u0026#39;、\u0026#39;, \u0026#39;；\u0026#39;, \u0026#39;“\u0026#39;, \u0026#39;”\u0026#39;, \u0026#39;……\u0026#39;,\u0026#39;0\u0026#39;,\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;5\u0026#39;,\u0026#39;6\u0026#39;,\u0026#39;7\u0026#39;,\u0026#39;8\u0026#39;,\u0026#39;9\u0026#39;,\u0026#39;=\u0026#39;,\u0026#39;~\u0026#39;,\u0026#39;…\u0026#39;]: //去除一些不希望被统计的东西 continue s+=str(line[x]) print(s) seg_list = jieba.cut(s, cut_all=False, HMM=True) for word in seg_list: if len(word)\u0026gt;=2: if not data.__contains__(word): data[word]=0 data[word]+=1 data=sorted(data.items(),key=lambda d:d[1],reverse=True) fw=open(\u0026#39;安工程失物招领群1词频统计.csv\u0026#39;,\u0026#39;w\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) //生成词频统计表 fw.write(str(len(data))+\u0026#39;\\n\u0026#39;) for item in data: fw.write(item[0]+\u0026#39;,\u0026#39;+str(item[1])+\u0026#39;\\n\u0026#39;) fr.close() fw.close() 这段代码结合上面分词的代码，即可生成统计表，近期来群里关键字出现的字眼一目了然 失物招领群1词频统计 谢谢的次数即代表我们一个月发过的消息总数，因为我们要求每条消息后面必须加个谢谢 由图可见，我们一个月发了1910条消息 其中校园卡类876条 寻物消息585条 失物消息526条 身份证消息41条 等等\n","permalink":"https://csqread.top/posts/tech/python%E5%AF%B9%E5%A4%B1%E7%89%A9%E6%8B%9B%E9%A2%86%E7%BE%A4%E8%BF%9B%E8%A1%8C%E8%AF%8D%E9%A2%91%E5%88%86%E6%9E%90/","summary":"上午两节课睡过了，就没去了，吃了个早饭，还有一堆事情没有做，就开始先把昨晚没弄完的python词频分析的代码给写完了。做一个记录吧，防止以后","title":"Python对失物招领群进行词频分析"},{"content":"“可是为什么要禁掉呢？”野人问道。遇见一位读过莎士比亚的人，使他一时忘了形。 元首耸耸肩膀：“”因为这本书太旧了，这是主要原因。旧东西在我们这儿是毫无用处的。 “即使他们是美好的？” “特别因为他们是美好的。美好便有吸引力了，而我们不要人们被旧的东西吸引。我们要他们喜欢新的。” “可是新的东西确是那么的愚昧和可怕。那些戏剧，空洞无物，只有直升机飞来飞去，而你感受到人家再接吻”他蹙眉颦额，“一群山羊和猴子！”只有《奥赛罗》里的字句才能适切地表达他的轻蔑和憎恨。 “然而是驯养好的好兽呢。”元首低声插嘴。 “你为什么不换成《奥赛罗》给他们看呢？” “我告诉过你了，那个旧了。此外，它们不可能懂得。” 对，这是真话。他记起汉姆赫兹怎样地嘲笑《罗密欧与朱丽叶》。“好吧，那么，”他停顿了一下，“一些像《奥赛罗》地新东西，他们能懂得东西” “那正是我们一直在写的”，汉姆赫兹打破了长时间的沉默说道。 “而那也正是你永远写不出来的。”元首说，“因为，如果那真的像《奥赛罗》，无论怎么新也不会有人懂得。而如果是新的就不可能像《奥赛罗》。” “为什么不可能？” “因为我们的世界不像奥赛罗的世界。没有钢铁你就造不出汽车，同理，没有不安定的社会你就造不出悲剧。今天的世界是安定的。人们很快乐，他们要什么就会得到什么，而他们永远不会要他们得不到的。”\n后面的这些对话确实发人深省\n2503年，一个婴儿养育室里。护士们在地板上摆了一堆图书和鲜花，然后把一群长得一摸一样的、8个月大的婴儿放到了地板上。婴儿们看到图书和鲜花，飞快地爬过去，拿起来玩耍。这时候，长官一声令下，护士长启动电路装置，一时间，刺耳的警报响起，地板被通上了电，触电的婴儿们在痛苦中痉挛并尖叫不已。过了一会儿，护士长关上了电闸。 “这样的试验大约重复200次左右，”长官微笑着对参观者说：“这些孩子们就会对图书和花朵形成本能的憎恨，他们的条件反射就这样被限定了。” “限定”，大约是《Brave New World》一书中的最关键词汇。在Aldous Huxley笔下的那个美好盛世里，人从受精开始就被“限定”了。精子和卵子在试管里被调制好，不健康的胚胎被“限定”出局，健康胎儿在孵化器里大。然后从婴儿养育室开始，孩子们一路被“限定”得厌恶书籍和自然、厌恶独处、厌恶家庭、厌恶宗教和艺术，同时被“限定”得热爱集体、热爱消费、热爱滥交。 当然，并不是所有的人被限定的方式都一样。美好新世界里，人类被分成了五级，Alpha、Beta、Gamma、Delta以及Epsilon——Alpha被限定得聪明漂亮，而Gamma以下的人不但被限定得矮小愚钝，还批量生产。不过 没关系，虽然在那个世界里人有等级贵贱，但是他们都一样幸福——因为无论哪个等级，其接受的“睡梦教育”都会告诉他，他所在的等级最美好最幸运。 这样的世界，有什么问题吗？ 美好新世界的首长Mustapha，问质疑者“野人”John。 有什么人类跋山涉水追求了几千年的东西，新世界里没有呢？经济发展？新世界里如此富足，上至Alphas下至Epsilons，人们不愁吃穿。健康？生物学家们早就把人类限定得不再有疾病。青春？这里人们青春永驻，直到突然死亡。美女帅哥的青睐？这个更不用担心，因为新世界里“每个人都属于他人”，滥交是最大的美德，你要是长期只跟一个美女上床，会成为该世界里骇人的丑闻。 不错，这个世界里没有艺术、诗歌、撕心裂肺的爱情、没有毕加索或者莎士比亚，但是，当你每天都幸福得晕眩时，为什么还会需要毕加索或者莎士比亚？文学艺术往往是为了表达冲突超越痛苦，那么，在一个冲突和痛苦根本不存在的世界里，文学艺术也就变成了社会的阑尾。更不要说“爱情”，那简直是高速公路上突然蹦出来的一头羚羊，如此危险，通通地，限定了之。 所以，这样的世界，有什么问题吗？ 柏拉图估计不会觉得有什么问题，因为新世界里政治家和科学家就是智慧非凡的哲学王。老子估计也不会觉得有什么问题，“劳心者治人，劳力者治于人”在这个桃花源里被充分实施。希特勒更是会欣喜若狂，因为将人类的未来当作一个巨大的生物工程来建设，简直是他的毕生追求。还有斯大林，荡漾在新世界人们脸上的微笑，与沉浸在丰收喜悦里的社会主义农民如出一辙，而新世界的“睡梦教育”，简直可以说是对苏式灌输教育赤裸裸的抄袭。所有那些信奉“精英治国”、信奉“稳定高于一切”、信奉“老百姓无非就是关心吃饱穿暖”的人，都会是“美好新世界”的热情粉丝。 这个新世界如此美好，它只有一个小小的缺陷——在那里，幸福的人们全都是“被幸福”的。 就是说，在那里，人们的幸福是政治家和科学家呕心沥血的科研成果，与每个个体自己的创造力、情感体验能力、审美能力都毫无关系。 民众只需像儿童那样，系上围兜，张口吞下哲学王或者先锋队一勺一勺送过来的食物，就乘坐直升电梯抵达了极乐世界。而精英们为了民众，制作食物既考虑营养，又考虑消化，可以说是殚精竭虑。有如此鞠躬尽瘁的统治者，民众的个体自由意志完全是多此一举。 如果说奥威尔的《1984》里，人们为失去自由而痛苦，那么Huxley的《勇敢新世界》里，人们则为摆脱了自由的重负而狂喜。真的，如果政治家科学家给民众带来如此丰盛的快乐，民众何必要自己去斗争？就像如果你可以从父亲那里继承一大笔遗产，何必要自己去辛苦挣钱？除非—— 你认为得到的过程比得到本身更有意义。除非你不识抬举地认为，通过个体努力去争取幸福比“被幸福”更体现生命的价值。 也正是在这个意义上，我在一切精英治国观里读到的是对生命的藐视。当统治者的恩赐被视为民众幸福的源泉时，统治者越高大，民众就越渺小。对有些人来说，幸福如此简单，无非是对着送过来的汤勺不断张嘴，而对另一些人来说，它如此复杂，需要汗滴禾下土粒粒皆辛苦。由于运气和能力，也许耕耘未必能带来收获，但是恩赐来的幸福和捕猎来的痛苦之间，你选什么呢？在幸福药丸soma和跌宕起伏的莎士比亚之间，野人John选择了莎士比亚。但是当然，对于美好新世界里的绝大多数人，这根本不是一个问题。他们从来没有选择的权利，无处不在的幸福不由分说，一把把他们给罩住，他们只能躺在幸福的牙缝里，被咀嚼，然后变成一堆残渣，被气势磅礴地给吐出来。\n","permalink":"https://csqread.top/posts/read/%E7%BE%8E%E4%B8%BD%E6%96%B0%E4%B8%96%E7%95%8C%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","summary":"“可是为什么要禁掉呢？”野人问道。遇见一位读过莎士比亚的人，使他一时忘了形。 元首耸耸肩膀：“”因为这本书太旧了，这是主要原因。旧东西在我们这","title":"《美丽新世界》读书笔记"},{"content":"一直以来，我在想做一个qq机器人，能够实现qq消息的转发，因为对于失物招领工作，其他的都挺满意的，但每天发那么多消息确实很浪费时间，所以就想做一个能够实现自动转发消息的机器人。对于这个需求如何实现，我也构思了很久。这个想法是大二下学期就有的，但一直没有去实现。暑假的时候在家中自学了前端，准备做好一个失物招领网站，但那又是一个半吊子项目，网站并没有完全达到我想要的那种结果。现在来做一个博客，来详细说明我的机器人是如何实现的：\n在gooogle的时候，发现了一个很好的框架，一位github大佬写的qqbot框架。于是乎我们只需要调用他的api，以及他的框架来实现我们所想要的需求就可以了。当然，这是需要用python来实现的，所以需要会一些python的基础语法。\n下面介绍github这位大佬的qqbot框架：\n一、介绍 qqbot 是一个用 python 实现的、基于腾讯 SmartQQ 协议的 QQ 机器人，可运行在 Linux, Windows 和 Mac OSX 平台下。 本项目 github 地址： https://github.com/pandolia/qqbot\n你可以通过扩展 qqbot 来实现：\n监控、收集 QQ 消息 自动消息推送 聊天机器人 通过 QQ 远程控制你的设备\n二、安装方法 在 Python 2.7/3.4+ 下使用，用 pip 安装： pip install qqbot 或者下载 源码 解压后 cd 到该目录并运行： pip install.\n三、使用方法 1. 启动 QQBot 在命令行输入： qqbot ，即可启动一个 QQBot 。 启动过程中会自动弹出二维码图片，需要用手机 QQ 客户端扫码并授权登录。启动成功后，会将本次登录信息保存到本地文件中，下次启动时，可以输入： qqbot -q qq号码 ，先尝试从本地文件中恢复登录信息（不需要手动扫码），只有恢复不成功或登录信息已过期时才会需要手动扫码登录。一般来说，保存的登录信息将在 2 天之后过期。\n注意： Linux 下，需要系统中有 gvfs-open 或者 shotwell 命令才能自动弹出二维码图片（一般安装有 GNOME 虚拟文件系统 gvfs 的系统中都会含这两个命令之一）。 Windows10 下，需要系统中已设置了 png 图片文件的默认打开程序才能自动弹出二维码图片。\n若系统无法自动弹出二维码图片，可以手动打开图片文件进行扫码，也可以将二维码显示模式设置为 邮箱模式 、 服务器模式 或 文本模式 进行扫码，详见本文档的第七节。\n操作 QQBot QQBot 启动后，在另一个控制台窗口使用 qq 命令操作 QQBot ，目前提供以下命令： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 1） 帮助、停机和重启命令 qq help|stop|restart|fresh-restart 2） 联系人查询、搜索命令 qq list buddy|group|discuss [$cinfo|$clike] ( $cinfo --\u0026gt; $qq|$name|$key=$val ) ( $clike --\u0026gt; :like:$qq|:like:$name|$key:like:$name ) qq list group-member|discuss-member $oinfo|$olike [$cinfo|$clike] ( $oinfo --\u0026gt; $oqq|$oname|$okey=$oval ) ( $cinfo --\u0026gt; $qq|$name|$key=$val ) ( $olike --\u0026gt; :like:$oqq|:like:$oname|$okey:like:$oname ) ( $clike --\u0026gt; :like:$qq|:like:$name|$key:like:$name ) 3） 联系人更新命令 qq update buddy|group|discuss qq update group-member|discuss-member $ginfo 4） 消息发送命令 qq send buddy|group|discuss $rinfo $message 5） 加载/卸载/显示插件 qq plug/unplug myplugin qq plugins list 命令提供强大的联系人查询和搜索功能，用法示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 列出所有好友 qq list buddy # 列出 名称 为 xxx 的群 qq list group xxx # 列出备注名为 jack 的好友 qq list buddy mark=jack # 列出 群“456班” 的所有成员 qq list group-member 456班 # 列出 群“456班” 中名片为 “mike” 的成员 qq list group-member 456班 card=mike # 列出 讨论组“XX小组” 中名为 jack 的好友 qq list discuss-member XX小组 jack 其中第三、四个参数如果是 key=val 的格式，则应为 name=xx|nick=xx|mark=xx|card=xx|qq=xx 的格式，如果不是 key=val 的格式，则按以下原则进行处理：若是一串数字，则按 QQ 号进行查询，否则，按名称进行查询。\n如果存在重名现象，会列出所有重名的联系人。如：\n1 qq list group 机器人测试 将列出所有名为 “机器人测试” 的群。\n如果在 list 命令的第三、四个参数中加入 “:like:” ，则会按部分匹配的模式进行搜索，用法示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 列出名称中含有 “李” 的好友 qq list buddy :like:李 # 列出 QQ 中含有 “234” 的群 qq list group :like:234 # 列出备注名中含有 jack 的好友 qq list buddy mark:like:jack # 列出 群“456班” 的中名称中含有 “李” 的成员 qq list group-member 456班 :like:李 # 列出 群“456班” 中名片中含有 “mike” 的成员 qq list group-member 456班 card:like:mike # 列出的 讨论组“xx小组” 中名为 jack 的好友 qq list discuss-member :like:小组 jack 从 v2.2.5 版开始， list 命令采用表格的形式输出联系人列表，其输出样式示例如下：\n为保证表格在终端中的显示效果，建议将终端的输出字体设置为 consolas 、且每行可打印的最大字符数大于 120 。另外需要注意：为保证表格的显示效果，当联系人的名称、名片等属性的长度太长或含有特殊字符时，将对这些属性进行截断或过滤后再输出至终端。\nupdate 命令更新指定的联系人列表，其参数含义和 list 命令相同，如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 更新好友列表 qq update buddy # 更新群列表 qq update group # 更新 群“456班” 的成员列表 qq update group-member 456班 send 命令中第三个参数和 list 命令中的第三个参数格式一致。要注意，如果有重名现象，会给所有重名的联系人发信息。 另外要注意，第二个参数只能是 buddy/group/discuss ，不能是 group-member/discuss-member 。示例： # 给 好友“jack” 发消息 “你好” qq send buddy jack 你好 # 给 群“198班” 发消息 “大家好” qq send group 198班 大家好 # 给 QQ 为 12345 的好友发消息 qq send buddy 12345 xxx # 给讨论组发消息 qq send discuss MyDiscuss hello 可以在消息内容中嵌入“/可爱”等表情关键词来向对方发送表情，详见 facemap.py。还可以在消息内容中使用 \\n,\\t 这两个转义字符（如： send buddy jack 第一行\\n第二行）。\n以上所有命令都提供对应的 HTTP API 接口，供 web 前端开发者调用，接口的 url 地址为 http://127.0.0.1:8188/{command} ，只需要将 qq 后面的命令各参数用 \u0026ldquo;/\u0026rdquo; 分隔开替换 url 中的 command 就可以了，如: http://127.0.0.1:8188/send/buddy/jack/hello ，其他示例详见 urltestbot.md 。注意：如果命令中含有中文或特殊字符，需要先进行 url 编码（ utf8 ），例如，调用 http://127.0.0.1:8188/send/buddy/jack/nihao%20%E4%BD%A0%E5%A5%BD%20wohao 将发送消息 ”nihao 你好 wohao“ 。（提示：在 JavaScript 中，可以使用 encodeURIComponent 函数进行编码）。\n另外， QQBot 启动后，用本 QQ 号在其他客户端（如：手机 QQ ）上向某个 群/讨论组 发消息 “\u0026ndash;version” ，则 QQBot 会自动在该 群/讨论组 回复： “QQBot-v2.x.x” 。\n四、实现你自己的 QQ 机器人 实现自己的 QQ 机器人非常简单，只需要定义一个自己的消息响应函数并按插件加载。示例代码： ``` # -*- coding: utf-8 -*- def onQQMessage(bot, contact, member, content): if content == \u0026lsquo;-hello\u0026rsquo;: bot.SendTo(contact, \u0026lsquo;你好，我是QQ机器人\u0026rsquo;) elif content == \u0026lsquo;-stop\u0026rsquo;: bot.SendTo(contact, \u0026lsquo;QQ机器人已关闭\u0026rsquo;) bot.Stop()\n1 2 3 4 5 6 7 8 9 注意，上面注册的响应函数的函数名必须为 “onQQMessage” ，函数参数也必须和上面的一致。 将以上代码另存为 sample.py （注意保存为 utf8 编码的文件）。放到 ~/.qqbot-tmp/plugins/ 目录下（ ~ 代表用户主目录， win7 下为 C:\\Users\\xxx ），或系统中可以 import 到的目录下（如 python 的安装目录下的 Lib/site-packages 目录）。 之后，保持前面的 qqbot 进程运行，在另一个控制台输入 qq plug sample ，则可将此文件中的 onQQMessage 函数注册到 QQBot 的相应事件上去。此时，用另外一个 QQ 向本 QQ 发送消息 “-hello”，则会自动回复 “你好，我是 QQ 机器人”，发送消息 “-stop” 则会关闭 QQ 机器人。 在控制台输入 qq unplug sample 可以卸载此插件及相应的回调函数。可以同时加载多个插件，此时各插件中的相应函数会依次被调用（但调用顺序和加载次序无关）。 QQBot 开始运行后，每收到一条 QQ 消息，会将消息来源、消息内容以及一个 QQBot 对象传递给已注册的消息响应函数。其中： bot : QQBot 对象，提供 List/SendTo/Stop/Restart 等接口，详见本文档第五节 contact : QContact 对象，消息的发送者，具有 ctype/qq/uin/nick/mark/card/name 等属性 member : QContact 对象，仅当本消息为 群消息或讨论组消息 时有效，代表实际发消息的成员 content : str 对象，消息内容 contact 代表消息发送者，其 ctype 属性可以为 buddy/group/discuss ，代表 好友/群/讨论组 对象，表示本消息是 好友消息/群消息/讨论组消息 。\n1 2 3 4 5 6 7 8 9 10 11 12 member 仅当本消息为 群消息或讨论组消息 时有效，代表实际发消息的成员，它的 ctype 属性可以为 group-member/discuss-member ，代表 群成员/讨论组成员 对象。当本消息为 好友消息 时， member 等于 None 。 contact 和 member 都是 QContact 对象，不同类型的 QContact 对象所具有的属性含义见： qcontact-attr 。注意所有 QContact 对象都是 只读对象 ，只能读取它的属性，不能设置它的属性，也不能向它添加额外的属性。 可以调用 QQBot 对象的 SendTo 接口向 QContact 对象发送消息，但要注意：只可以向 好友/群/讨论组 发消息， 不可以向 群成员/讨论组成员 发送消息 。也就是说，只可以调用 bot.SendTo(contact, \u0026#39;xxx\u0026#39;) ， 不可以调用 bot.SendTo(member, \u0026#39;xxx\u0026#39;) 。 \u0026lt;h1\u0026gt;五、 QQBot 对象的公开接口和属性\u0026lt;/h1\u0026gt; QQBot 对象提供 List/Update/SendTo/Plug/Unplug/Login/Stop/Restart/FreshRestart 共计 9 个公开接口，这些接口的第一个字母都是大写的。另外，提供一个公开属性 conf 保存全局的配置信息。 一般情况下，请勿 调用/存取 此对象的其他 方法/属性 。特别的， 请勿在子线程中调用这些接口 。 以下介绍前 7 个接口和 conf 属性。 如果需要在 IDE 或 python-shell 中运行或测试以上接口，需要先关闭 qqbot 进程，并在 IDE 或 python-shell 中运行以下代码进行登录： from qqbot import _bot as bot bot.Login([\u0026rsquo;-q\u0026rsquo;, \u0026lsquo;1234\u0026rsquo;])\n1 2 3 4 5 6 （1） bot.List(tinfo, [cinfo]) --\u0026gt; [contact0, contact1, ..., ]/[]/None 对应本文档第三节的 list 命令。返回联系人对象（ QContact 对象）列表或者 None 。第一个参数 tinfo 是联系人列表的代号，第二个参数是可选的（和 list 命令的第三个参数格式一致）。 参数 tinfo 用来代表某个联系人列表，该参数在联系人的查询中非常重要，请务必理解以下两种情况 ： tinfo 的含义（情况1）： tinfo 可以为 buddy/group/discuss ，分别代表 好友列表/群列表/讨论组列表 。示例： 返回 好友列表： bot.List(\u0026lsquo;buddy\u0026rsquo;)\n返回名为 \u0026lsquo;jack\u0026rsquo; 的好友的列表： bot.List(\u0026lsquo;buddy\u0026rsquo;, \u0026lsquo;jack\u0026rsquo;)\n返回 群列表： bot.List(\u0026lsquo;group\u0026rsquo;)\n返回名为 “机器人测试” 的群的列表： bot.List(\u0026lsquo;group\u0026rsquo;, \u0026lsquo;机器人测试\u0026rsquo;)\n1 tinfo 的含义（情况2）： tinfo 也可以是一个 ctype 等于 group/discuss 的 QContact 对象，代表该 群/讨论组 的成员列表。如以下第二句和第三句分别返回 群“456班” 的成员列表和该群中名片为 “jack” 的成员列表： g = bot.List(\u0026lsquo;group\u0026rsquo;, \u0026ldquo;456班\u0026rdquo;)[0] # g 是一个 Group 对象（群“456班”） bot.List(g) # 返回 群“456班” 的成员列表 bot.List(g, \u0026lsquo;card=jack\u0026rsquo;) # 返回 群“456班” 中名片为 “jack” 的成员列表 注意上面第三句不允许是 bot.List(g, card=\u0026lsquo;jack\u0026rsquo;) 的格式。\n1 2 3 4 5 6 7 List 接口的内部执行顺序： 首先在 QQBot 的联系人数据库内查找 tinfo 所代表的联系人列表；若数据库内已有此列表，则在此列表内进行搜索，并返回一个包含 “此列表中所有和 cinfo 匹配的联系人” 的列表；若数据库内没有此列表，则向 QQ 服务器请求数据获取联系人列表，获取成功后将联系人列表保存到数据库内，然后再进行搜索并返回一个包含 “此列表中所有和 cinfo 匹配的联系人” 的列表；如果在向 QQ 服务器请求数据的过程中出错了，则打印相关的失败信息，并返回 None 。 List 接口返回值的含义： 返回一个非空列表表示 tinfo 所指定的联系人列表内所有和 cinfo 匹配的联系人；返回一个空列表表示该联系人列表内没有和 cinfo 匹配的联系人；返回 None 表示向 QQ 服务器请求联系人列表和资料失败，不知道是否有相匹配的联系人。 调用 List 接口后， 务必 先根据以上三种情况对返回值进行判断，然后再执行后续代码。 注意： 当 List 接口返回非空列表时，列表内的元素是 QContact 对象，而不是 str 对象： g = bot.List(\u0026lsquo;group\u0026rsquo;)[0] # g 是一个 Group 对象 print([g, type(g), g.qq, g.name, g.uin, g.mark])\t# 打印 g 的各项属性\n1 2 3 4 5 6 7 8 不同类型的 QContact 对象所具有的属性含义见： qcontact-attr 。 （2） bot.Update(tinfo) --\u0026gt; True/False Update 接口的参数 tinfo 和 List 接口中的参数含义相同，调用此接口会立即向 QQ 服务器请求相应的联系人列表并更新联系人数据库，并一直阻塞至更新成功。更新最慢的是好友列表，若好友较多可能会阻塞 5 ~ 10 秒。成员列表更新的较快，即便是 2000 人的大群，更新时间仅 1 ~ 2 秒。 若更新成功，返回 True ，否则，返回 False 。 示例： 更新 好友列表 ： bot.Update(\u0026lsquo;buddy\u0026rsquo;)\n更新 群列表 ： bot.Update(\u0026lsquo;group\u0026rsquo;)\n更新 某个群的成员列表 ： gl = bot.List(\u0026lsquo;group\u0026rsquo;, \u0026ldquo;456班\u0026rdquo;) if gl: g = gl[0] bot.Update(g)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 （3） bot.SendTo(contact, content, resendOn1202=True) --\u0026gt; \u0026#39;向 xx 发消息成功\u0026#39;/\u0026#39;错误：...\u0026#39; 向联系人发送消息。第一个参数为 QContact 对象，第二个参数为消息内容。再次提醒： 只可以向 好友/群/讨论组 发消息， 不允许向 群成员/讨论组成员 发消息 。 可以在消息内容中嵌入“/微笑”等表情关键词来向对方发送表情，详见 facemap.py 。 若发送成功，返回字符串（向 xx 发消息成功）。否则，返回含错误原因的字符串（错误：...）。 发消息时可能会重复发消息，这是因为 QQ 服务器返回代码 1202 的原因。v2.1.17版已针对此问题在 bot.SendTo 接口中增加了一个参数： resendOn1202 ，若此参数为 True （默认值），则发消息时如果 QQ 服务器返回代码 1202 （表明发消息可能失败），还会继续发送 3 次，直至返回代码 0 ， 若此参数为 False ，则不会尝试重发。 设为 True 在绝大部分情况下能保证消息一定能发出去，但缺点是有时一条消息会重复发送。设为 False 则相反，消息不会重复发送，但有时消息发送不出去。 总之因为这个 1202 代码的不确定性，没有完美的解决办法。请根据各自的实际情况选择 resendOn1202 的值。 第一个参数 contact 必须是通过 bot.List 返回的 QContact 对象、或回调函数 onQQMessage 传递进来的第一个参数。示例： 向 昵称 为 jack 的好友发消息 bl = bot.List(\u0026lsquo;buddy\u0026rsquo;, \u0026lsquo;jack\u0026rsquo;) if bl: b = bl[0] bot.SendTo(b, \u0026lsquo;hello\u0026rsquo;)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 （4） bot.conf bot.conf 中保存全局的配置信息，各项配置详见本文档第七节。如 bot.conf.termServerPort 保存 QQBot 命令行服务器的端口号， bot.conf.qq 保存本次登录的 QQ 号码。 注意： bot.conf 中保存的配置信息是只读的，请勿修改这些配置信息。 \u0026lt;h1\u0026gt;六、 注册回调函数、被他人 @ 的通知、判断是否是自己发的消息、定制定时任务\u0026lt;/h1\u0026gt; 注册回调函数 除了上面提到的 onQQMessage 响应函数，还可以注册 onInit/onQrcode/onStartupComplete/onInterval/onUpdate/onPlug/onUnplug/onExit 共计九种事件的回调函数，所有事件的回调函数参数格式、含义及示例详见 sampleslots.py 。 程序的运行流程以及各回调函数的调用时机如下： \u0026lt;img src=\u0026#34;define-my-qqbot/main.png\u0026#34;\u0026gt; 再次提醒：注册的回调函数的函数名以及函数参数（数量和名称）都不得更改 。 被群内其他成员 @ 的通知 QQBot 收到群消息时，会先根据消息内容判断是否有人 @ 自己。如果是，则在消息内容的开头加一个 [@ME] 的标记，再传递给 onQQMessage 函数；否则，将消息内容中的所有 @ME 替换成 @Me 再传给 onQQMessage 。因此，在 onQQMessage 函数内，只需要判断 content 内是否含有 @ME 就知道自己是否被消息发送者 @ 了。例如： def onQQMessage(bot, contact, member, content): if \u0026lsquo;@ME\u0026rsquo; in content: bot.SendTo(contact, member.name+\u0026rsquo;，艾特我干嘛呢？\u0026rsquo;)\n1 2 3 4 请注意，若群内有另一个成员的名字和自己的名字的开头部分相同（如：自己的名字是 ab ，另一个成员的名字是 abc ），那么当有人 @abc 时，也会误报成 @ME ，在这种情况下，需要修改自己的群名片，以免误报。 判断是否是自己发的消息 当本 QQ 发消息时， QQBot 也会收到一条同样的消息， bot 对象提供一个 isMe 方法来判断是否是自己发的消息： def onQQMessage(bot, contact, member, content): if bot.isMe(contact, member): print(\u0026lsquo;This is me\u0026rsquo;)\n1 2 定制定时任务 从 2.1.13 起， qqbot 提供一个功能强大的函数装饰器 -- qqbotsched 来定制定时任务，示例代码： from qqbot import qqbotsched\n@qqbotsched(hour=\u0026lsquo;11,17\u0026rsquo;, minute=\u0026lsquo;55\u0026rsquo;) def mytask(bot): gl = bot.List(\u0026lsquo;group\u0026rsquo;, \u0026lsquo;456班\u0026rsquo;) if gl is not None: for group in gl: bot.SendTo(group, \u0026lsquo;同志们：开饭啦啦啦啦啦啦！！！\u0026rsquo;)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 以上代码以插件形式加载后，每到 11:55 和 17:55 ，都会自动向 群“456班” 发送消息：“同志们：开饭啦啦啦啦啦啦！！！” 。 qqbotsched 装饰器接受 year, month, day, week, day_of_week, hour, minute, second, start_date, end_date, timezone 共计 11 个关键字参数，每个参数表示任务的定制时间的分量所应匹配的值。例如： hour=\u0026#39;11,17\u0026#39; 表示应在 11:xx 或 17:xx 执行任务， minute=\u0026#39;55\u0026#39; 表示应在 xx:55 执行任务， minute=\u0026#39;0-55/5\u0026#39; 表示应在 xx:00, xx:05, xx:10, ..., xx:55 执行任务， day_of_week=\u0026#39;mon-fri\u0026#39; （或 \u0026#39;0-4\u0026#39; ） 表示应在 星期一 ~ 星期五 执行任务。 qqbotsched 是对 Python 的定时任务框架 apscheduler 的简单封装，其各项参数应采用 Unix 系统中的 crontab 格式输入。有关 crontab 以及 Python 的定时任务框架 apscheduler 的内容可参见以下参考资料： https://code.tutsplus.com/tutorials/scheduling-tasks-with-cron-jobs--net-8800/ http://apscheduler.readthedocs.io/en/latest/userguide.html https://lz5z.com/Python定时任务的实现方式/ http://debugo.com/apscheduler/ crontab 各项参数格式说明详见： http://apscheduler.readthedocs.io/en/latest/modules/triggers/cron.html 注册回调函数和定制定时任务的注意事项 注册回调函数和定制定时任务是对 QQBot 进行扩展的唯一方式，在编写这些函数时，请注意以下事项： 回调函数的函数名、参数名、参数数量、参数顺序都不得更改 定时任务的函数名可以自己定义，但参数有且只有一个，参数名必须为 bot ，为一个 QQBot 对象。 所有回调函数和定时任务都将在主线程中被依次调用，因此不必担心全局变量的线程安全问题。 回调函数和定时任务的运行时间应尽量短，尽量不要再这些函数中进行阻塞式的操作，否则会阻塞整个程序的运行。一般来说，每个函数的运行时间在 5 秒以内是可以接受的。 绝对不要 在回调函数、定时任务或 qqbot 主线程的内部调用 os.system 执行 本 QQ 号对应的 qq 命令 （ 如 os.system(\u0026#39;qq send buddy jack hello\u0026#39;) ）或请求 本 QQ 号对应的 HTTP-API 接口 ，否则整个程序会形成死锁（因为 os.system 要等 qq 命令执行完成后才返回、而 qq 命令要等 os.system 返回后才会被执行）。请直接使用 bot 的 SendTo/List 等接口。 \u0026lt;h1\u0026gt;七、二维码管理器、QQBot 配置、命令行参数以及工作目录\u0026lt;/h1\u0026gt; 二维码的显示模式 WebQQ 登录时需要用手机 QQ 扫描二维码图片，在 QQBot 中，二维码图片可以通过以下四种模式显示： GUI模式： 在 GUI 界面中自动弹出二维码图片 邮箱模式： 将二维码图片发送到指定的邮箱 服务器模式： 在一个 HTTP 服务器中显示二维码图片 文本模式： 在 Term 中以文本形式展示二维码(需要自行安装 pillow 和 wcwidth 库) GUI 模式是默认的模式，只适用于个人电脑。邮箱模式可以适用于个人电脑和远程服务器。服务器模式一般只在有公网 ip 的系统中使用。如果使用 QQ 邮箱来接收二维码，则发送二维码图片之后，手机 QQ 客户端会立即收到通知，在手机 QQ 客户端上打开邮件，再长按二维码就可以扫描了。文本模式方便在开发过程或者服务器部署时使用，为开发者提供快捷方式登陆 QQ 。 注意：当开启了 邮箱模式/服务器模式/文本模式 时， GUI 模式是关闭的，登陆时不会自动弹出二维码图片。 每次登录时会创建一个二维码管理器 （ QrcodeManager 对象） ，二维码管理器会根据配置文件及命令行参数来选择二维码图片的显示方式。 配置文件的使用方法 配置文件为 ~/.qqbot-tmp/v2.x.conf （ ~ 代表用户主目录， win7 下为 C:\\Users\\xxx ， linux 下为 /home/xxx ），第一次运行 QQBot 后就会自动创建这个配置文件，其中内容如下： {\n# QQBot 的配置文件 # 使用 qqbot -u somebody 启动程序时，依次加载： # 根配置 -\u0026gt; 默认配置 -\u0026gt; 用户 somebody 的配置 -\u0026gt; 命令行参数配置 # 使用 qqbot 启动程序时，依次加载： # 根配置 -\u0026gt; 默认配置 -\u0026gt; 命令行参数配置 # 用户 somebody 的配置 \u0026quot;somebody\u0026quot; : { # QQBot-term （HTTP-API） 服务器端口号（该服务器监听 IP 为 127.0.0.1 ） # 设置为 0 则不会开启本服务器（此时 qq 命令和 HTTP-API 接口都无法使用）。 \u0026quot;termServerPort\u0026quot; : 8188, # 二维码 http 服务器 ip，请设置为公网 ip 或空字符串 \u0026quot;httpServerIP\u0026quot; : \u0026quot;\u0026quot;, # 二维码 http 服务器端口号 \u0026quot;httpServerPort\u0026quot; : 8189, # 自动登录的 QQ 号 \u0026quot;qq\u0026quot; : \u0026quot;3497303033\u0026quot;, # 接收二维码图片的邮箱账号 \u0026quot;mailAccount\u0026quot; : \u0026quot;3497303033@qq.com\u0026quot;, # 该邮箱的 IMAP/SMTP 服务授权码 \u0026quot;mailAuthCode\u0026quot; : \u0026quot;feregfgftrasdsew\u0026quot;, # 是否以文本模式显示二维码 \u0026quot;cmdQrcode\u0026quot; : False, # 显示/关闭调试信息 \u0026quot;debug\u0026quot; : False, # QQBot 掉线后自动重启 \u0026quot;restartOnOffline\u0026quot; : False, # 在后台运行 qqbot ( daemon 模式) \u0026quot;daemon\u0026quot;: False, # 完成全部联系人列表获取之后才启动 QQBot \u0026quot;startAfterFetch\u0026quot; : False, # 插件目录 \u0026quot;pluginPath\u0026quot; : \u0026quot;.\u0026quot;, # 启动时需加载的插件 \u0026quot;plugins\u0026quot; : [], # 插件的配置（由用户自定义） \u0026quot;pluginsConf\u0026quot; : {}, }, # 可以在 默认配置 中配置所有用户都通用的设置 \u0026quot;默认配置\u0026quot; : { \u0026quot;qq\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;pluginPath\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;plugins\u0026quot; : [ 'qqbot.plugins.sampleslots', 'qqbot.plugins.schedrestart', ], \u0026quot;pluginsConf\u0026quot; : { 'qqbot.plugins.schedrestart': '8:00', } }, # # 注意：根配置是固定的，用户无法修改（在本文件中修改根配置不会生效） # \u0026quot;根配置\u0026quot; : { # \u0026quot;termServerPort\u0026quot; : 8188, # \u0026quot;httpServerIP\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;httpServerPort\u0026quot; : 8189, # \u0026quot;qq\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;mailAccount\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;mailAuthCode\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;cmdQrcode\u0026quot; : False, # \u0026quot;debug\u0026quot; : False, # \u0026quot;restartOnOffline\u0026quot; : False, # \u0026quot;daemon\u0026quot; : False, # \u0026quot;startAfterFetch\u0026quot; : False, # \u0026quot;pluginPath\u0026quot; : \u0026quot;\u0026quot;, # \u0026quot;plugins\u0026quot; : [], # \u0026quot;pluginsConf\u0026quot; : {} # }, }\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 可以在配置文件中添加自己的用户配置（即在该文件的字典中新增一个 item ，此 item 的 key 就代表一个用户），例如，该文件中已有的 somebody 项目就代表名为 somebody 的用户，运行 QQBot 时，输入 qqbot -u somebody ，则会加载 somebody 项目下的各项配置。 下面介绍配置文件中各项配置的功能，以下内容均假定已修改了 somebody 下的配置，且以 qqbot -u somebody 的方式运行。 邮箱模式的配置（ mailAccount 和 mailAuthCode ） 如果需要使用邮箱模式显示二维码，可以将 mailAccount 和 mailAuthCode 项中分别设置为邮箱帐号和授权码，运行后，二维码管理器会将二维码图片发送至该邮箱。 注意：授权码不是邮箱的登录密码，而是邮箱服务商提供的开通 IMAP/SMTP 服务的授权码（提醒：不是 POP3/SMTP 服务）， QQ/网易 邮箱可以在网页版的邮箱设置里面开通此项服务，并得到授权码。如果只定义了 mailAccount 而没定义 mailAuthCode ，则程序运行的开始时会要求手工输入此授权码。 邮箱模式已在 QQ 、 网易 和 Google 邮箱中测试过。 服务器模式的配置（ httpServerIP 和 httpServerPort ） 如果需要使用服务器模式，可以配置 httpServerIP 和 httpServerPort 项，一般来说应该设置为公网 ip 。服务器模式开启后，可以通过 http://{httpServerIP}:{httpServerPort}/{any} 来访问二维码图片。其中 {any} 可以是任何非空的数字或字母串。 当邮箱模式和服务器模式同时开启时，发邮件时不会发送真正的图片，只会将图片地址发到邮箱中去，而且只发送一次，二维码过期时刷新一下邮件就可以了。如果只开启邮箱模式，则发邮件时会发送真正的图片，当二维码过期时，需要将邮件设置为已读（用手机 QQ 点开邮件后该邮件就是已读了），之后才会发送最新的二维码图片。 文本模式显示二维码（cmdQrcode） 若 cmdQrcode 项设置为 True ，则会在 term 中以文本模式显示二维码。注意：要使用文本模式，需要自行安装 pillow 和 wcwidth 库，可使用 pip 安装。 自动登录的 QQ 号码（ qq ） 配置文件中每个用户都有 qq 这一项，若此项已设置为某 QQ 号码，则 QQBot 在启动时会先使用此 QQ 号上次登录保存的登录信息来自动登录。 掉线后自动重启（ restartOnOffline ） 如果配置文件中将 restartOnOffline 项设置为 True ，则当 QQBot 掉线或出错终止时，会自动重新启动 QQBot 。 在后台运行 qqbot （ daemon ） 此选项仅在 UNIX 类系统中有效，将配置中的 daemon 选项设置为 True 则会以 daemon 模式运行程序。此时，标准输出和标准错误会重定向到 daemon-$qq.log 文件（其中 $qq 是配置中 qq 选项的值）。 联系人列表获取完成后再启动（ startAfterFetch ） 一般情况下，扫码登录完成就立即启动 QQBot，只有在需要的时候才会去获取联系人列表并更新联系人数据库。如果将配置文件中的 startAfterFetch 设置为 True ，则 QQBot 会等待所有联系人列表获取完成后才启动 ，注意，如果联系人较多，会耗费较长的时间。 QQBot-term 服务器端口号（ termServerPort ） QQBot 启动后，会开启一个 QQBot-term 服务器监听用户通过 qq 命令行工具发过来的操作命令以及通过 HTTP API 接口发过来的操作命令，此服务器的监听 IP 永远为 127.0.0.1 ，监听端口号默认为 8188 ，可以通过修改 termServerPort 的值来修改此端口号。 如果配置的 QQBot-term 服务器端口号不是默认的 8188 ，那么在运行 qq 命令时，需要在第一个参数中指定端口号，如： $ qq 8100 send buddy jack hello $ qq 8100 list group-member chatbot 同样，HTTP API 接口的端口号也需要改变，如： http://127.0.0.1:8100/send/buddy/jack/hello 。 如果不需要使用 qq 命令和 HTTP-API 接口，可以将此端口号设置为 0 ，此时 QQBot-term 服务器不会开启。 如果需要在同一台机器上登录多个 QQ 号码，可以直接在不同的终端中开启多个 qqbot 进程进行登录，但是，每个 qqbot 进程必须设置专有的 termServerPort 和 httpServerPort （或者全部设置为 0 或 空值 ），否则会造成端口号冲突。 调试模式（ debug ） 若 debug 项设置为 True ，则运行过程中会打印调试信息。 插件的配置（ pluginPath 和 plugins ） 一般情况下，插件需要存放在系统的 import 目录下或 ~/.qqbot-tmp/plugins 目录下，可以在 pluginPath 选项中配置其他的存放目录。另外，在 plugins 选项中可以指定 QQBot 启动时需要加载的插件。 命令行参数及配置的优先级 配置文件中的所有选项都有对应的命令行参数，在命令行参数中输入的选项优先级比配置文件高。输入 qqbot -h 可查看所有命令行参数格式。 程序一共有四个级别的配置，其优先级如下： 使用 qqbot -u somebody 启动程序时，依次加载： 根配置 -\u0026gt; 默认配置 -\u0026gt; 用户 somebody 的配置 -\u0026gt; 命令行参数配置 使用 qqbot 启动程序时，依次加载： 根配置 -\u0026gt; 默认配置 -\u0026gt; 命令行参数配置 其中：根配置 是固定的，用户无法修改； 默认配置 和 用户配置 可由用户在 v2.x.conf 文件中进行修改；最后，还可以在 命令行参数 中输入配置。 工作目录 qqbot 运行时，会在 工作目录 下 搜索/创建 以下 文件/目录 ： 配置文件： v2.x.conf 插件目录： plugins/ 登录文件： v2.x-pyx-xxxx.pickle 联系人数据库文件： 2017-05-06-20-03-12-xxxx-contact.db 临时二维码图片： xxxx.png 保存QQ的文件： qq(pid9816) 以 daemon 模式运行时的 log 文件： daemon-xxx.log 默认的工作目录为 ~/.qqbot-tmp/ ，可以在启动 qqbot 时通过命令行参数 -b|--bench 指定其他工作目录，例如： qqbot -b bench 。 \u0026lt;h1\u0026gt;八、 插件\u0026lt;/h1\u0026gt; 插件的存放位置 插件实际上是一个 python 模块，因此可以是一个 python 文件，也可以是一个 python package。 qqbot 会根据插件名在以下目录中搜索插件： 配置中的 pluginPath 选项（命令行参数 -pp|--pluginPath ）指定的目录 工作目录下的 plugins 目录 python 的导入目录 插件的加载/卸载 hot-plug 方式 可以在 qqbot 的运行过程中动态的加载/卸载插件，有以下三种方法： 利用 qq 命令行工具： qq plug pluginname 或 qq unplug pluginname 利用 http-api 接口： http://127.0.0.1:8188/plug/pluginname 或 http://127.0.0.1:8188/unplug/pluginname 利用 bot 对象的接口： bot.Plug(\u0026#39;pluginname\u0026#39;) 或 bot.Unplug(\u0026#39;pluginname\u0026#39;) 前面两种方法是供 qqbot 进程的外部进程调用的，第三种方法是在 qqbot 进程内部使用的。请勿在 qqbot 进程的内部使用前面两种方法。 注意：采用 hot-plug 方式加载的插件在 qqbot 重启后会丢失。 auto-plug-at-start 方式 也可以在 qqbot 的启动时自动加载插件，在配置中的 plugins 选项（命令行参数 -pl|--plugins ）中指定需要加载的插件名就可以了。这些插件将在启动时、登录之前被加载。 另外，如果系统中（或插件目录中）存在名为 qqbotdefault 的 package ，那么该 package 下面的所有子模块都会被当成插件在启动时自动加载（注意：qqbotdefault 本身不会作为插件加载）。 插件内的 onPlug 和 onUnplug 回调函数 插件被加载时，会执行 reload(pluginName) ，因此插件内的所有代码都会被执行一次 当采用 hot-plug 的方式加载时，插件内的 onPlug 函数会紧接在 reload 成功后被执行 当采用 auto-plug-at-start 方式加载时，插件在启动时、登录之前被加载，但插件内的 onPlug 函数会延迟到登录成功后才被执行 插件被卸载时，插件内的 onUnplug 被执行 插件的编写 编写插件主要就是编写回调函数或定时任务函数，详见 第四~六节 。 插件列表 名称\tgithub作者\t功能说明\t是否默认加载 qqbot.plugins.sampleslots\tpandolia\t回调函数示例\t是 qqbot.plugins.schedrestart\tpandolia\t定时重启\t是 qqbot.plugins.miniirc\tpandolia\tIRC服务器\t否 passwordlogin\tpandolia\t使用用户名-密码登录\t否 adblock\tfeisuweb\t群广告拦截\t否 chatlog\tfeisuweb\t聊天内容记录\t否 如果您有好用的插件分享，欢迎发邮件给我。 \u0026lt;h1\u0026gt;九、 命令行模式下使用 IRC 聊天\u0026lt;/h1\u0026gt; linux 系统下，由于无法使用 QQ 客户端，可以使用插件 qqbot.plugins.miniirc 来实现用 IRC 聊天的功能。加载方式： qq plug qqbot.plugins.miniirc ，或启动时加载： qqbot -pl qqbot.plugins.miniirc ，或者在配置文件中的 plugins 选项中加入 qqbot.plugins.miniirc 。 插件加载后将在 6667 端口开启一个微型的 IRC 服务器，用户可以使用 IRC 客户端（如 weechat, irssi 等）连接此服务器来实现命令行模式下的聊天。以下以 weechat 为例介绍使用方法： 启动 weechat ： weechat\n连接本服务器： /connect localhost\n进入 群聊天 会话： /join group-name\n进入 讨论组聊天 会话： /join !discuss-name\n进入 好友聊天 会话： /query buddy-name\n进入 聊天会话 后，直接敲入文本并回车就可以向对方发送消息了。所有接收到的 QQ 消息也会被转发给相应的 聊天会话 。\n在聊天会话之间切换： ctrl+P 或 ctrl+N\n显示所有 群和讨论组 的名称： /list\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 以上几乎就是此微型 IRC 服务器所提供的所有功能了，但已经足够用来和 QQ 好友/群/讨论组 聊天了。 \u0026lt;h1\u0026gt;十、 smartqq 协议支持及限制\u0026lt;/h1\u0026gt; 本项目已实现绝大部分 smartqq 协议支持的功能，如下： 消息收/发 联系人（包括 好友/群/讨论组/群成员/讨论组成员）资料获取和查询（包括 昵称/名称/备注名） 联系人资料根据需要动态更新 被群内其他成员 @ 的通知 发送、接收表情（详见 facemap.py） 可以获取到自身发送的 好友/群/讨论组 消息。但若是自身发送的 好友 消息，只能获取消息文本，无法知道该消息发送给谁。 其他功能： 调用系统默认图片浏览器显示登录二维码、将登录二维码发送至邮箱、开启一个 http 服务器用来显示登录二维码、在命令行窗口使用文本模式显示二维码 用 qq 命令行工具发消息、查询|更新联系人 提供 HTTP-API 接口发消息、查询|更新联系人 提供 miniirc 插件，可以在命令行模式下使用 IRC 客户端聊天 掉线后自动重启功能（有时需要手工扫码） 定时执行任务（通过 qqbotsched 实现） 因 smartqq 协议的限制，以下问题尚无完美的解决方法： 无法长时间保持在线状态，每次登录成功后的 cookie 会每在 1 ~ 2 天后失效，将被腾讯服务器强制下线，此时 必须 重新登录。可以打开邮箱模式和自动重启模式，并配合 qqbot.plugins.schedrestart 插件使用，每天在固定的时间 手工扫码 登录一次，基本上可以稳定的保持在线状态。 无法发送图片、文件、音频、 xml 卡片消息 无法获取到联系人的实际 QQ 无法在群内 @ 其他成员，即便用本程序在群里发送了 “@jack xxx” 这样的消息， jack 也只能收到这个纯文本，收不到“有人@我”的提醒。 无法向 群/讨论组 内的其他非好友成员发消息，也无法收到非好友成员发过来的临时会话消息 在非常少的情况下，发消息时会重复发送多次，也可能对方已收到消息但返回发送失败的结果 \u0026lt;h1\u0026gt;十一、其他\u0026lt;/h1\u0026gt; 常见问题 更新日志 \u0026lt;h1\u0026gt;十二、参考资料\u0026lt;/h1\u0026gt; QQBot 参考了以下开源项目： ScienJus/qqbot (ruby) floatinghotpot/qqbot (node.js) sjdy521/Mojo-Webqq (perl) 在此感谢以上三位作者的无私分享，特别是感谢 ScienJus 对 SmartQQ 协议所做出的深入细致的分析。 \u0026lt;h1\u0026gt;十三、反馈\u0026lt;/h1\u0026gt; 有任何问题或建议可以发邮件给我 pandolia@yeah.net ，或者直接提 issue ，也可以加 QQ 群： 577126408 。但还是希望您在提问之前通读一下本文档，很有可能您想要的答案已经在文档中了。 \u0026lt;h1\u0026gt;定制我想要的机器人\u0026lt;/h1\u0026gt; 根据这位大佬的框架，如何写一个自己的机器人呢？ 我开始是这样构思的，如何将机器人实现，如果他接收到一条失物消息，那么就将这条消息转发到我们失物招领三个群当中去，这是我们的用户需求。 我们现在来把它转换成系统需求： （1）机器人需要监控它所接收到的消息，判断是否是失物招领消息。 （2）如果是失物招领消息，那么把消息提取出来作为一个字符串发送到我所指定的三个群当中去。 嗯，大体上就是这个样子的，应该定义一个函数就能解决了。 一开始我写了这样的一个函数： from qqbot import qqbotsched def onQQMessage(bot,cntact,member,content): qian=bot.list(\u0026lsquo;buddy\u0026rsquo;,\u0026lsquo;不一\u0026rsquo;)[0] //将我qq号昵称检测到qian中 linux=bot.list(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;驾校\u0026rsquo;)[0] //将我们寝室群放到linux中 windows=bot.list(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;Ingress\u0026rsquo;)[0] //将另一个群放到windows中 if content from qian.name: //如果监测到消息中有我qq号的消息 bot.SendTo(linux,content) //那么将这条消息转发到指定的群\n1 2 但经过测试，这几行代码并没有什么用，然后我有做了一些修改： form qqbot import qqbotsched def onQQMessage(bot,cntact,member,content): linux=bot.List(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;安工程失物招领群1\u0026rsquo;) windows=bot.List(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;安工程失物招领群2\u0026rsquo;) deepin=bot.List(\u0026lsquo;group\u0026rsquo;,\u0026lsquo;安工程失物招领群3\u0026rsquo;) if \u0026lsquo;@ME\u0026rsquo; in content: bot.SendTo()\n1 2 3 就酱紫，就这么简单的几行代码。但是我却想了很久，看了很长时间的文档。 现在是深刻的理解到了。编程并不是写代码，思考和选择算法的过程才是最重要的！ ","permalink":"https://csqread.top/posts/tech/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%88%91%E7%9A%84qq%E6%9C%BA%E5%99%A8%E4%BA%BA/","summary":"一直以来，我在想做一个qq机器人，能够实现qq消息的转发，因为对于失物招领工作，其他的都挺满意的，但每天发那么多消息确实很浪费时间，所以就想","title":"自定义我的QQ机器人"}]